# Natural Language Processing

è‡ªç„¶è¨€èªå‡¦ç†ã€‚ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ã¯[æ©Ÿæ¢°å­¦ç¿’](../machine-learning/README.qmd)ã‚‚ã”è¦§ãã ã•ã„ã€‚

## LLM

### GPT-4

- [gpt\-4\.pdf](https://cdn.openai.com/papers/gpt-4.pdf)

### OPT

Open Pre-trained Transformerã€‚2022å¹´5æœˆã«MetaãŒç™ºè¡¨ã—ãŸLLMã€‚ãƒ­ãƒ¼ã‚«ãƒ«ã§LLMã‚’å‹•ã‹ã—ãŸã„å ´åˆã«é‡å®ã™ã‚‹ã“ã¨ãŒã‚ã‚Šãã†ã ã€‚

- [\[2205.01068\] OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068)
- [metaseq/projects/OPT at main Â· facebookresearch/metaseq Â· GitHub](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT)

### ChatGPT

OpenAIãŒå…¬é–‹ã—ãŸã€GPT-3.5ã‚’ãƒ™ãƒ¼ã‚¹ã«RLHFã§Fine Tuningã—ãŸä¼šè©±ç‰¹åŒ–ã®LLMã€‚  
[APIçµŒç”±ã®ãƒ‡ãƒ¼ã‚¿ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«åˆ©ç”¨ã•ã‚Œãšã€é€†ã«ChatGPTçµŒç”±ã®ãƒ‡ãƒ¼ã‚¿ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«åˆ©ç”¨ã•ã‚Œã‚‹ã€‚](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance)ã‚ªãƒ—ãƒˆã‚¢ã‚¦ãƒˆã‚‚å¯èƒ½ã€‚

- [ChatGPT](https://chat.openai.com/chat)
- [Introducing ChatGPT](https://openai.com/blog/chatgpt)

### Facebook LLaMa

2023å¹´2æœˆã«Meta AI ResearchãŒç™ºè¡¨ã—ãŸLLMã€‚  
å…¬é–‹ã‹ã‚‰ï¼‘é€±é–“å¾Œã«å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ãŒ4chanã«æµå‡ºã—ãŸã€‚ä¸€æ–¹ã§ã€M1Macã§å‹•ä½œã™ã‚‹[llama.cpp](https://github.com/ggerganov/llama.cpp)ãŒç™»å ´ã—ã€ä»¥å¾Œãƒ©ã‚ºãƒ‘ã‚¤ã§ã‚‚å‹•ããªã©æ¶ˆè²»è€…å‘ã‘ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã§å®Ÿè¡Œå¯èƒ½ã«ãªã£ã¦ã„ã‚‹ã€‚

- [Introducing LLaMA: A foundational, 65\-billion\-parameter language model](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
- [facebookresearch/llama: Inference code for LLaMA models | GitHub](https://github.com/facebookresearch/llama)
- [\[2302.13971v1\] LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971v1)

### Stanford Alpaca

Stanfordå¤§å­¦ãŒLLaMA-7Bã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã€‚

ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯æ¬¡ã®ã¨ãŠã‚Šã€‚ï¼ˆ2023-03-15æ™‚ç‚¹ï¼‰  
è¦‹ã¦ã®é€šã‚Šã€ã™ã¹ã¦èª¤ã£ã¦ã„ã‚‹ã€‚

```txt
Q: What is your name?
A: My name is Joe.

Q: 111 * 111 = ?
A: 111 * 111 = 1231.

Q: æ—¥æœ¬ã®éƒ½é“åºœçœŒã‚’ã€äººå£ãŒå¤šã„é †ã«3ã¤æŒ™ã’ã¦ãã ã•ã„ã€‚
A: 1. Tokyo 2. Yokohama 3. Osaka
```

- [tatsu-lab/stanford_alpaca | GitHub](https://github.com/tatsu-lab/stanford_alpaca)
- [Alpaca demo](https://crfm.stanford.edu/alpaca/)

### Vicuna

ãƒ“ã‚¯ãƒ¼ãƒ‹ãƒ£ã€‚åå‰ã®ç”±æ¥ã¯ã‚¢ãƒ«ãƒ‘ã‚«ã‚„ãƒ©ãƒã¨åŒã˜ç¨®é¡ã®å‹•ç‰©ã€‚

### Claude

Anthoropic[^Anthoropic]ãŒ2023-03-14ã«ç™ºè¡¨ã—ãŸAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‚µãƒ¼ãƒ“ã‚¹ã€‚APIã§ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒå¯èƒ½ãªã©ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚’æ„è­˜ã—ã¦ã„ã‚‹ã“ã¨ãŒä¼ºãˆã‚‹ã€‚åå‰ã®å…ƒãƒã‚¿ã¯ãŠãã‚‰ãã€[Claude Shannon (ã‚¯ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚·ãƒ£ãƒãƒ³)](https://ja.wikipedia.org/wiki/%E3%82%AF%E3%83%AD%E3%83%BC%E3%83%89%E3%83%BB%E3%82%B7%E3%83%A3%E3%83%8E%E3%83%B3)ã€‚

[^Anthoropic]: å…ƒOpenAIç¤¾å“¡ãŒèµ·æ¥­ã€‚"Anthorop-"ã¯ã€Œäººé¡ã€ã‚’è¡¨ã™æ¥é ­èªã€‚

## Training method

### RLHF

Reinforcement Learning from Human Feedbackã€‚Fine Tuningã®æ‰‹æ³•ã®ä¸€ã¤ã€‚  
ä½™è«‡ã ãŒã€è«–æ–‡ä¸­ã«ã¯RLFHã‚„Reinforcement Learning from Human Feedbackã¨ã„ã£ãŸè¡¨è¨˜ã¯å‡ºã¦ã“ãªã„ã€‚

- [\[1909.08593\] Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593)
- [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)

### LoRA

MicrosoftãŒ2021å¹´ã«ç™ºè¡¨ã—ãŸè«–æ–‡ã€[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)ã€ã§ææ¡ˆã•ã‚ŒãŸæ‰‹æ³•ã€‚  
å­¦ç¿’å¯¾è±¡ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ä¸€éƒ¨ã«é™å®šã™ã‚‹ã“ã¨ã§ã€GPUãƒ¡ãƒ¢ãƒªã‚„ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’å‰Šæ¸›ã—ã¤ã¤Fine Tuningä¸¦ã®ç²¾åº¦ã‚’é”æˆã—ã¦ã„ã‚‹ã€‚

- [ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ãƒ¬ãƒãƒ¼ãƒˆã€‘6.7Bæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹LoRAãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°](https://engineering.linecorp.com/ja/blog/lora-tuning-for-japanese-model)

### RLAIF

Reinforcement Learning for AI Fairnessã€‚AnthoropicãŒ2022å¹´ã«ç™ºè¡¨ã—ãŸè«–æ–‡ã€[Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)ã€ã§ææ¡ˆã•ã‚ŒãŸæ‰‹æ³•ã€‚æœ‰ç”¨ãªãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æœ‰å®³æ€§ã‚’å–ã‚Šé™¤ãéš›ã«ã€äººé–“ã®ä»£ã‚ã‚Šã«æ†²æ³•ã«ã‚ˆã£ã¦åˆ¤æ–­ã™ã‚‹AIï¼ˆCAI, Constitutional AIï¼‰ã‚’æ´»ç”¨ã™ã‚‹ã€‚

### PEFT

Parameter-Efficient Fine-Tuningã€‚HuggingFaceãŒå…¬é–‹ã—ã¦ã„ã‚‹ã€åŠ¹ç‡çš„ã«è¨“ç·´ã‚’è¡Œã†ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚

- [huggingface/peft: ğŸ¤— PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.](https://github.com/huggingface/peft)

## Benchmark

OpenAIã®å…¬é–‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯[Evals](https://github.com/openai/evals)ãŒå‚è€ƒã«ãªã‚‹ã€‚

ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã§ã¯ã€LLMã¨ä¸€èˆ¬çš„ãªæ—¥æœ¬èªãƒã‚¤ãƒ†ã‚£ãƒ–ã‚’æ¯”è¼ƒã—ãŸã„ã€‚ãƒ†ã‚¹ãƒˆç”¨ã®ã‚¿ã‚¹ã‚¯ã‚’ã‚‚ã¨ã«ã€æ¬¡ã®ã‚ˆã†ãªè³ªå•ã‚’å®šã‚ãŸã€‚

```txt
# åŸºæœ¬ã¨ãªã‚‹çŸ¥è­˜å•é¡Œ
Q: What is the first Japanese prime minister?

# æ—¥æœ¬èªã«ã‚ˆã‚‹çŸ¥è­˜å•é¡Œ
Q: æ—¥æœ¬ã®éƒ½é“åºœçœŒã‚’ã€äººå£ãŒå¤šã„é †ã«3ã¤æŒ™ã’ã¦ãã ã•ã„ã€‚

# è¨ˆç®—
Q: What's 111*111?

# ã‚¸ãƒ§ãƒ¼ã‚¯
Q: ãƒŸã‚«ãƒ³ã‚’ä½¿ã£ãŸãƒ€ã‚¸ãƒ£ãƒ¬ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚


# ãã®ä»–
Q: What is your name?
```

## Prompt Engineering

### Chain of Thought

LLMã«Reasoningã‚¿ã‚¹ã‚¯[^reasoning]ã‚’ä¾é ¼ã™ã‚‹éš›ã«ã€ã€Œé€”ä¸­å¼ã‚’æ›¸ã„ã¦ã€ã€Œã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã§è€ƒãˆã¦ã€ã¨æŒ‡ç¤ºã™ã‚‹ã“ã¨ã€‚
[^reasoning]: è«–ç†çš„æ€è€ƒåŠ›ã‚’æ¸¬ã‚‹ã‚¿ã‚¹ã‚¯ã€‚ã“ã“ã§ã¯ã€ã¤ã‚‹ã‹ã‚ç®—ãªã©ã€‚å€‹äººçš„ã«ã¯ã€ICUã®ãƒªãƒ™ãƒ©ãƒ«ã‚¢ãƒ¼ãƒ„é©æ­£è€ƒæŸ»ã®ã‚ˆã†ãªã‚‚ã®ï¼ˆ[ä¾‹é¡Œ](https://icu.bucho.net/icu/pastexams/SAT80.pdf)ï¼‰ã‚’æƒ³åƒã—ãŸã€‚

- [\[2201.11903\] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)

### Self-Consistency

è‡ªå·±ç„¡çŸ›ç›¾æ€§ã€‚ç°¡å˜ã«è¨€ãˆã°ã€AIã«æ¤œç®—ã•ã›ã‚‹ã“ã¨ã§å›ç­”ã®ç²¾åº¦ã‚’ä¸Šã’ã‚‹ã‚„ã‚Šæ–¹ã€‚  

- [\[2203.11171\] Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)
