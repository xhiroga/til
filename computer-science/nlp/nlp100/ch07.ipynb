{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 60. 単語ベクトルの読み込みと表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model: KeyedVectors = KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "# 実行に50秒弱かかる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コサイン類似度について...  \n",
    "要するにコサインのことらしい。\n",
    "ベクトルの内積の求め方に2通りあることを利用すると簡単に算出できる。すなわち、`（ベクトルの要素同士をかけ合わせたものの合計）/（長さを全てかけ合わせたもの）＝コサイン`\n",
    "\n",
    "もちろん実際には、ライブラリにn次元のベクトルを渡せば計算してくれる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73107743"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('United_States', \"U.S.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52173746"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('United_States', \"Japan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50480795"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('United_States', \"China\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/ch07.ipynb Cell 7'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/ch07.ipynb#ch0000007?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mmost_similar(\u001b[39m'\u001b[39;49m\u001b[39mUnited_States\u001b[39;49m\u001b[39m'\u001b[39;49m, topn\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m~/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/gensim/models/keyedvectors.py:750\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    747\u001b[0m positive \u001b[39m=\u001b[39m _ensure_list(positive)\n\u001b[1;32m    748\u001b[0m negative \u001b[39m=\u001b[39m _ensure_list(negative)\n\u001b[0;32m--> 750\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfill_norms()\n\u001b[1;32m    751\u001b[0m clip_end \u001b[39m=\u001b[39m clip_end \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectors)\n\u001b[1;32m    753\u001b[0m \u001b[39mif\u001b[39;00m restrict_vocab:\n",
      "File \u001b[0;32m~/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/gensim/models/keyedvectors.py:635\u001b[0m, in \u001b[0;36mKeyedVectors.fill_norms\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39mEnsure per-vector norms are available.\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m \n\u001b[1;32m    633\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorms \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m force:\n\u001b[0;32m--> 635\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorms \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mnorm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectors, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/numpy/linalg/linalg.py:2546\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2543\u001b[0m     \u001b[39mreturn\u001b[39;00m add\u001b[39m.\u001b[39mreduce(\u001b[39mabs\u001b[39m(x), axis\u001b[39m=\u001b[39maxis, keepdims\u001b[39m=\u001b[39mkeepdims)\n\u001b[1;32m   2544\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mord\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mord\u001b[39m \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m   2545\u001b[0m     \u001b[39m# special case for speedup\u001b[39;00m\n\u001b[0;32m-> 2546\u001b[0m     s \u001b[39m=\u001b[39m (x\u001b[39m.\u001b[39;49mconj() \u001b[39m*\u001b[39;49m x)\u001b[39m.\u001b[39mreal\n\u001b[1;32m   2547\u001b[0m     \u001b[39mreturn\u001b[39;00m sqrt(add\u001b[39m.\u001b[39mreduce(s, axis\u001b[39m=\u001b[39maxis, keepdims\u001b[39m=\u001b[39mkeepdims))\n\u001b[1;32m   2548\u001b[0m \u001b[39m# None of the str-type keywords for ord ('fro', 'nuc')\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m \u001b[39m# are valid for vectors\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.most_similar('United_States', topn=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Athens', 0.6826056838035583),\n",
       " ('Greece', 0.4856836199760437),\n",
       " ('Athens_Greece', 0.46644291281700134),\n",
       " ('Spain', 0.4448360204696655),\n",
       " ('Rome', 0.41419845819473267),\n",
       " ('Organising_Committee_ATHOC', 0.41101542115211487),\n",
       " ('prosecutor_Costas_Simitzoglou', 0.4097808599472046),\n",
       " ('bronze_medalist_Alicia_Molik', 0.3909006416797638),\n",
       " ('Greek', 0.39005470275878906),\n",
       " ('silver_medalist_Mardy_Fish', 0.38434845209121704)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = model['Spain'] - model['madrid'] + model['Athens'] # スペインの首都はマドリード、ギリシャの首都はアテネ\n",
    "model.most_similar(vec, topn=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/questions-words.txt', sep=' ', header=None, names=['vec1', 'vec2', 'vec3', 'vec4'], skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['vec1'].str.startswith(':') == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def get_most_similar_word_and_similarity(word: str) -> Tuple[str, float]:\n",
    "    return model.most_similar(word, topn=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1: pd.Series = df[0:10]['vec1']\n",
    "df_most_similar_word_and_similarity = vec1.map(get_most_similar_word_and_similarity) # WIP: 本当は2列にしたかった\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic = df[df.index <= 8872] # 手抜き\n",
    "syntactic = df[df.index > 8872]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sim_df = pd.read_csv('./data/wordsim353/set1.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(row):\n",
    "    # print(row['Word 1'], row['Word 2'])\n",
    "    return model.similarity(row['Word 1'], row['Word 2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Human (mean)</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>Machine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.263938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>8.5</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.517296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.363463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.396392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word 1    Word 2  Human (mean)     1     2     3   4   5   6     7   8  \\\n",
       "0      love       sex          6.77   9.0   6.0   8.0   8   7   8   8.0   4   \n",
       "1     tiger       cat          7.35   9.0   7.0   8.0   7   8   9   8.5   5   \n",
       "2     tiger     tiger         10.00  10.0  10.0  10.0  10  10  10  10.0  10   \n",
       "3      book     paper          7.46   8.0   8.0   7.0   7   8   9   7.0   6   \n",
       "4  computer  keyboard          7.62   8.0   7.0   9.0   9   8   8   7.0   7   \n",
       "\n",
       "      9  10    11  12  13   Machine  \n",
       "0   7.0   2   6.0   7   8  0.263938  \n",
       "1   6.0   9   7.0   5   7  0.517296  \n",
       "2  10.0  10  10.0  10  10  1.000000  \n",
       "3   7.0   8   9.0   4   9  0.363463  \n",
       "4   6.0   8  10.0   3   9  0.396392  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sim_df['Machine'] = word_sim_df[['Word 1', 'Word 2',]].apply(lambda row: get_similarity(row), axis=1)\n",
    "word_sim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Human (mean)</th>\n",
       "      <th>Machine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Human (mean)</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.665139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Machine</th>\n",
       "      <td>0.665139</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Human (mean)   Machine\n",
       "Human (mean)      1.000000  0.665139\n",
       "Machine           0.665139  1.000000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sim_df[['Human (mean)', 'Machine']].corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9829b077c52378d5e1bc55420cf8cc5a4f9f459501a552e8de61cb75f7311e5d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
