# データ構造・アルゴリズム (data structures and algorithms)

> [!NOTE]
> $O(NM)$の処理では、よく二重ループが登場する。文字列探索におけるナイーブ法を例に、Pythonにおける個人的に好ましい書き方を考える。
>
> ```python
> def partial_match(s: str, t, str) -> bool:
>   i = 0
>   while i < len(s):
>     j = 0
>     while j < len(t):
>       if s[i+j] != t[j]:
>         break # 文字列が一致しないことはここで明らかなのに、ループを抜けた後に一致の判定がある
>       j += 1
>     if j == len(t): # 一致の判定
>       return True
>     i += 1
> 
>   return False
> ```
>
> Pythonには、Javaにあるような`ラベル付きbreak/continue`がない。そのため、ループは内側→外側の順で抜けるしかなく、結果として内側のループを抜けたとき、ループの結果を改めて判定することになってしまう。  
> 実はPythonでも`ラベル付きbreak/continue`の提案はあったようだ。[^pep3136]しかしRejectされているため、代替案として「例外処理を用いる」「内側のループを関数で括り、不一致時にはreturnする」などの方法が考えられる。けれど見づらいし、関数や例外処理は感覚的にコストが重そうなので、例示の通り内側のループ直後に改めて結果の判定を行うことにする。

[^pep3136]: [PEP 3136 – Labeled break and continue](https://peps.python.org/pep-3136/)

## データ構造

### スタック・キュー・両端キュー

先頭または末尾にデータを追加・削除できるデータ構造を次に述べる。

- スタック (stack)
- キュー (queue)
- 両端キュー, デック (deque, double-ended queue)

それぞれのPythonにおける実装とデータ追加・削除の操作をまとめた。（`pop()`するのはスタック？キュー？といつも迷うので）

| データ構造\実装 | list                                     | collections.deque                           | queue.Queue                  |
| --------------- | ---------------------------------------- | ------------------------------------------- |
| スタック        | 追加: append(), 取り出し: pop()          | 追加: append(), 取り出し: pop()             | -                            |
| キュー          | 追加: append(), 取り出し: pop(0)         | 追加: append(), 取り出し: popleft()         | 追加: put(), 取り出し: get() |
| 両端キュー      | 追加: append(), 取り出し: pop() / pop(0) | 追加: append(), 取り出し: pop() / popleft() | -                            |

末尾を取得するのが`pop()`と覚えておけば良さそうだ。なお、先頭を取得するのが`pull()`の言語もあるようだが、Pythonは違う。

### ヒープ

- 最小ヒープは、親が子よりも小さく、根が最小になるヒープ。単にヒープといえば最小ヒープ。
- 最大ヒープはその逆となる。
- 二分ヒープの読みは 「にぶん」ヒープ
- 二分ヒープと優先度付きキューの違いは、（WIP）

### Union-Find

互いに素な集合がある時に、ある2つの要素が同じ集合に属しているか？を調べることをUnion-Findという。

集合を木構造で表したとき、同じ集合に属しているか？を調べるのは根を求めることに等しく、計算量が$O(N)$となりえる。

しかし、union by rank を行うことで$O(log N) に、さらに経路圧縮を行うことで$O(α(N))$となる。$α(N)$はアッカーマンの逆関数と呼ばれ、増加がとても遅いことで知られる。[^union_find]

- union by rank: 2つの木を合体させるとき、背の高い木に背の低い木をつなげる
- 経路圧縮: find処理時、中間節点を全て根に繋ぎなおす

[^union_find]: [Union-Find の 2 つの工夫](https://algo-method.com/descriptions/133)

### データ構造: 参考

- 10章 ヒープ
- 14章 高度なデータ構造
- [Pythonのスタックとキューには何を使えばいいのか（各データ構造の速度比較）](https://qiita.com/saba/items/107c4237206e31acdbef)

## アルゴリズム

### 番兵

- `for`ループまたは`while`ループで、探索のための比較以外に終了条件の判定を行うことが面倒であるため、リストの最後に目的のデータをダミーで入れておく。
- Pythonでループを実装する場合や、TypeScriptで`find`などを用いる場合、勝手に最後まで探索してくれるので考慮不要。

### ハッシュの衝突

アルゴリズム図鑑で紹介されているチェーン法の他に、オープンアドレス法による対応がある。オープンアドレス法には、線形走査法とダブルハッシュ法がある。

- 線形走査法
  - 衝突が発生した場合、その1つ後ろの位置を格納箇所とする
  - 線形走査法・オープンアドレス法の両方に言えるが、いずれはハッシュ表が満杯になる。
- ダブルハッシュ法
  - ハッシュが衝突したら、引数に衝突回数を加えて再度ハッシュを求める。

[ハッシュ探索②（オープンアドレス法） | Programming Place Plus　アルゴリズムとデータ構造編【探索アルゴリズム】　第７章](https://programming-place.net/ppp/contents/algorithm/search/007.html#linear_probing)

### ソート

- 挿入ソート
  - カードの山を右手に取り、1枚引いて引いて左手側に加える、この時左手側の適切な位置に挿入することを繰り返す。
  - 再帰でも書ける。
  - 個人的には左手ソートと呼びたい。
- 選択ソート
  - データの先頭（または後方）が徐々に整理済みになっていく点は、挿入ソートと変わらない。
  - 1番小さい（大きい）値、2番目に小さい（大きい）値…と順番に探していく点と、値が見つかった後は元々n番目にいた値が入れ替わりで飛ばされてしまう点が異なる。
  - この入れ替わりで飛ばされてしまう点が面白いので、個人的には（ダンジョン飯の）ミスルンソートと呼びたい。
- バブルソート
  - 右から左に、小さい値がバトンタッチしながらやってくる様子から、個人的には玉突きソートと呼びたい。
- シェルソート
- マージソート
- クイックソート
  - 再帰的にパーティションを行う。
- 計数ソート
  - [計数ソート | アルゴリズムビジュアル大事典](https://yutaka-watanobe.github.io/star-aida/1.0/algorithms/counting_sort/print.html)を参照。
- 基数ソート

シェルソート

![シェルソート（螺旋本）](/images/シェルソート（螺旋本）.svg)

パーティション

![パーティション](/images/パーティション.svg)

## 探索

### KMP法

文字列探索とは、文字列Sが単語Wを含む場合に、それが文字列Sの何文字目から何文字目かを調べるタスクである。例えば、S="Hello,world!"、W="world"の場合、6文字目から10文字目が一致する。普通に計算すると、計算量は$O(|S| \cdot |W|)$となる。

KMP法は、一度比較した文字は、なるべく2回比較したくない、という気持ちのアルゴリズムである。例えば次の文字列を考える。

- S: "firefirefighter"
- W: "firefighter"

この文字列は偶然にも先頭から6文字が一致しているので、先頭から比較を始めたKMP法は7文字目で一致しないことに気づく。ここで単純なアルゴリズムなら、次にSの2文字目"i"とWの1文字目"f"を比較するところから始めるだろう。しかしKMP法は、「もうSの6文字目までは比較したから、次はSの7文字目"r"とWの1文字目"f"を比較したい！」と考える。もちろんこれは失敗する！

失敗する原因は、すでに比較した文字列の中に、先頭が一致する他のパターンが含まれているためである。Sの部分文字列"fire**fi**r"の後半の**fi**は、Wの"fighter"だけでなく"fire"とも比較しなければいけないのだ！そこでKMP法では、Wの各文字に対して、「この文字で比較が失敗したら、Wの直前のX文字分と一致していたSの文字列はWの先頭とも一致するはずだから、Sの現在地をX文字巻き戻した箇所とWの先頭を揃えてから再開してね」という数Xを事前に計算しておく。これを部分マッチテーブルという。

## 木の探索

次の4通り。

- 深さ優先探索
  - 行きがけ順
    - 最もシンプル。迷路の右手法と同じ。
  - 通りがけ順
    - 西から順番に印を付けるイメージ。
  - 帰りがけ順
    - 個人的には行き止まり順だと思っている。
- 幅優先探索

### アルゴリズム: 参考

- 5章 探索
- 7章 高等的整列

## アルゴリズムの解析

アルゴリズムの実行時間を解析するうえで、私たちは実行時間が入力サイズに対して漸近的に増える効率に興味がある。入力サイズと相対的な実行時間を、おそらく計算量という。

計算量には時間計算量と領域計算量がある。単に計算量といえば、普通時間計算量を指す。領域計算量は空間計算量とも言う。

アルゴリズムの実行時間を表すための、漸近記法をまとめた。

![漸近記法](/images/漸近記法.svg)

- Θ記法, シータ記法 (theta notation)
- 𝑂記法、ビッグオー記法(big-o notation)
  - アルファベットのO (オー)ではなく、ギリシャ文字のΟ (オミクロン)。
  - $f(x)=Ο(g(x))$のとき、$f(x)$が$g(x)$の定数倍と同じかそれより小さいことを指す。
- 𝑜記法, リトルオー記法, スモールオー記法 (little-o notation)
  - $f(x)=ο(g(x))$のとき、$f(x)$が$g(x)$の定数倍より小さいことを指す。
- Ω記法, ビッグオメガ記法 (big-omega notation)
  - $f(x)=Ω(g(x))$のとき、$f(x)$が$g(x)$の定数倍と同じかそれより大きいことを指す。
- 𝜔記法, リトルオメガ記法 (little-omega notation)

なお、Ο (オミクロン)を変換するのが面倒なので、以降はアルファベットのO (オー)を用いる。

### アルゴリズムの解析: 参考

- [アルゴリズムイントロダクション 第3版 - 3 関数の増加](https://amzn.to/4avVZdK)
- [計算量の漸近記法（Θ記法・O記法・Ω記法・o記法・ω記法）](https://blog.publictheta.com/articles/qCAEowAbozLeMcFreq73f)
- 2章 アルゴリズムと計算量

## 再帰

- 再帰木
- 再帰方程式

## バックトラッキング

### バックトラッキングの問題

- 最長増加部分列
  - ある列に含まれる文字を、順序を崩さぬようにいくつか選んで再構成された列。
  - 列が元の列において連続である場合は「部分文字列」と呼ぶ。
  - 最長増加部分列とは、元のアルファベットなどの文字列の部分列のうち、a,b,c...の順番になっているようなものを指す。

## 動的計画法

- あずぱんさん曰く「表的計画法」。
  - [動的計画法(DP)をみんながわかるように超ていねいにじっくり説明した | あずぱん動画](動的計画法(DP)をみんながわかるように超ていねいにじっくり説明した)

<!-- https://www.mathenachia.blog/dp/ から分かる範囲で理解して言語化する。 -->

### 動的計画法の問題

- 編集距離
- 連鎖行列積
  - なるべく大きい数字を早めに消してしまうのが方針となる。

動的計画法の例として、編集距離の問題を考える。編集距離とは、単語Aを1文字づつ置換・挿入・削除することで、何手で単語Bに変身できるかを指す値である。例えば、`MONEY`を`FOOD`に変身させるにあたって、次の手順では4手で変身でき、同時に2単語間の最短距離でもある。

1. MONEY → FONEY
2. FONEY → FOOEY
3. FOOEY → FOODY
4. FOODY → FOOD

次に、`MONEY`を`LOVE`に変身させる場合で、編集距離の依存関係をフローチャートで示す。[動的計画法（編集距離の依存関係の可視化）](./DP.md)を参照のこと。

### 最大部分配列問題

最大部分配列とは、正〜負の整数を要素に持つ配列に対して、連続する部分列で和が最大のものを求める問題である。これを解くKadane[^j_kadane]'s algorithm（カデインのアルゴリズム）について、最適な小構造に注目した説明を考えた。
[^j_kadane]: 2024年現在もCMUに勤めておられる。[ご本人の映像](https://www.dailymotion.com/video/xlbpc2)があるので、見るとアルゴリズムに親近感が湧くかもしれない。

正〜負の整数がバランスよく含まれる最大部分配列を2つの部分配列に分けると、どちらの部分配列の和も正になる。それを踏まえたうえで、次の手順で最大部分配列を求める。

1. N文字目から順番に数を足していき、M文字目で和の合計が負になった時、最大部分配列がN文字目から始まるとしたらM−1文字目までのどこかである。
2. N文字目からM文字目までの各段階の和の合計のうち最大のものが、最大部分配列がN文字目から始まる場合の候補となる
3. M文字目で和の合計が負になった時、M+1文字目から再び手順を繰り返す。こうして得られた最大部分配列の候補で決勝トーナメントを行い、最大のものが配列全体の最大部分配列である。

### 動的計画法: 参考

- 11章 動的計画法

## 貪欲アルゴリズム

## グラフ

### グラフの性質

#### グラフの要素

- 頂点 (vertex), ノード (node)
  - 辺$uv$および$u→v$について、頂点$u$と$v$を endpoint(端点)
    - $u→v$について、$u$をtail(尾), $v$をhead(頭)
  - カット点
  - ソース (source): 有向グラフにおいて、ある頂点に向かう辺がない頂点
  - シンク (sink): 有向グラフにおいて、ある頂点から出る辺がない頂点。シンクは沈む、流すといった意味
  
- 辺 (edge), アーク (arc)
  - 特に有向辺をarcと呼ぶ場合もある
  - 橋

- 列 (sequence): 頂点の列
  - 歩道 (walk): 隣接する頂点からなる列

親、根、木辺などは<#探索木(森)の性質>を参照。

#### グラフの種類

- 辺に向きがあるかどうか
  - 有効グラフ (directed graph)
  - 無向グラフ (undirected graph, unordered graph)

- ループ・多重辺を持っているか
  - simple: ループ・多重辺を持たないグラフ
  - 多重グラフ (multigraph): ループ・多重辺を持つグラフ

- グラフ上のどの頂点からでも、他の頂点に行けるか？
  - 連結グラフ (connected graph)
  - 非連結グラフ (disconnected graph)
  - 強連結 (strongly connected): 有向グラフにおいて、どの頂点からでも他の頂点に行けること

- 閉路を含むか
  - 巡回グラフ (cyclic graph)
  - 非巡回グラフ (acyclic graph)

他に次のような定義がある。

- 森 (forest): 非巡回グラフ
- 木 (tree): 連結非巡回グラフ

#### グラフどうしの関係

グラフ$G=(V,E)$に対して、次のように定義される。

- 部分グラフ (subgraph): 頂点と辺のいずれも$G$に含まれるグラフ
  - 真部分グラフ (proper subgraph): subgraphであって、$G$と等しくないもの
  - 同型 (isomorphic): $G$と$G’$が、それぞれ対応する頂点と辺を持っている時、$G$と$G'$は同型
  - 誘導部分グラフ (induced subgraph): $E'=E∩\binom{V}{2}$、つまり部分グラフ内のすべての頂点は、元のグラフ$G$で持っていた辺を持っている、ということ
  - 全域部分グラフ (spanning subgraph): $V'=V$である部分グラフ。$G-e$や$G-F$で表される
    - 全域木 (spanning tree)
      - 最小全域木 (minimum spanning tree): 全域木のうち、合計の重みが最も軽い木。「すべての地域に電力を届けるための最も安いネットワークは？」といった応用がある。
      - 最短路全域木 (shortest path spanning tree): 単にshortest path tree(最短路木)といってこれを指すことが多い印象。
  - 誘導全域部分グラフ (induced spanning subgraph): 定義する必要なし（それって同型なので）

なぜ誘導グラフと呼ぶかについては、[「生成部分グラフ」という用語について](http://www.co.mi.i.nagoya-u.ac.jp/~yagiura/surijoho8/induced_subgraph.pdf)を参照。

- 逆 (reversal): $G$のすべての辺$u→w$を$w→u$に取り換えたグラフ

- 成分 (component): 極大な連結部分グラフ
  - 強連結成分 (SCC, strongly connected component), 強成分 (strong component)
  - ソース成分 (source component)
  - シンク成分 (sink component)
- 強連結成分グラフ (strong component graph): 強連結成分を1つの頂点にまとめたグラフ

### グラフの問題

- 到達可能性
- 成分の検出
- 関節点 (articulation)の検出
- 最長路 (longest path)
- SSSP(Single Source Shortest Path, 単一始点最短経路、または単に最短路)
- APSP(All Pair Shortest Path, 全点対最短経路)

### グラフのデータ構造

- 隣接リスト
  - メモリが少ない（＝文字数が少なく済む）性質から、競プロでは、グラフは隣接リストに近い形式で渡されることが多い。
- 隣接行列

### 何か優先探索

グラフの探索には、深さ優先や幅優先などいろいろあるが、実は袋（＝候補となる辺を入れておくデータ構造）に何を採用するかの違いに過ぎない。

| 名前         | 袋                               | 解決する問題 |
| ------------ | -------------------------------- | ------------ |
| 深さ優先探索 | スタック                         |              |
| 幅優先探索   | キュー                           |              |
| 最良優先探索 | 優先度付きキュー                 |              |
| 最良優先探索 | 優先度付きキュー（辺の重み）     | 最小全域木   |
| 最良優先探索 | 優先度付きキュー（辺の重みの和） | 最短路木     |
| 最良優先探索 | 優先度付きキュー（路の幅）       | 最幅路       |

#### 探索木(森)の性質

- root(根)
- parent(親)
- tree edge(木辺): ?
- forward edge(前方辺): ?
- backward edge(後方辺): ?
- cross edge(交差辺): ?

### トポロジカルソート (topological sort)

- 自分なりに言い換えると「左手法」。迷路の左手法と考え方が同じだから。
- 左手法の要領でグラフを探索し、そこから先は行き止まり、というところまで行ったら現在地をリストに記録する。
- 行き止まりから引き返すごとに、そこから先が行き止まりなら現在地をリストに記録するし、分岐があれば分岐に進む。
- [Topological Sort Visualized and Explained | Carl the Person](https://www.youtube.com/watch?v=7J3GadLzydI)が短くて分かりやすい。

### 強連結成分 (strong component), 関節点 (articulation)

強連結成分を計算するためのアルゴリズムは次の通り。線形時間で計算するための工夫がポイント。

- グラフの頂点全てに対して、お互いに到達できる点を何か優先探索で計算する（$O(VE)$）
- Kosaraju(コサラジュ)とSharir(シャリア)のアルゴリズム
  - 逆グラフの帰りがけ順に辿った頂点を、さらに逆順に何か優先探索する時に、何か優先探索がシンク成分の内側に留まることを利用したアルゴリズム
  <!-- TODO - 単にグラフの帰りがけ順ではない -->
- Tarjan(タージャン)のアルゴリズム
  <!-- TODO -->

<!-- TODO 関節点を求めるのに用いるのもTarjanやKosaraju? -->

### 最小全域木 (MST, Minimum Spanning Tree) アルゴリズム

いくつもアルゴリズムがあるが、次に述べる戦略の実装違いといえる。

その戦略とは、入力グラフ$G$に対して、その頂点だけからなる$F$（つまり、$F=(V, ∅)$）をintermediate spanning forest(中間全域森)として定義し、$F$に*いい感じ*の辺を徐々に加えていく戦略である。

<!-- ただの全域木の場合はO(E)なのに対して、最小全域木の場合はO(?)である理由 -->

#### ブルーフカ法 (Borůvka's algorighm)

<!-- 解説 -->

```python
def boruvka(V, E):
  F = (V, ∅)
  count = count_and_label(F) # 成分数
  while count > 1:
    add_all_safe_edges(E, F, count)
    count ← count_and_label(F)
  return F
```

#### プリム法 (Jarník's (Prim's) algorithm)

- 二分ヒープを用いることで高速化できる。
- [プリム法 (Prim's Algorithm) | サルでもわかるアルゴリズム](https://www.youtube.com/watch?v=anuJPP3FZ8c)が分かりやすかった。

#### クラスカル法 (Kruskal's algorithm)

<!-- TODO -->

### 最短路 (shortest path)

#### ダイクストラ法 (Dijkstra's algorithm)

- 自分なりに言い換えると、「近さランキング法」。
- 単一始点最短経路を求めるのに用いる。計算量は$O(|V|^2)$になる。
- 隣接リストと優先度付きキューを用いると、計算量は$O((|V|+|E|)log|V|)$になる。
  - [Dijkstra’s Algorithm | Depth First](https://www.youtube.com/watch?v=NyrHRNiRpds)が非常に分かりやすい。
  - ウィザードリィのような主観視点で例えると次の通り。
    1. 今いる場所（＝頂点）から見える場所について、1つづつ距離を書き留め、探索候補（優先度付きキュー）に加える。
    2. 全て書き留めたら、最も近い場所を候補から選んで進む。
    3. 同じく、進んだ先から見える場所について、1つづつ距離を測る。その時、すでに書き留めた場所だったなら、今いる場所を経由した方が近道である場合に限って書き留め、探索候補に加える。
    4. 書き留めたら、またスタートに戻ってから、次に近い場所に進む。これを場所の数繰り返す（注）
  - 注: 同じ場所を2回訪問することはない。言い換えると、探索候補の中で一番近い場所だけは、後からより近いルートが見つかることは無い。
- 螺旋本には、隣接リストと二分ヒープを用いると、計算量は$O((|V|+|E|)log|V|)$になる、とあるが...正直言って優先度付きキューでの実装より複雑だし計算量も変わらないので、私は理解していません...。

#### ベルマンフォード法 (Bellman Ford's algorithm)

- ダイクストラ法とは異なり、負の重みがあっても機能する。
- また、負の閉路を検出できる。
- 完全にイメージだけの説明になるが…例えば本のページを逆から読んでいたとして、1周目に意味が理解できるのは1ページ目だけになる。2周目には、前の周回で1ページ目を読んでいるから2ページ目が理解できる。そのようにして、最悪でもページ数分周回すれば本の内容が理解できる、ということになる。という感じのアルゴリズム。
- [Bellman-Ford in 5 minutes | Michael Sambol](https://www.youtube.com/watch?v=obWXjtg0L64)が分かりやすい。

#### ジョンソン法 (Johnson's algorithm)

- グラフに負の重みがある場合、ダイクストラ法が機能しない。
- 重みを変更し、負の重みをなくすことで、ダイクストラ法を機能させる方法。
<!-- TODO -->
- アルゴリズムの教科書では全組最短路のアルゴリズムとして紹介されている。

### 全組最短路 (APSP, All Pairs Shortest Path)

#### 動的計画法による全組最短路

<!-- TODO -->

#### 動的計画法＋分割統治による全組最短路

<!-- TODO -->

#### ワーシャルフロイド法 (Floyd–Warshall Algorithm)

<!-- TODO -->

### グラフの参考

- 12章 グラフ
- 13章 重み付きグラフ
- 15章 高度なグラフアルゴリズム

## 簡潔データ構造 (succinct data structure)

競技プログラミングの基礎的な問題では時間計算量を$O(N^2)$未満に抑えれば良いことが多いが、現実で遺伝子情報などの巨大データを扱うと$O(N)$でも実用的でない。更に空間計算量も問題になる。

そこで、空間計算量を最低限に抑えつつ、時間計算量も$O(N)$未満に抑えよう、というのが簡潔データ構造の動機である。ここで最低限とは、理論値に迫ろう、というくらいのニュアンス。

理論値には「情報理論的下限」という名前がついており、データ本体（簡潔表現 (succinct representation)）とインデックス（簡潔索引 (succinct index)）に対してだいたい次のように定義されている。正確な定義は教科書を参照してください。[^sadakane_2018]

[^sadakane_2018]: [簡潔データ構造](https://amzn.to/3XFVkEb)

- 簡潔表現: データを表すのに必要なビット数が、$log_2(データの種類)$ + 僅かな補助データであること。要するに全パターンにIDを振ってそれを2^nビットで表すと言っている。
- 簡潔索引: 索引を表現するのに用いるビット数がデータ量と同じペースで増えたりしない、つまり$o(lg(n))$（リトルオーであることに注意）である索引。

ちなみに succinct は「サクスィンクト」と発音し、簡潔な、準備ができた、という意味。

### 簡潔データ構造の計算モデル

word-RAMという架空のアーキテクチャのマシンを想定する。特徴は次の通り。

- メモリロケーションあたりの容量が1ビット（通常は8ビット = 1バイト）
- したがって、メモリ全体の容量がUビットの場合、メモリアドレスは $U / 1 = U$ ビット
- 1単位時間でメモリアドレスを指定できるように、CPUのワード長は$log_2(U)$ビット

### ビットベクトル

#### ビットベクトルのrank演算

rankとは、ビットベクトル中の特定の位置までの0または1を数える処理である。シンプルに実装すると時間計算量が$N$になってしまうので、様々な工夫がなされてきた。

まず、全てのビットベクトルに対してその位置までの1の数をあらかじめ数えて表に記録しておくことを考える。時間計算量は表引きに係る分だけになるが、空間計算量が表の行数($N$)×各行の値を表すのに必要なビット数($log_2{N+1}$)となり、簡潔索引の条件を満たさない。

次に、ビットベクトルの$l$ビットごとに累計の1の数をあらかじめ記録することを考える。ここで試しに$l$を$log_2{N}$とすると、簡潔索引の空間計算量は行数($\frac{N}{log_2{N}}$)×値を表すビット数($log_2{N+1}$) = $N$となり、やはり条件を満たさない。もっと広い間隔で記録する必要があるため、$l$を$(log_2{n})^2$とすると、簡潔索引の空間計算量が$\frac{N}{(log_2{N})^2} \cdot log_2{N+1} = \frac{n}{log_2{N}}$となり、これは条件を満たす。この表の1行を教科書では大ブロックという。

<!-- 小ブロックについても時間があればまとめる -->

## 参考

- [プログラミングコンテスト攻略のためのアルゴリズムとデータ構造](https://amzn.to/3QAHDSk)
- [Algorithms | Jeff Erickson (inzkyk訳)](https://booth.pm/ja/items/1633486)
- [アルゴリズムイントロダクション 第3版](https://amzn.to/4avVZdK)
- [アルゴリズム | ともめも](https://www.tomotaku.com/category/algorithm/)
