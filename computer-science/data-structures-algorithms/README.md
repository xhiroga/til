# データ構造, アルゴリズム (data structures and algorithms)

ページ構成の際、次の情報源を参考にした。

- [アルゴリズムイントロダクション 第3版 (CLRS)](https://amzn.to/4avVZdK)
- [プログラミングコンテスト攻略のためのアルゴリズムとデータ構造](https://amzn.to/3QAHDSk)
- [Algorithms | Jeff Erickson (inzkyk訳)](https://booth.pm/ja/items/1633486)
- [アルゴリズム | ともめも](https://www.tomotaku.com/category/algorithm/)
- [Claude🔐](https://claude.ai/chat/faa01c3e-fb05-40f0-9f0e-6a6bb2b80d0f)

## 基礎知識

### 漸近記法

アルゴリズムの実行時間を解析するうえで、私たちは実行時間が入力サイズに対して漸近的に増える効率に興味がある。入力サイズと相対的な実行時間を、おそらく計算量という。

計算量には時間計算量と領域計算量がある。単に計算量といえば、普通時間計算量を指す。領域計算量は空間計算量とも言う。

アルゴリズムの実行時間を表すための、漸近記法をまとめた。

![漸近記法](/images/漸近記法.svg)

- Θ記法, シータ記法 (theta notation)
- 𝑂記法、ビッグオー記法(big-o notation)
  - アルファベットのO (オー)ではなく、ギリシャ文字のΟ (オミクロン)。
  - $f(x)=Ο(g(x))$のとき、$f(x)$が$g(x)$の定数倍と同じかそれより小さいことを指す。
- 𝑜記法, リトルオー記法, スモールオー記法 (little-o notation)
  - $f(x)=ο(g(x))$のとき、$f(x)$が$g(x)$の定数倍より小さいことを指す。
- Ω記法, ビッグオメガ記法 (big-omega notation)
  - $f(x)=Ω(g(x))$のとき、$f(x)$が$g(x)$の定数倍と同じかそれより大きいことを指す。
- 𝜔記法, リトルオメガ記法 (little-omega notation)

なお、Ο (オミクロン)を変換するのが面倒なので、以降はアルファベットのO (オー)を用いる。

## データ構造 (data structures)

### スタック・キュー・両端キュー

先頭または末尾にデータを追加・削除できるデータ構造を次に述べる。

- スタック (stack)
- キュー (queue)
- 両端キュー, デック (deque, double-ended queue)

それぞれのPythonにおける実装とデータ追加・削除の操作をまとめた。（`pop()`するのはスタック？キュー？といつも迷うので）

| データ構造\実装 | list                                     | collections.deque                           | queue.Queue                  |
| --------------- | ---------------------------------------- | ------------------------------------------- |
| スタック        | 追加: append(), 取り出し: pop()          | 追加: append(), 取り出し: pop()             | -                            |
| キュー          | 追加: append(), 取り出し: pop(0)         | 追加: append(), 取り出し: popleft()         | 追加: put(), 取り出し: get() |
| 両端キュー      | 追加: append(), 取り出し: pop() / pop(0) | 追加: append(), 取り出し: pop() / popleft() | -                            |

末尾を取得するのが`pop()`と覚えておけば良さそうだ。なお、先頭を取得するのが`pull()`の言語もあるようだが、Pythonは違う。

### ヒープ

- 最小ヒープは、親が子よりも小さく、根が最小になるヒープ。単にヒープといえば最小ヒープ。
- 最大ヒープはその逆となる。
- 二分ヒープの読みは 「にぶん」ヒープ
- 二分ヒープと優先度付きキューの違いは、（WIP）

### B木

### フィボナッチヒープ

### van Emde Boas木

### Union-Find

互いに素な集合がある時に、ある2つの要素が同じ集合に属しているか？を調べることをUnion-Findという。

集合を木構造で表したとき、同じ集合に属しているか？を調べるのは根を求めることに等しく、計算量が$O(N)$となりえる。

しかし、union by rank を行うことで$O(log N) に、さらに経路圧縮を行うことで$O(α(N))$となる。$α(N)$はアッカーマンの逆関数と呼ばれ、増加がとても遅いことで知られる。[^union_find]

- union by rank: 2つの木を合体させるとき、背の高い木に背の低い木をつなげる
- 経路圧縮: find処理時、中間節点を全て根に繋ぎなおす

[^union_find]: [Union-Find の 2 つの工夫](https://algo-method.com/descriptions/133)

> [!NOTE] Union-Findでfind時に経路圧縮すると、rankと木の高さが一致しないのでは？
> その通り。rankと木の高さが常に等しいとは限らない。rankの代わりに木のサイズを用いても時間計算量は$O(N)$で変わらないため、説明しやすさのためにそちらを用いても良い。[^saka_pon_2022]
[^saka_pon_2022]: [Union-Find の実装方法まとめ](https://qiita.com/saka_pon/items/2f18c84f1b6834e4fe4a#union-by-size)

### ハッシュ表 (hash table)

アルゴリズム図鑑で紹介されているチェーン法の他に、オープンアドレス法による対応がある。オープンアドレス法には、線形走査法とダブルハッシュ法がある。

- チェーン法
- オープンアドレス法 (開番地法)
  - 線形走査法・ダブルハッシュ法に共通して言えるが、いずれはハッシュ表が満杯になる。
  - 線形走査法
    - 衝突が発生した場合、その1つ後ろの位置を格納箇所とする
  - ダブルハッシュ法 (二重ハッシュ法)
    - ハッシュが衝突したら、引数に衝突回数を加えて再度ハッシュを求める。

[ハッシュ探索②（オープンアドレス法） | Programming Place Plus　アルゴリズムとデータ構造編【探索アルゴリズム】　第７章](https://programming-place.net/ppp/contents/algorithm/search/007.html#linear_probing)

> [!NOTE] オープンアドレス法って、ハッシュ値を削除したら、同じハッシュ値で再ハッシュされた値を探せなくならない？
> なる。したがって、ハッシュ値の削除時には空の番地であることを示す値をいれる。この値をTombstone (墓石)という。[kumagi^2023]
[kumagi^2023]: [あなたの知らないハッシュテーブルの世界](https://www.docswell.com/s/kumagi/ZGXXRJ-hash-table-world-which-you-dont-know#p20)

## 基礎的なアルゴリズム (algorithms)

番兵

- `for`ループまたは`while`ループで、探索のための比較以外に終了条件の判定を行うことが面倒であるため、リストの最後に目的のデータをダミーで入れておく。
- Pythonでループを実装する場合や、TypeScriptで`find`などを用いる場合、勝手に最後まで探索してくれるので考慮不要。

### ソート

- 挿入ソート
  - カードの山を右手に取り、1枚引いて引いて左手側に加える、この時左手側の適切な位置に挿入することを繰り返す。
  - 再帰でも書ける。
  - 個人的には左手ソートと呼びたい。
- 選択ソート
  - データの先頭（または後方）が徐々に整理済みになっていく点は、挿入ソートと変わらない。
  - 1番小さい（大きい）値、2番目に小さい（大きい）値…と順番に探していく点と、値が見つかった後は元々n番目にいた値が入れ替わりで飛ばされてしまう点が異なる。
  - この入れ替わりで飛ばされてしまう点が面白いので、個人的には（ダンジョン飯の）ミスルンソートと呼びたい。
- バブルソート
  - 右から左に、小さい値がバトンタッチしながらやってくる様子から、個人的には玉突きソートと呼びたい。
- シェルソート
- マージソート
- クイックソート
  - 再帰的にパーティションを行う。
- 計数ソート
  - [計数ソート | アルゴリズムビジュアル大事典](https://yutaka-watanobe.github.io/star-aida/1.0/algorithms/counting_sort/print.html)を参照。
- 基数ソート

シェルソート

![シェルソート（螺旋本）](/images/シェルソート（螺旋本）.svg)

パーティション

![パーティション](/images/パーティション.svg)

### 探索

#### KMP法

文字列探索とは、文字列Sが単語Wを含む場合に、それが文字列Sの何文字目から何文字目かを調べるタスクである。例えば、S="Hello,world!"、W="world"の場合、6文字目から10文字目が一致する。普通に計算すると、計算量は$O(|S| \cdot |W|)$となる。

KMP法は、一度比較した文字は、なるべく2回比較したくない、という気持ちのアルゴリズムである。例えば次の文字列を考える。

- S: "firefirefighter"
- W: "firefighter"

この文字列は偶然にも先頭から6文字が一致しているので、先頭から比較を始めたKMP法は7文字目で一致しないことに気づく。ここで単純なアルゴリズムなら、次にSの2文字目"i"とWの1文字目"f"を比較するところから始めるだろう。しかしKMP法は、「もうSの6文字目までは比較したから、次はSの7文字目"r"とWの1文字目"f"を比較したい！」と考える。もちろんこれは失敗する！

失敗する原因は、すでに比較した文字列の中に、先頭が一致する他のパターンが含まれているためである。Sの部分文字列"fire**fi**r"の後半の**fi**は、Wの"fighter"だけでなく"fire"とも比較しなければいけないのだ！そこでKMP法では、Wの各文字に対して、「この文字で比較が失敗したら、Wの直前のX文字分と一致していたSの文字列はWの先頭とも一致するはずだから、Sの現在地をX文字巻き戻した箇所とWの先頭を揃えてから再開してね」という数Xを事前に計算しておく。これを部分マッチテーブルという。

### 木の探索

次の4通り。

- 深さ優先探索
  - 行きがけ順
    - 最もシンプル。迷路の右手法と同じ。
  - 通りがけ順
    - 西から順番に印を付けるイメージ。
  - 帰りがけ順
    - 個人的には行き止まり順だと思っている。
- 幅優先探索

## アルゴリズムの設計

### 再帰

### バックトラッキング

- 最長増加部分列
  - ある列に含まれる文字を、順序を崩さぬようにいくつか選んで再構成された列。
  - 列が元の列において連続である場合は「部分文字列」と呼ぶ。
  - 最長増加部分列とは、元のアルファベットなどの文字列の部分列のうち、a,b,c...の順番になっているようなものを指す。

### 動的計画法

- あずぱんさん曰く「表的計画法」。
  - [動的計画法(DP)をみんながわかるように超ていねいにじっくり説明した | あずぱん動画](動的計画法(DP)をみんながわかるように超ていねいにじっくり説明した)

<!-- https://www.mathenachia.blog/dp/ から分かる範囲で理解して言語化する。 -->

次のような問題がある

- 編集距離
- 連鎖行列積
  - なるべく大きい数字を早めに消してしまうのが方針となる。

動的計画法の例として、編集距離の問題を考える。編集距離とは、単語Aを1文字づつ置換・挿入・削除することで、何手で単語Bに変身できるかを指す値である。例えば、`MONEY`を`FOOD`に変身させるにあたって、次の手順では4手で変身でき、同時に2単語間の最短距離でもある。

1. MONEY → FONEY
2. FONEY → FOOEY
3. FOOEY → FOODY
4. FOODY → FOOD

次に、`MONEY`を`LOVE`に変身させる場合で、編集距離の依存関係をフローチャートで示す。[動的計画法（編集距離の依存関係の可視化）](./DP.md)を参照のこと。

#### 最大部分配列問題

最大部分配列とは、正〜負の整数を要素に持つ配列に対して、連続する部分列で和が最大のものを求める問題である。これを解くKadane[^j_kadane]'s algorithm（カデインのアルゴリズム）について、最適な小構造に注目した説明を考えた。
[^j_kadane]: 2024年現在もCMUに勤めておられる。[ご本人の映像](https://www.dailymotion.com/video/xlbpc2)があるので、見るとアルゴリズムに親近感が湧くかもしれない。

正〜負の整数がバランスよく含まれる最大部分配列を2つの部分配列に分けると、どちらの部分配列の和も正になる。それを踏まえたうえで、次の手順で最大部分配列を求める。

1. N文字目から順番に数を足していき、M文字目で和の合計が負になった時、最大部分配列がN文字目から始まるとしたらM−1文字目までのどこかである。
2. N文字目からM文字目までの各段階の和の合計のうち最大のものが、最大部分配列がN文字目から始まる場合の候補となる
3. M文字目で和の合計が負になった時、M+1文字目から再び手順を繰り返す。こうして得られた最大部分配列の候補で決勝トーナメントを行い、最大のものが配列全体の最大部分配列である。

### 貪欲アルゴリズム

## グラフアルゴリズム

### グラフの性質

#### グラフの要素

- 頂点 (vertex), ノード (node)
  - 辺$uv$および$u→v$について、頂点$u$と$v$を endpoint(端点)
    - $u→v$について、$u$をtail(尾), $v$をhead(頭)
  - カット点
  - ソース (source): 有向グラフにおいて、ある頂点に向かう辺がない頂点
  - シンク (sink): 有向グラフにおいて、ある頂点から出る辺がない頂点。シンクは沈む、流すといった意味
  
- 辺 (edge), アーク (arc)
  - 特に有向辺をarcと呼ぶ場合もある
  - 橋

- 列 (sequence): 頂点の列
  - 歩道 (walk): 隣接する頂点からなる列

親、根、木辺などは<#探索木(森)の性質>を参照。

#### グラフの種類

- 辺に向きがあるかどうか
  - 有効グラフ (directed graph)
  - 無向グラフ (undirected graph, unordered graph)

- ループ・多重辺を持っているか
  - simple: ループ・多重辺を持たないグラフ
  - 多重グラフ (multigraph): ループ・多重辺を持つグラフ

- グラフ上のどの頂点からでも、他の頂点に行けるか？
  - 連結グラフ (connected graph)
  - 非連結グラフ (disconnected graph)
  - 強連結 (strongly connected): 有向グラフにおいて、どの頂点からでも他の頂点に行けること

- 閉路を含むか
  - 巡回グラフ (cyclic graph)
  - 非巡回グラフ (acyclic graph)

他に次のような定義がある。

- 森 (forest): 非巡回グラフ
- 木 (tree): 連結非巡回グラフ

#### グラフどうしの関係

グラフ$G=(V,E)$に対して、次のように定義される。

- 部分グラフ (subgraph): 頂点と辺のいずれも$G$に含まれるグラフ
  - 真部分グラフ (proper subgraph): subgraphであって、$G$と等しくないもの
  - 同型 (isomorphic): $G$と$G’$が、それぞれ対応する頂点と辺を持っている時、$G$と$G'$は同型
  - 誘導部分グラフ (induced subgraph): $E'=E∩\binom{V}{2}$、つまり部分グラフ内のすべての頂点は、元のグラフ$G$で持っていた辺を持っている、ということ
  - 全域部分グラフ (spanning subgraph): $V'=V$である部分グラフ。$G-e$や$G-F$で表される
    - 全域木 (spanning tree)
      - 最小全域木 (minimum spanning tree): 全域木のうち、合計の重みが最も軽い木。「すべての地域に電力を届けるための最も安いネットワークは？」といった応用がある。
      - 最短路全域木 (shortest path spanning tree): 単にshortest path tree(最短路木)といってこれを指すことが多い印象。
  - 誘導全域部分グラフ (induced spanning subgraph): 定義する必要なし（それって同型なので）

なぜ誘導グラフと呼ぶかについては、[「生成部分グラフ」という用語について](http://www.co.mi.i.nagoya-u.ac.jp/~yagiura/surijoho8/induced_subgraph.pdf)を参照。

- 逆 (reversal): $G$のすべての辺$u→w$を$w→u$に取り換えたグラフ

- 成分 (component): 極大な連結部分グラフ
  - 強連結成分 (SCC, strongly connected component), 強成分 (strong component)
  - ソース成分 (source component)
  - シンク成分 (sink component)
- 強連結成分グラフ (strong component graph): 強連結成分を1つの頂点にまとめたグラフ

### グラフの問題

- 到達可能性
- 成分の検出
- 関節点 (articulation)の検出
- 最長路 (longest path)
- SSSP(Single Source Shortest Path, 単一始点最短経路、または単に最短路)
- APSP(All Pair Shortest Path, 全点対最短経路)

### グラフのデータ構造

- 隣接リスト
  - メモリが少ない（＝文字数が少なく済む）性質から、競プロでは、グラフは隣接リストに近い形式で渡されることが多い。
- 隣接行列

### 何か優先探索

グラフの探索には、深さ優先や幅優先などいろいろあるが、実は袋（＝候補となる辺を入れておくデータ構造）に何を採用するかの違いに過ぎない。

| 名前         | 袋                               | 解決する問題 |
| ------------ | -------------------------------- | ------------ |
| 深さ優先探索 | スタック                         |              |
| 幅優先探索   | キュー                           |              |
| 最良優先探索 | 優先度付きキュー                 |              |
| 最良優先探索 | 優先度付きキュー（辺の重み）     | 最小全域木   |
| 最良優先探索 | 優先度付きキュー（辺の重みの和） | 最短路木     |
| 最良優先探索 | 優先度付きキュー（路の幅）       | 最幅路       |

#### 探索木(森)の性質

- root(根)
- parent(親)
- tree edge(木辺): ?
- forward edge(前方辺): ?
- backward edge(後方辺): ?
- cross edge(交差辺): ?

### トポロジカルソート (topological sort)

- 自分なりに言い換えると「左手法」。迷路の左手法と考え方が同じだから。
- 左手法の要領でグラフを探索し、そこから先は行き止まり、というところまで行ったら現在地をリストに記録する。
- 行き止まりから引き返すごとに、そこから先が行き止まりなら現在地をリストに記録するし、分岐があれば分岐に進む。
- [Topological Sort Visualized and Explained | Carl the Person](https://www.youtube.com/watch?v=7J3GadLzydI)が短くて分かりやすい。

### 強連結成分 (strong component), 関節点 (articulation)

強連結成分を計算するためのアルゴリズムは次の通り。線形時間で計算するための工夫がポイント。

- グラフの頂点全てに対して、お互いに到達できる点を何か優先探索で計算する（$O(VE)$）
- Kosaraju(コサラジュ)とSharir(シャリア)のアルゴリズム
  - 逆グラフの帰りがけ順に辿った頂点を、さらに逆順に何か優先探索する時に、何か優先探索がシンク成分の内側に留まることを利用したアルゴリズム
  <!-- TODO - 単にグラフの帰りがけ順ではない -->
- Tarjan(タージャン)のアルゴリズム
  <!-- TODO -->

<!-- TODO 関節点を求めるのに用いるのもTarjanやKosaraju? -->

### 最小全域木 (MST, Minimum Spanning Tree) アルゴリズム

いくつもアルゴリズムがあるが、次に述べる戦略の実装違いといえる。

その戦略とは、入力グラフ$G$に対して、その頂点だけからなる$F$（つまり、$F=(V, ∅)$）をintermediate spanning forest(中間全域森)として定義し、$F$に*いい感じ*の辺を徐々に加えていく戦略である。

プリム法とクラスカル法が有名である。グラフが密な場合に限ってはプリム法が早いが、それ以外は実装が容易なクラスカル法を用いれば良い。

#### ブルーフカ法 (Borůvka's algorighm)

<!-- 解説 -->

```python
def boruvka(V, E):
  F = (V, ∅)
  count = count_and_label(F) # 成分数
  while count > 1:
    add_all_safe_edges(E, F, count)
    count ← count_and_label(F)
  return F
```

#### プリム法 (Jarník's (Prim's) algorithm)

任意の頂点をスタート地点として、接続する中で重みが最小かつ閉路を作らない辺を付け加えることを繰り返す。グラフが隣接リストで表現されている場合、計算量はO(ElogV)となる。一方で隣接行列を用いた場合の計算量はO(V^2)となるため、辺に比べて頂点が少ない場合に特に高速になる。

[プリム法 (Prim's Algorithm) | サルでもわかるアルゴリズム](https://www.youtube.com/watch?v=anuJPP3FZ8c)が分かりやすかった。

#### クラスカル法 (Kruskal's algorithm)

すべての辺の中から、重みが最小かつ付け加えても閉路にならない辺を選ぶ。このようにして選んだ辺の集合が最小全域木となる。

閉路の検出はUnion−Findを用いて高速に行うため、計算量はほぼ辺のソートにかかると考えてよく、計算量はO(ElogE)となる。

### 最短路問題 (shortest path problem)

重み付きグラフにおいて、2つの頂点を結ぶ経路の中で、重みが最小の経路を求める問題。次の種類がある。

- 2頂点対最短経路問題 (single-pair shortest path problem)
- 単一始点最短経路問題 (SSSP, single-source shortest path problem)
- 単一終点最短経路問題 (SDSP, single-destination shortest path problem)
- 全点対最短経路問題 (APSP, all-pairs shortest path problem)

#### ダイクストラ法 (Dijkstra's algorithm)

- 自分なりに言い換えると、「近さランキング法」。
- 単一始点最短経路を求めるのに用いる。計算量は$O(|V|^2)$になる。
- 隣接リストと優先度付きキューを用いると、計算量は$O((|V|+|E|)log|V|)$になる。
  - [Dijkstra’s Algorithm | Depth First](https://www.youtube.com/watch?v=NyrHRNiRpds)が非常に分かりやすい。
  - ウィザードリィのような主観視点で例えると次の通り。
    1. 今いる場所（＝頂点）から見える場所について、1つづつ距離を書き留め、探索候補（優先度付きキュー）に加える。
    2. 全て書き留めたら、最も近い場所を候補から選んで進む。
    3. 同じく、進んだ先から見える場所について、1つづつ距離を測る。その時、すでに書き留めた場所だったなら、今いる場所を経由した方が近道である場合に限って書き留め、探索候補に加える。
    4. 書き留めたら、またスタートに戻ってから、次に近い場所に進む。これを場所の数繰り返す（注）
  - 注: 同じ場所を2回訪問することはない。言い換えると、探索候補の中で一番近い場所だけは、後からより近いルートが見つかることは無い。
- 螺旋本には、隣接リストと二分ヒープを用いると、計算量は$O((|V|+|E|)log|V|)$になる、とあるが...正直言って優先度付きキューでの実装より複雑だし計算量も変わらないので、私は理解していません...。

> [!NOTE] 実世界でのダイクストラ法の応用を知りたい

#### ベルマンフォード法 (Bellman Ford's algorithm)

始点から辺の数がn以内でたどり着ける頂点について、n=1,2,3...|V|-1まで順番に最短経路を探す手法。[^sambol_2015]v-1回目のループの後に再度最短経路を探し、距離が更新されれば負の閉路があると判断できる。最短路は閉路を含まないため、その長さは|V|-1となる。負閉路の検出も行うため、計算量はO(|V|*|E|)となる。
[^sambol_2015]: [Bellman-Ford in 5 minutes | Michael Sambol](https://www.youtube.com/watch?v=obWXjtg0L64)が分かりやすい。

#### ジョンソン法 (Johnson's algorithm)

- グラフに負の重みがある場合、ダイクストラ法が機能しない。
- 重みを変更し、負の重みをなくすことで、ダイクストラ法を機能させる方法。
<!-- TODO -->
- アルゴリズムの教科書では全組最短路のアルゴリズムとして紹介されている。

#### 動的計画法による全組最短路

<!-- TODO -->

#### 動的計画法＋分割統治による全組最短路

<!-- TODO -->

#### ワーシャルフロイド法 (Floyd–Warshall Algorithm)

<!-- TODO -->

## 計算複雑性 (computational complexity)

### 複雑性クラス (complexity class)

入力サイズ$n$, 定数$k$について、計算量が$O(n^k)$で抑えられるアルゴリズムを多項式時間アルゴリズムという。計算量が$O(n^{k^k})$や$O(k^n)$の場合はいわない。

多項式時間で解ける問題は、複雑性クラスPに属しているという。逆に、多項式時間で解けない問題をNP困難 (NP-Hard)という。また、多項式時間で解けるかどうかに関わらず、多項式時間で検証できる問題をNPという。そして、NP困難かつNP、つまり多項式時間では解けないが、解の検証はできる問題をNP完全 (NP-Complete)という。多くの場合、NP完全の証明では、すでにNP完全であることが証明されている問題に多項式時間で変換できることを示す。

よく言われるP≠NPとは、PがNPの真部分集合である、つまり多項式時間では解けないが解の検証はできる問題が存在する、という命題である。

### ヒューリスティック探索 (heuristic search)

本章の執筆にあたってヒューリスティック探索入門[^jinnaiyuu_2023]を参考にした。
[^jinnaiyuu_2023]: [ヒューリスティック探索入門](https://jinnaiyuu.github.io/pdf/textbook.pdf)

#### 状態空間問題 (state-space problem)

ヒューリスティック探索のベンチマークとして、状態空間問題がある。これは初期状態とゴール条件が与えられ、それを満たす経路を探索する問題である。

よく知られたNP困難な問題として巡回セールスパーソン問題 (TSP, Traveling Salesperson Problem)がある。地図上の全ての地点を通りつつ最初の地点に戻る最短経路を求める問題である。最適な経路はその部分経路も最適だから動的計画法で解けるが、その計算量は$O(n\cdot 2^n)$など指数時間になるため、NP困難である。

状態空間問題は完全情報であり、状態遷移が決定的であることを仮定した。状態遷移が確率的である問題はマルコフ決定過程 (MDP, Markov Decision Process)という。

#### ヒューリスティック探索のアルゴリズム

ダイクストラ法では、スタート地点から近い地点を順に探索する。しかし、例えばゴールまでの直線距離が分かったら、もっと効率的な探索ができないか？このように、ノードの有望さを定量化する関数をヒューリスティック関数 (heuristic function)といい、それを用いたアルゴリズムをヒューリスティック探索という。逆に言えば、有望さの見積もりを用いない探索アルゴリズムは、ヒューリスティック探索が常に定数を返す特別な場合とみなせる。

##### A*法 (A-star algorithm)

## 簡潔データ構造 (succinct data structure)

競技プログラミングの基礎的な問題では時間計算量を$O(N^2)$未満に抑えれば良いことが多いが、現実で遺伝子情報などの巨大データを扱うと$O(N)$でも実用的でない。更に空間計算量も問題になる。

そこで、空間計算量を最低限に抑えつつ、時間計算量も$O(N)$未満に抑えよう、というのが簡潔データ構造の動機である。ここで最低限とは、理論値に迫ろう、というくらいのニュアンス。

理論値には「情報理論的下限」という名前がついており、データ本体（簡潔表現 (succinct representation)）とインデックス（簡潔索引 (succinct index)）に対してだいたい次のように定義されている。正確な定義は教科書を参照してください。[^sadakane_2018]

[^sadakane_2018]: [簡潔データ構造](https://amzn.to/3XFVkEb)

- 簡潔表現: データを表すのに必要なビット数が、$log_2(データの種類)$ + 僅かな補助データであること。要するに全パターンにIDを振ってそれを2^nビットで表すと言っている。
- 簡潔索引: 索引を表現するのに用いるビット数がデータ量と同じペースで増えたりしない、つまり$o(lg(n))$（リトルオーであることに注意）である索引。

ちなみに succinct は「サクスィンクト」と発音し、簡潔な、準備ができた、という意味。

### 簡潔データ構造の計算モデル

word-RAMという架空のアーキテクチャのマシンを想定する。特徴は次の通り。

- メモリロケーションあたりの容量が1ビット（通常は8ビット = 1バイト）
- したがって、メモリ全体の容量がUビットの場合、メモリアドレスは $U / 1 = U$ ビット
- 1単位時間でメモリアドレスを指定できるように、CPUのワード長は$log_2(U)$ビット

### ビットベクトル

#### ビットベクトルのrank演算

rankとは、ビットベクトル中の特定の位置までの0または1を数える処理である。シンプルに実装すると時間計算量が$N$になってしまうので、様々な工夫がなされてきた。

まず、全てのビットベクトルに対してその位置までの1の数をあらかじめ数えて表に記録しておくことを考える。時間計算量は表引きに係る分だけになるが、空間計算量が表の行数($N$)×各行の値を表すのに必要なビット数($log_2{N+1}$)となり、簡潔索引の条件を満たさない。

次に、ビットベクトルの$l$ビットごとに累計の1の数をあらかじめ記録することを考える。ここで試しに$l$を$log_2{N}$とすると、簡潔索引の空間計算量は行数($\frac{N}{log_2{N}}$)×値を表すビット数($log_2{N+1}$) = $N$となり、やはり条件を満たさない。もっと広い間隔で記録する必要があるため、$l$を$(log_2{n})^2$とすると、簡潔索引の空間計算量が$\frac{N}{(log_2{N})^2} \cdot log_2{N+1} = \frac{n}{log_2{N}}$となり、これは条件を満たす。この表の1行を教科書では大ブロックという。

<!-- 小ブロックについても時間があればまとめる -->
