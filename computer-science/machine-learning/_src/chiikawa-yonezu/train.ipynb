{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_len=512):\n",
    "        self.dataframe = pd.read_csv(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataframe.iloc[idx, 0]\n",
    "        label = 1 if self.dataframe.iloc[idx, 1] == '米津玄師' else 0\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = inputs['input_ids'].flatten()\n",
    "        attention_mask = inputs['attention_mask'].flatten()\n",
    "        return input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "dataset = TextDataset('data/train.csv', tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassifierModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "        self.linear = torch.nn.Linear(768, 2)  # BERTの隠れ層の次元数と出力クラス数\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs[0]\n",
    "        pooled_output = last_hidden_state[:, 0]\n",
    "        return self.linear(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'device: cuda'\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_cuda else 'cpu')\n",
    "pprint(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(device, *args):\n",
    "    return [x.to(device) for x in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\hiroga\\miniconda3\\envs\\chiikawa-yonezu\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "epoch = 3 # for debug\n",
    "net: torch.nn.Module = ClassifierModel().to(device)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "for e in range(epoch):\n",
    "    for input_ids, attention_mask, labels in loader:\n",
    "        input_ids, attention_mask, labels = to_device(device, input_ids, attention_mask, labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(input_ids, attention_mask)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hiroga\\miniconda3\\envs\\chiikawa-yonezu\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'テキスト, 正解ラベル, 予測ラベル'\n",
      "泣 いても 涙 がでないや ちいかわ 米津玄師\n",
      "お 前 になんかやるもんか 米津玄師 米津玄師\n",
      "守 りたいんだ みんなが 戻 ってくるまで ちいかわ 米津玄師\n",
      "この 像 に 誓 ったんだ 強 くなると ちいかわ 米津玄師\n",
      "どうしてどうしてどうして 米津玄師 米津玄師\n",
      "難 解 なパズルみたい ちいかわ 米津玄師\n",
      "そこから 見 ていてね 大 丈 夫 ありがとう 米津玄師 米津玄師\n",
      "ヤンパパン ラララルルラ ちいかわ ちいかわ\n",
      "ヒッピヒッピシェイク ダンディダンディドン 米津玄師 ちいかわ\n",
      "るるらったったったった 米津玄師 米津玄師\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    dataset = TextDataset('data/test.csv', tokenizer)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    for input_ids, attention_mask, labels in loader:\n",
    "        input_ids, attention_mask, labels = to_device(device, input_ids, attention_mask, labels)\n",
    "        outputs = net(input_ids, attention_mask)\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        decoded = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "        label_text = ['米津玄師' if l == 1 else 'ちいかわ' for l in labels]\n",
    "        predicted_text = ['米津玄師' if l == 1 else 'ちいかわ' for l in predicted]\n",
    "        zipped = zip(decoded, label_text, predicted_text)\n",
    "        pprint(\"テキスト, 正解ラベル, 予測ラベル\")\n",
    "        for d, l, p in zipped:\n",
    "            print(f'{d} {l} {p}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-mnist-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
