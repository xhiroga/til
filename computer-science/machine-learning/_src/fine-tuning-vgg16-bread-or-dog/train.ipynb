{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save images from below websites with Firefox.\n",
    "- https://www.dreamstime.com/photos-images/corgi-butt.html\n",
    "- https://www.pinterest.com/I_love_Corgi/corgi-butt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hiroga\\miniconda3\\envs\\fine-tuning-vgg16-bread-or-dog\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "stanford_dogs_dataset = load_dataset(\"Alanox/stanford-dogs\", split=\"full\", trust_remote_code=True)\n",
    "# OR !kaggle datasets download -d jessicali9530/stanford-dogs-dataset -p \"data\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['name', 'annotations', 'target', 'image'],\n",
       "    num_rows: 20580\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stanford_dogs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "bread_dataset = load_dataset(\"imagefolder\", data_dir=\"data/images.cv_fg0xp9w733695pvws1a4yh/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 1478\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 240\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 738\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bread_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bedlington Terrier', 'Clumber', 'Bluetick', 'German Short Haired Pointer', 'Labrador Retriever', 'Bernese Mountain Dog', 'Saluki', 'German Shepherd', 'Komondor', 'Kuvasz', 'Weimaraner', 'Great Pyrenees', 'Rottweiler', 'Pekinese', 'Gordon Setter', 'Tibetan Terrier', 'Soft Coated Wheaten Terrier', 'Brittany Spaniel', 'Leonberg', 'English Foxhound', 'Collie', 'Basset', 'Wire Haired Fox Terrier', 'Norwegian Elkhound', 'Chesapeake Bay Retriever', 'Cardigan', 'Borzoi', 'Border Collie', 'Malamute', 'Australian Terrier', 'Silky Terrier', 'Affenpinscher', 'Pomeranian', 'American Staffordshire Terrier', 'Otterhound', 'Staffordshire Bullterrier', 'West Highland White Terrier', 'Boston Bull', 'Redbone', 'Irish Water Spaniel', 'Giant Schnauzer', 'Flat Coated Retriever', 'Norwich Terrier', 'Dhole', 'Airedale', 'Miniature Poodle', 'Malinois', 'Sealyham Terrier', 'Cairn', 'Eskimo Dog', 'Siberian Husky', 'Papillon', 'Greater Swiss Mountain Dog', 'Sussex Spaniel', 'African Hunting Dog', 'Pembroke', 'Dingo', 'Appenzeller', 'Irish Setter', 'Kelpie', 'Brabancon Griffon', 'Groenendael', 'Norfolk Terrier', 'Lakeland Terrier', 'Italian Greyhound', 'Great Dane', 'Yorkshire Terrier', 'Miniature Schnauzer', 'Dandie Dinmont', 'Maltese Dog', 'Border Terrier', 'Rhodesian Ridgeback', 'Blenheim Spaniel', 'Miniature Pinscher', 'Japanese Spaniel', 'Afghan Hound', 'Toy Poodle', 'Old English Sheepdog', 'Doberman', 'Golden Retriever', 'Samoyed', 'Standard Schnauzer', 'Ibizan Hound', 'Mexican Hairless', 'Bouvier Des Flandres', 'Shih Tzu', 'Irish Terrier', 'Standard Poodle', 'Cocker Spaniel', 'Pug', 'Walker Hound', 'Bull Mastiff', 'Toy Terrier', 'Chihuahua', 'Beagle', 'Newfoundland', 'Black And Tan Coonhound', 'Welsh Springer Spaniel', 'Kerry Blue Terrier', 'French Bulldog', 'Tibetan Mastiff', 'English Setter', 'Boxer', 'Curly Coated Retriever', 'Irish Wolfhound', 'Shetland Sheepdog', 'Briard', 'Bloodhound', 'Saint Bernard', 'Whippet', 'Basenji', 'English Springer', 'Scotch Terrier', 'Entlebucher', 'Scottish Deerhound', 'Lhasa', 'Vizsla', 'Keeshond', 'Schipperke', 'Chow']\n"
     ]
    }
   ],
   "source": [
    "unique_targets = stanford_dogs_dataset.unique('target')\n",
    "print(unique_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n"
     ]
    }
   ],
   "source": [
    "# もしコーギー(Pembroke)だけで数百件あればそれを使い、なければ犬の画像すべてを使う\n",
    "pembroke_count = sum(target == 'Pembroke' for target in stanford_dogs_dataset['target'])\n",
    "print(pembroke_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20580/20580 [00:02<00:00, 8794.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Add a new column 'dog_or_bread' to the stanford_dogs_dataset and bread_dataset\n",
    "bread_dataset = bread_dataset.map(lambda example: {'bread_or_dog': 0})\n",
    "stanford_dogs_dataset = stanford_dogs_dataset.map(lambda example: {'bread_or_dog': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['name', 'annotations', 'target', 'image', 'bread_or_dog'],\n",
       "    num_rows: 20580\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stanford_dogs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['name', 'annotations', 'target', 'image', 'bread_or_dog'],\n",
       "        num_rows: 16464\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['name', 'annotations', 'target', 'image', 'bread_or_dog'],\n",
       "        num_rows: 2058\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['name', 'annotations', 'target', 'image', 'bread_or_dog'],\n",
       "        num_rows: 2058\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "train_test_dataset = stanford_dogs_dataset.train_test_split(test_size=0.2)\n",
    "test_valid_dataset = train_test_dataset[\"test\"].train_test_split(test_size=0.5)\n",
    "stanford_dogs_dataset_dict = DatasetDict({\n",
    "    \"train\": train_test_dataset[\"train\"],\n",
    "    \"test\": test_valid_dataset[\"train\"],\n",
    "    \"validation\": test_valid_dataset[\"test\"]\n",
    "})\n",
    "stanford_dogs_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['name', 'annotations', 'target', 'image', 'bread_or_dog', 'label'],\n",
      "        num_rows: 17942\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['name', 'annotations', 'target', 'image', 'bread_or_dog', 'label'],\n",
      "        num_rows: 2298\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['name', 'annotations', 'target', 'image', 'bread_or_dog', 'label'],\n",
      "        num_rows: 2796\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Concatenate the datasets for each split\n",
    "merged_train_dataset = concatenate_datasets([stanford_dogs_dataset_dict['train'], bread_dataset['train']])\n",
    "merged_validation_dataset = concatenate_datasets([stanford_dogs_dataset_dict['validation'], bread_dataset['validation']])\n",
    "merged_test_dataset = concatenate_datasets([stanford_dogs_dataset_dict['test'], bread_dataset['test']])\n",
    "merged_dataset = DatasetDict({\n",
    "    \"train\": merged_train_dataset,\n",
    "    \"validation\": merged_validation_dataset,\n",
    "    \"test\": merged_test_dataset\n",
    "})\n",
    "\n",
    "print(merged_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.vgg16()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='features.0.weight'\n",
      "name='features.0.bias'\n",
      "name='features.2.weight'\n",
      "name='features.2.bias'\n",
      "name='features.5.weight'\n",
      "name='features.5.bias'\n",
      "name='features.7.weight'\n",
      "name='features.7.bias'\n",
      "name='features.10.weight'\n",
      "name='features.10.bias'\n",
      "name='features.12.weight'\n",
      "name='features.12.bias'\n",
      "name='features.14.weight'\n",
      "name='features.14.bias'\n",
      "name='features.17.weight'\n",
      "name='features.17.bias'\n",
      "name='features.19.weight'\n",
      "name='features.19.bias'\n",
      "name='features.21.weight'\n",
      "name='features.21.bias'\n",
      "name='features.24.weight'\n",
      "name='features.24.bias'\n",
      "name='features.26.weight'\n",
      "name='features.26.bias'\n",
      "name='features.28.weight'\n",
      "name='features.28.bias'\n",
      "name='classifier.0.weight'\n",
      "name='classifier.0.bias'\n",
      "name='classifier.3.weight'\n",
      "name='classifier.3.bias'\n",
      "name='classifier.6.weight'\n",
      "name='classifier.6.bias'\n"
     ]
    }
   ],
   "source": [
    "for name, _param in model.named_parameters():\n",
    "    print(f\"{name=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "\n",
    "def train(model, criterion, optimizer, dataloaders_dict, num_epochs, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-------------')\n",
    "        \n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            epoch_corrects = 0\n",
    "            \n",
    "            for images, bread_or_dog in dataloaders_dict[phase]:\n",
    "                images, bread_or_dog = images.to(device), bread_or_dog.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 学習時のみ勾配を計算させる設定にする\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(images)\n",
    "                    \n",
    "                    # 損失を計算\n",
    "                    loss = criterion(outputs, bread_or_dog)\n",
    "                    \n",
    "                    # ラベルを予測\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # イテレーション結果の計算\n",
    "                    # lossの合計を更新\n",
    "                    # PyTorchの仕様上各バッチ内での平均のlossが計算される。\n",
    "                    # データ数を掛けることで平均から合計に変換をしている。\n",
    "                    # 損失和は「全データの損失/データ数」で計算されるため、\n",
    "                    # 平均のままだと損失和を求めることができないため。\n",
    "                    epoch_loss += loss.item() * image.size(0)\n",
    "                    \n",
    "                    # 正解数の合計を更新\n",
    "                    epoch_corrects += torch.sum(preds == bread_or_dog.data)\n",
    "\n",
    "            # epochごとのlossと正解率を表示\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "            log = {\n",
    "                \"epoch\": epoch +1,\n",
    "                \"phase\": phase,\n",
    "                \"loss\": epoch_loss,\n",
    "                \"acc\": epoch_acc,\n",
    "            }\n",
    "            print(log)\n",
    "            wandb.log(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/17942 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'JpegImageFile' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m     examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [composed(img) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m examples\n\u001b[1;32m---> 32\u001b[0m merged_dataset_train_resized \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m merged_dataset_validation_resized \u001b[38;5;241m=\u001b[39m merged_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(transform)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# set_format は set_transform をリセットするらしい。\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# [python - datasets.Dataset.set_transform() doesn't seem to apply transformations to images - Stack Overflow](https://stackoverflow.com/questions/76965662/datasets-dataset-set-transform-doesnt-seem-to-apply-transformations-to-images)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# merged_dataset['train'].set_format(type='torch', columns=['image', 'bread_or_dog'])\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# merged_dataset['validation'].set_format(type='torch', columns=['image', 'bread_or_dog'])\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Assuming that the datasets 'train' and 'validation' are available in the dataloaders_dict\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hiroga\\miniconda3\\envs\\fine-tuning-vgg16-bread-or-dog\\Lib\\site-packages\\datasets\\arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hiroga\\miniconda3\\envs\\fine-tuning-vgg16-bread-or-dog\\Lib\\site-packages\\datasets\\arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    556\u001b[0m }\n\u001b[0;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hiroga\\miniconda3\\envs\\fine-tuning-vgg16-bread-or-dog\\Lib\\site-packages\\datasets\\arrow_dataset.py:3105\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3101\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3102\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3103\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3104\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3105\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3106\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3107\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32mc:\\Users\\hiroga\\miniconda3\\envs\\fine-tuning-vgg16-bread-or-dog\\Lib\\site-packages\\datasets\\arrow_dataset.py:3458\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3456\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3457\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[1;32m-> 3458\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3459\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[0;32m   3460\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hiroga\\miniconda3\\envs\\fine-tuning-vgg16-bread-or-dog\\Lib\\site-packages\\datasets\\arrow_dataset.py:3361\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[0;32m   3360\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[1;32m-> 3361\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3363\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3364\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[0;32m   3365\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[44], line 29\u001b[0m, in \u001b[0;36mtransform\u001b[1;34m(examples)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(examples):\n\u001b[1;32m---> 29\u001b[0m     examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mcomposed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m examples\n",
      "\u001b[1;31mTypeError\u001b[0m: 'JpegImageFile' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Fine Tuning VGG16\n",
    "import torch\n",
    "import wandb\n",
    "from safetensors.torch import save_file\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model_name = \"vgg16\"\n",
    "model.classifier[6] = torch.nn.Linear(in_features=4096, out_features=2)\n",
    "\n",
    "features = [param for param in model.named_parameters() if \"features\" in param]\n",
    "classifier = [param for param in model.named_parameters() if \"classifier.0\" in param or \"classifier.3\" in param]\n",
    "last_classifier = [param for param in model.named_parameters() if \"classifier.6\" in param]\n",
    "param_groups = [\n",
    "    {'params': features, 'lr': 1e-4},\n",
    "    {'params': classifier, 'lr': 5e-4},\n",
    "    {'params': last_classifier, 'lr': 1e-3},\n",
    "]\n",
    "momentum = 0.9\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# torchvision の datasets とは違い、transforms をそのままセットすれば良いわけではないので留意。\n",
    "composed = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "])\n",
    "def transform(examples):\n",
    "    examples['image'] = [composed(img) for img in examples['image']]\n",
    "    return examples\n",
    "\n",
    "merged_dataset_train_resized = merged_dataset['train'].map(transform)\n",
    "merged_dataset_validation_resized = merged_dataset['validation'].map(transform)\n",
    "\n",
    "# set_format は set_transform をリセットするらしい。\n",
    "# [python - datasets.Dataset.set_transform() doesn't seem to apply transformations to images - Stack Overflow](https://stackoverflow.com/questions/76965662/datasets-dataset-set-transform-doesnt-seem-to-apply-transformations-to-images)\n",
    "# merged_dataset['train'].set_format(type='torch', columns=['image', 'bread_or_dog'])\n",
    "# merged_dataset['validation'].set_format(type='torch', columns=['image', 'bread_or_dog'])\n",
    "\n",
    "# Assuming that the datasets 'train' and 'validation' are available in the dataloaders_dict\n",
    "train_dataloader = DataLoader(merged_dataset_train_resized, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(merged_dataset_validation_resized, batch_size=batch_size, shuffle=False)\n",
    "dataloaders_dict = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"validation\": valid_dataloader\n",
    "}\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "wandb.init(\n",
    "    project=\"bread-or-dog\",\n",
    "    config={\n",
    "        \"model_name\": model_name,\n",
    "        \"architecture\": \"CNN\",\n",
    "        \"dataset\": [\"Alanox/stanford-dogs\", \"images.cv_fg0xp9w733695pvws1a4yh\"],\n",
    "        \"param_groups\": param_groups,\n",
    "        \"num_epoch\": num_epochs,\n",
    "        \"momentum\": momentum,\n",
    "    }\n",
    ")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(param_groups, momentum=momentum)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "train(model, optimizer, criterion, dataloaders_dict, num_epochs=num_epochs, device=device)\n",
    "\n",
    "save_file(model.state_dict(), f\"../models/{model_name}_epoch{num_epochs}.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fune-tuning-vgg16-bread-or-dog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
