{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7章 畳み込みニューラルネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "sys.path.append(f\"{os.pardir}/deep-learning-from-scratch\")\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from beartype import beartype\n",
    "from collections import OrderedDict\n",
    "from nptyping import *\n",
    "from typing import Any\n",
    "from common.layers import SoftmaxWithLoss\n",
    "from common.util import im2col,col2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(10,1,28,28)\n",
    "x.shape\n",
    "# (10, 1, 28, 28) ミニバッチ, チャンネル, 高さ, 幅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.rand(1,3,7,7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)   # (9, 75) フィルターの適用領域の数(=1*(((7-5)/1)+1)**2), 入力特徴マップの要素数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.29071762 0.36059174 0.06859933 0.72546234 0.05395164 0.80230681\n",
      "  0.19251097]\n",
      " [0.17624593 0.04464497 0.58770625 0.23806146 0.78852219 0.46851595\n",
      "  0.17487459]\n",
      " [0.77161536 0.85546382 0.70683669 0.94498593 0.29982821 0.41599263\n",
      "  0.24431456]\n",
      " [0.62169088 0.62845354 0.82541094 0.86526018 0.23798586 0.50953767\n",
      "  0.49000122]\n",
      " [0.44169407 0.56941924 0.0515174  0.64070452 0.03580293 0.37490072\n",
      "  0.20375657]\n",
      " [0.84952834 0.86074655 0.03138799 0.90464859 0.43669989 0.76119287\n",
      "  0.83416915]\n",
      " [0.86807934 0.54002743 0.39898301 0.14993485 0.68205731 0.70570936\n",
      "  0.27732789]]\n",
      "[0.29071762 0.36059174 0.06859933 0.72546234 0.05395164 0.17624593\n",
      " 0.04464497 0.58770625 0.23806146 0.78852219 0.77161536 0.85546382\n",
      " 0.70683669 0.94498593 0.29982821 0.62169088 0.62845354 0.82541094\n",
      " 0.86526018 0.23798586 0.44169407 0.56941924 0.0515174  0.64070452\n",
      " 0.03580293 0.89852111 0.05393244 0.9003714  0.3464487  0.81096756\n",
      " 0.16638271 0.51134911 0.42585963 0.31034956 0.46406704 0.76090651\n",
      " 0.908755   0.89015204 0.19151237 0.94458086 0.83648333 0.50363856\n",
      " 0.27011438 0.11579034 0.10898955 0.50794167 0.9412699  0.75130347\n",
      " 0.6712071  0.1405726  0.34476833 0.58860507 0.92148878 0.20170427\n",
      " 0.70510831 0.70566588 0.13416478 0.08232271 0.01054177 0.95390034\n",
      " 0.17526805 0.39407222 0.57500087 0.36085492 0.90072334 0.04969895\n",
      " 0.70893063 0.55732469 0.69103193 0.63570247 0.43560031 0.280244\n",
      " 0.66997114 0.2671913  0.27569717]\n"
     ]
    }
   ],
   "source": [
    "print(x1[0][0])\n",
    "print(col1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "x2 = np.random.rand(10,3,7,7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)   # (90, 75) フィルターの適用領域の数(=10*(((7-5)/1)+1)**2), 入力特徴マップの要素数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_size(x_len, pad, filter_len, stride) -> int:\n",
    "    rest = ((x_len + 2 * pad) - filter_len)\n",
    "    if rest < 0:\n",
    "        raise Exception(f\"Filter length {filter_len} is longer than input size {x_len} + {pad}!\")\n",
    "    elif rest % stride != 0:\n",
    "        raise Exception(f\"Rest length {rest} and stride {stride} are conflicted!\")\n",
    "    else:\n",
    "        return int(rest / stride + 1)  # reshapeにも使うのでintにする\n",
    "\n",
    "class Convolution:\n",
    "    # 今回はフィルターのチャンネル数を3で固定している\n",
    "    @beartype\n",
    "    def __init__(self, W: NDArray[Shape['FN,C,FH,FW'],Float], b: NDArray[Shape['FN'], Float], stride=1, pad=0):\n",
    "        # FN: Filter Number\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        # 中間データ（backward時に使用）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 重み・バイアスパラメータの勾配\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    @beartype\n",
    "    def forward(self, x: NDArray[Shape['N,C,H,W'],Any]):\n",
    "        # Float32が送られてくる。コード全体を通してFloat64の場合もあり使い分けがよく分からないので、いったん型は指定しない。\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = output_size(H, self.pad, FH, self.stride)\n",
    "        out_w = output_size(W, self.pad, FW, self.stride)\n",
    "\n",
    "        col: NDArray[Shape['N*Out_h*Out_w,C*FH*FW'], Float] = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W: NDArray[Shape['C*FH*FW,FN'], Float] = self.W.reshape(FN, -1).T\n",
    "        out: NDArray[Shape['N*Out_h*Out_w,FN', Float]] = np.dot(col, col_W) + self.b\n",
    "        reshaped_out: NDArray[Shape['N,FN,Out_h,Out_w'], Float] = out.reshape(N, out_h, out_w, FN).transpose(0,3,1,2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return reshaped_out\n",
    "\n",
    "    @beartype\n",
    "    def backward(self, dout: NDArray[Shape['N,FN,Out_h,Out_w'], Float]):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout_matrix: NDArray[Shape['N*Out_h*Out_w,FN'], Float] = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db: NDArray[Shape['1,N'], Float] = np.sum(dout_matrix, axis=0)\n",
    "        dW_matrix: NDArray[Shape['C*FH*FW,FN'], Float] = np.dot(self.col.T, dout_matrix)\n",
    "        self.dW: NDArray[Shape['FN,C,FH,FW']] = dW_matrix.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol: NDArray[Shape['N*Out_h*Out_w,C*FH*FN'], Float] = np.dot(dout_matrix, self.col_W.T)\n",
    "        dx: NDArray[Shape['N,C,H,W']] = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=2, pad=0):\n",
    "        # pool_h, pool_wはそれぞれプーリング適用領域の高さ・幅。例えば3x3=9からmaxを取るなら、pool_h=3, pool_w=3\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        self.x = None\n",
    "        self.arg_max: NDArray[Shape['N*Out_h*Out_w*C,1'], Int] = None\n",
    "\n",
    "    @beartype\n",
    "    def forward(self, x: NDArray[Shape['N,C,H,W'], Float]):\n",
    "        # 出力特徴マップの奥行きを、対象が色ではないのにチャンネルと呼ぶのは個人的にまだ違和感があるが、そのうち慣れる。\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col: NDArray[Shape['N*Out_h*Out_w,C*Pool_h*Pool_w'], Float] = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        # im2colはフィルタ適用領域向けに実装されている。フィルタはチャンネルをまたいで適用されるが、プーリングはチャンネルごとに適用される。\n",
    "        # そのため、colをチャンネルごとに改行することで、プーリング適用領域ごとに別々の行にする。\n",
    "        reshaped_col: NDArray[Shape['N*Out_h*Out_w*C,Pool_h*Pool_w'], Float] = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        arg_max = np.argmax(reshaped_col, axis=1)\n",
    "        out: NDArray[Shape['N*C,1'], Float] = np.max(reshaped_col, axis=1)\n",
    "        reshaped_out: NDArray[Shape['N,C,Out_h,Out_w'], Float] = out.reshape(N, out_h, out_w, C).transpose(0,3,1,2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return reshaped_out\n",
    "\n",
    "    @beartype\n",
    "    def backward(self, dout: NDArray[Shape['N,C,Out_h,Out_w'], Float]):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rectifyは電流の交流を整流にすることから名付けられた。電流の交流は正負の電流が交互に流れるが、整流にすると正の電流のみが流れる。\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    @beartype\n",
    "    def forward(self, x:NDArray):\n",
    "        # NDArray[Shape['N,FN,Out_h,Out_w'], Any]|NDArray[Shape['N,Hidden_size'], Any] で定義したい。isinstance()では判定できるが、beartypeでは失敗するため諦めた。\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    @beartype\n",
    "    def backward(self, dout: NDArray):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W: NDArray[Shape['S,WS'], Float], b: NDArray[Shape['D'], Float]):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    @beartype\n",
    "    def forward(self, x: NDArray):\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    @beartype\n",
    "    def backward(self, dout: NDArray[Shape['N,WS'], Float]):\n",
    "        dx: NDArray[Shape['N,S'], Float] = np.dot(dout, self.W.T)\n",
    "        self.dW: NDArray[Shape['S,WS'], Float] = np.dot(self.x.T, dout)\n",
    "        self.db: NDArray[Shape['1'], Float] = np.sum(dout, axis=0)\n",
    "\n",
    "        dx = dx.reshape(*self.original_x_shape)  # 入力データの形状に戻す（テンソル対応）\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 畳み込み層 → 全結合層(ReLU) → 全結合層(Softmax)を想定\n",
    "\n",
    "class SimpleConvNet:\n",
    "    @beartype\n",
    "    def __init__(self, input_dim=(1,28,28),\n",
    "        # filter_size:5は、5x5を表す。正方形がメジャー。\n",
    "        conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "        hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = int(1 + (input_size + 2*filter_pad - filter_size) / filter_stride)\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W1']: NDArray[Shape['FN,C,FS,FS'],Float] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2']: NDArray[Shape['PS,HS']] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3']: NDArray[Shape['HS,OS']] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    @beartype\n",
    "    def predict(self, x: NDArray):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    @beartype\n",
    "    def loss(self, x: NDArray, t: NDArray[Shape['N'], Int]):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y,t)\n",
    "\n",
    "    @beartype\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    @beartype\n",
    "    def gradient(self, x: NDArray, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3001801661352212\n",
      "=== epoch:1, train acc:0.208, test acc:0.219 ===\n",
      "train loss:2.296732256845275\n",
      "train loss:2.2928850492122224\n",
      "train loss:2.290013603671233\n",
      "train loss:2.284018685787875\n",
      "train loss:2.2707778013264743\n",
      "train loss:2.260144170903759\n",
      "train loss:2.2449025683850894\n",
      "train loss:2.223492559175965\n",
      "train loss:2.187705469838722\n",
      "train loss:2.1671641642128137\n",
      "train loss:2.1392639641352504\n",
      "train loss:2.0610642966090644\n",
      "train loss:2.034386152134264\n",
      "train loss:2.02723391317461\n",
      "train loss:1.9110515422262497\n",
      "train loss:1.7957259080845922\n",
      "train loss:1.8092916186091321\n",
      "train loss:1.7956966959703615\n",
      "train loss:1.634565735427775\n",
      "train loss:1.599107354104922\n",
      "train loss:1.5922319818868857\n",
      "train loss:1.4360950824206582\n",
      "train loss:1.3493298290039875\n",
      "train loss:1.2339569841455118\n",
      "train loss:1.1549720346594576\n",
      "train loss:1.1398969648280586\n",
      "train loss:1.0766146622790418\n",
      "train loss:0.9884613959903747\n",
      "train loss:0.9816034325955588\n",
      "train loss:0.8698234518245633\n",
      "train loss:0.8482119284631314\n",
      "train loss:0.9030677427869094\n",
      "train loss:0.7703681906410883\n",
      "train loss:0.7997283081027199\n",
      "train loss:0.8384806597391402\n",
      "train loss:0.6596794153965324\n",
      "train loss:0.5519859072041892\n",
      "train loss:0.5856894302248868\n",
      "train loss:0.5975203432103722\n",
      "train loss:0.6684643486286247\n",
      "train loss:0.5925552778187666\n",
      "train loss:0.4952656019100261\n",
      "train loss:0.529341972256628\n",
      "train loss:0.6190930443574307\n",
      "train loss:0.5598670050959749\n",
      "train loss:0.537222453889254\n",
      "train loss:0.46155127075876384\n",
      "train loss:0.6491852318264739\n",
      "train loss:0.4337487437990809\n",
      "train loss:0.5116871604737476\n",
      "=== epoch:2, train acc:0.831, test acc:0.807 ===\n",
      "train loss:0.3086470900331677\n",
      "train loss:0.44438596186898477\n",
      "train loss:0.48806913106028255\n",
      "train loss:0.7757174401054518\n",
      "train loss:0.553796374616409\n",
      "train loss:0.4209687549596176\n",
      "train loss:0.5759156116483857\n",
      "train loss:0.5208064097530313\n",
      "train loss:0.3638969319616367\n",
      "train loss:0.5392110780786833\n",
      "train loss:0.5078590271469724\n",
      "train loss:0.45334284343343634\n",
      "train loss:0.5342557039636069\n",
      "train loss:0.4242972372698813\n",
      "train loss:0.2631475704666474\n",
      "train loss:0.4454885250785454\n",
      "train loss:0.36662994838815427\n",
      "train loss:0.3248709297521437\n",
      "train loss:0.3134235482623252\n",
      "train loss:0.5677406723936934\n",
      "train loss:0.55964741559706\n",
      "train loss:0.3495311880437215\n",
      "train loss:0.5850157987975336\n",
      "train loss:0.32049795235787976\n",
      "train loss:0.35002407362200266\n",
      "train loss:0.5592523511034392\n",
      "train loss:0.27092593083490835\n",
      "train loss:0.4484409015038141\n",
      "train loss:0.41384979123421917\n",
      "train loss:0.5012457261337583\n",
      "train loss:0.2246446331764149\n",
      "train loss:0.22314923910724094\n",
      "train loss:0.5853512354657286\n",
      "train loss:0.33543205256524017\n",
      "train loss:0.3581922159151021\n",
      "train loss:0.27449715366824456\n",
      "train loss:0.32095257842830904\n",
      "train loss:0.31387346792204573\n",
      "train loss:0.42554381942977315\n",
      "train loss:0.525572764232588\n",
      "train loss:0.3669873779913565\n",
      "train loss:0.4490133675663138\n",
      "train loss:0.3584373178958789\n",
      "train loss:0.2635425721822273\n",
      "train loss:0.4254954865170688\n",
      "train loss:0.3824862369225692\n",
      "train loss:0.28626005123890347\n",
      "train loss:0.28727763836103015\n",
      "train loss:0.3229939224701276\n",
      "train loss:0.1882673650657546\n",
      "=== epoch:3, train acc:0.894, test acc:0.855 ===\n",
      "train loss:0.16899856339019353\n",
      "train loss:0.37667843301797904\n",
      "train loss:0.25474163138187766\n",
      "train loss:0.42973479984035456\n",
      "train loss:0.2542029124333494\n",
      "train loss:0.2635922795129004\n",
      "train loss:0.3273709798953542\n",
      "train loss:0.3997630697606106\n",
      "train loss:0.323271008312536\n",
      "train loss:0.27875156472623713\n",
      "train loss:0.2980881818406773\n",
      "train loss:0.4685188986123721\n",
      "train loss:0.2092774154125131\n",
      "train loss:0.26619527357646244\n",
      "train loss:0.5129198411426729\n",
      "train loss:0.31348906122890924\n",
      "train loss:0.22501215995736085\n",
      "train loss:0.4585106383510268\n",
      "train loss:0.29041170009291817\n",
      "train loss:0.4277444071772162\n",
      "train loss:0.22187004861008355\n",
      "train loss:0.25528520826535256\n",
      "train loss:0.18160347309752947\n",
      "train loss:0.26473669106026915\n",
      "train loss:0.29719477815185746\n",
      "train loss:0.24928171283141143\n",
      "train loss:0.21362915337018237\n",
      "train loss:0.39056352258659877\n",
      "train loss:0.1284624315431926\n",
      "train loss:0.22492539287711458\n",
      "train loss:0.3515223826320079\n",
      "train loss:0.2910726319359018\n",
      "train loss:0.24560635764316316\n",
      "train loss:0.2486049840465894\n",
      "train loss:0.20861426159514354\n",
      "train loss:0.24372476779798669\n",
      "train loss:0.22534205947344993\n",
      "train loss:0.20551571106765348\n",
      "train loss:0.24113788353249618\n",
      "train loss:0.3503931482578693\n",
      "train loss:0.21024811998495652\n",
      "train loss:0.2694644614298335\n",
      "train loss:0.2113325554284623\n",
      "train loss:0.158990321265406\n",
      "train loss:0.15498317320004618\n",
      "train loss:0.1475709373852089\n",
      "train loss:0.2904058855842609\n",
      "train loss:0.2981804004302695\n",
      "train loss:0.23713342357762635\n",
      "train loss:0.24744853675365774\n",
      "=== epoch:4, train acc:0.904, test acc:0.9 ===\n",
      "train loss:0.10662986559687622\n",
      "train loss:0.22267858823488446\n",
      "train loss:0.19290551783380835\n",
      "train loss:0.17030708144305506\n",
      "train loss:0.2578127834890007\n",
      "train loss:0.4418201283393836\n",
      "train loss:0.31765596192210005\n",
      "train loss:0.19549089156947289\n",
      "train loss:0.22208525152935077\n",
      "train loss:0.18898705336999128\n",
      "train loss:0.12632985919770504\n",
      "train loss:0.1700826627077865\n",
      "train loss:0.4269548014472855\n",
      "train loss:0.24509718135475234\n",
      "train loss:0.17966852148843887\n",
      "train loss:0.2547744426392448\n",
      "train loss:0.2545515524511241\n",
      "train loss:0.2931797696634849\n",
      "train loss:0.14945512612244585\n",
      "train loss:0.19039747267091212\n",
      "train loss:0.11529681415948476\n",
      "train loss:0.25965664445312486\n",
      "train loss:0.15769110205924755\n",
      "train loss:0.2836284372589877\n",
      "train loss:0.14616547142100197\n",
      "train loss:0.11305154237526302\n",
      "train loss:0.2500658848943622\n",
      "train loss:0.22619902441058756\n",
      "train loss:0.4200764443262021\n",
      "train loss:0.23909914164710874\n",
      "train loss:0.20959565514627243\n",
      "train loss:0.4838014370323318\n",
      "train loss:0.19099623461431314\n",
      "train loss:0.3632814686137108\n",
      "train loss:0.1801040221469052\n",
      "train loss:0.1818608086653045\n",
      "train loss:0.19567446659345036\n",
      "train loss:0.26111797948965776\n",
      "train loss:0.20379988147840322\n",
      "train loss:0.19947015387633862\n",
      "train loss:0.23661865329096674\n",
      "train loss:0.13210622437687852\n",
      "train loss:0.26431601526771553\n",
      "train loss:0.19009664725102243\n",
      "train loss:0.17396803426935517\n",
      "train loss:0.20602491419370922\n",
      "train loss:0.3139683336553758\n",
      "train loss:0.419826553752438\n",
      "train loss:0.2701073177540195\n",
      "train loss:0.24458371942614213\n",
      "=== epoch:5, train acc:0.923, test acc:0.905 ===\n",
      "train loss:0.16808840173249984\n",
      "train loss:0.19716098949041708\n",
      "train loss:0.20000332074478108\n",
      "train loss:0.10860787583133423\n",
      "train loss:0.17111143612143628\n",
      "train loss:0.11864073841298646\n",
      "train loss:0.34555865221045345\n",
      "train loss:0.3129426354346619\n",
      "train loss:0.20899933505225854\n",
      "train loss:0.13283315877965723\n",
      "train loss:0.23970840555343492\n",
      "train loss:0.1746443063028512\n",
      "train loss:0.14852754818276057\n",
      "train loss:0.16142753432040607\n",
      "train loss:0.32475960096782075\n",
      "train loss:0.14102583092864662\n",
      "train loss:0.1387398337193632\n",
      "train loss:0.20271002663356757\n",
      "train loss:0.18890710001369798\n",
      "train loss:0.15200936587330793\n",
      "train loss:0.12329595737379093\n",
      "train loss:0.338228881489149\n",
      "train loss:0.0955349516380462\n",
      "train loss:0.18949993014298033\n",
      "train loss:0.17462322876083433\n",
      "train loss:0.09667404670368844\n",
      "train loss:0.0761287514865795\n",
      "train loss:0.18558143115897527\n",
      "train loss:0.12545526052785178\n",
      "train loss:0.2185906021663173\n",
      "train loss:0.16503522532624268\n",
      "train loss:0.208759734455943\n",
      "train loss:0.1708076867010997\n",
      "train loss:0.18053614320152078\n",
      "train loss:0.12823491680193172\n",
      "train loss:0.18015967050320672\n",
      "train loss:0.24871468745461178\n",
      "train loss:0.22175309591675355\n",
      "train loss:0.21515946477401207\n",
      "train loss:0.26031721238841343\n",
      "train loss:0.23979357516825164\n",
      "train loss:0.10461036393255263\n",
      "train loss:0.24504577705143452\n",
      "train loss:0.13431659763551423\n",
      "train loss:0.3474147023829824\n",
      "train loss:0.1126950153687028\n",
      "train loss:0.17748219250745645\n",
      "train loss:0.1667257526285307\n",
      "train loss:0.07117884875605195\n",
      "train loss:0.15560237626328183\n",
      "=== epoch:6, train acc:0.938, test acc:0.921 ===\n",
      "train loss:0.1856257048535195\n",
      "train loss:0.28973327185576353\n",
      "train loss:0.12978925822470852\n",
      "train loss:0.14991946578312654\n",
      "train loss:0.14362908776018052\n",
      "train loss:0.1597982929025191\n",
      "train loss:0.16179428357732095\n",
      "train loss:0.1525770672916843\n",
      "train loss:0.20072151788669693\n",
      "train loss:0.1491098717518857\n",
      "train loss:0.1935477507436584\n",
      "train loss:0.07593871373651381\n",
      "train loss:0.16468163891091653\n",
      "train loss:0.17812080563156976\n",
      "train loss:0.1999930188565681\n",
      "train loss:0.14352206038670615\n",
      "train loss:0.11648248945659455\n",
      "train loss:0.13214532804666532\n",
      "train loss:0.15798391478388044\n",
      "train loss:0.09620805206934623\n",
      "train loss:0.16800383492930865\n",
      "train loss:0.18788809708376444\n",
      "train loss:0.23065132666435484\n",
      "train loss:0.17759867241490568\n",
      "train loss:0.0914854602344225\n",
      "train loss:0.12682727012309797\n",
      "train loss:0.2623692484742192\n",
      "train loss:0.12616097181886926\n",
      "train loss:0.10382057292419024\n",
      "train loss:0.1565930407556087\n",
      "train loss:0.16442863085989692\n",
      "train loss:0.11331743294892639\n",
      "train loss:0.166713317099798\n",
      "train loss:0.1245632444662087\n",
      "train loss:0.13863629270413194\n",
      "train loss:0.11237182354904707\n",
      "train loss:0.2506232951714907\n",
      "train loss:0.10204898311027197\n",
      "train loss:0.18356905999562095\n",
      "train loss:0.07346528857097329\n",
      "train loss:0.11810414150055218\n",
      "train loss:0.11199979709295958\n",
      "train loss:0.13207688304191506\n",
      "train loss:0.06328078100424793\n",
      "train loss:0.1487013898571211\n",
      "train loss:0.14948976306248232\n",
      "train loss:0.12759671406072465\n",
      "train loss:0.12892310343983654\n",
      "train loss:0.07702206192323828\n",
      "train loss:0.1790759683609086\n",
      "=== epoch:7, train acc:0.95, test acc:0.928 ===\n",
      "train loss:0.13112580351428907\n",
      "train loss:0.09214776305255129\n",
      "train loss:0.22573990651657025\n",
      "train loss:0.15216502778365926\n",
      "train loss:0.18766630173050458\n",
      "train loss:0.16155075860360968\n",
      "train loss:0.09081903867353514\n",
      "train loss:0.16402418454456977\n",
      "train loss:0.10879316309445565\n",
      "train loss:0.21107247912090835\n",
      "train loss:0.07827596549583028\n",
      "train loss:0.20864468777575984\n",
      "train loss:0.09074534766580968\n",
      "train loss:0.07338099723441101\n",
      "train loss:0.22514848478622132\n",
      "train loss:0.14684598689189843\n",
      "train loss:0.1220459061096465\n",
      "train loss:0.15394880642501607\n",
      "train loss:0.12683448073142745\n",
      "train loss:0.16414381390251936\n",
      "train loss:0.20493281685633896\n",
      "train loss:0.16146672822450983\n",
      "train loss:0.15726651079238618\n",
      "train loss:0.13595958288581472\n",
      "train loss:0.16273977553014393\n",
      "train loss:0.1051876088737334\n",
      "train loss:0.11184660438387738\n",
      "train loss:0.06540376422741334\n",
      "train loss:0.1578244615293133\n",
      "train loss:0.12025043127714113\n",
      "train loss:0.21484255199113686\n",
      "train loss:0.1925216415318425\n",
      "train loss:0.26629439500554325\n",
      "train loss:0.13428654207252433\n",
      "train loss:0.15029498792739054\n",
      "train loss:0.1631632341273745\n",
      "train loss:0.12802867480682834\n",
      "train loss:0.08040937498326853\n",
      "train loss:0.21348767656959197\n",
      "train loss:0.09216989133660151\n",
      "train loss:0.17019653391171144\n",
      "train loss:0.20071567584118644\n",
      "train loss:0.1458967978367612\n",
      "train loss:0.09255020281759617\n",
      "train loss:0.06544454101807495\n",
      "train loss:0.24496668343726707\n",
      "train loss:0.12756457289852746\n",
      "train loss:0.13810381332018312\n",
      "train loss:0.09803049964463978\n",
      "train loss:0.11914011782078664\n",
      "=== epoch:8, train acc:0.954, test acc:0.935 ===\n",
      "train loss:0.09064481510260253\n",
      "train loss:0.036692857189269644\n",
      "train loss:0.06484085134158148\n",
      "train loss:0.07488647825253034\n",
      "train loss:0.111888545204597\n",
      "train loss:0.10641264110116717\n",
      "train loss:0.07166672714038559\n",
      "train loss:0.09042997599038388\n",
      "train loss:0.0910914138493894\n",
      "train loss:0.060487708171147236\n",
      "train loss:0.09960598009526271\n",
      "train loss:0.05130901721345643\n",
      "train loss:0.10037754546280578\n",
      "train loss:0.0999957751675086\n",
      "train loss:0.13627895492481598\n",
      "train loss:0.06785658144104907\n",
      "train loss:0.17390541372200694\n",
      "train loss:0.059244345493680545\n",
      "train loss:0.10367085899990562\n",
      "train loss:0.06255228069620059\n",
      "train loss:0.11965826208328213\n",
      "train loss:0.07905955701060453\n",
      "train loss:0.14533731368099986\n",
      "train loss:0.24962851306017875\n",
      "train loss:0.07317109594498673\n",
      "train loss:0.07026567028226255\n",
      "train loss:0.056000643926062396\n",
      "train loss:0.13571606281871917\n",
      "train loss:0.11442912620523094\n",
      "train loss:0.16066966083733042\n",
      "train loss:0.12592898358162344\n",
      "train loss:0.1350704951283652\n",
      "train loss:0.11569008118651418\n",
      "train loss:0.1938650672130295\n",
      "train loss:0.05358845639321132\n",
      "train loss:0.14312417640837882\n",
      "train loss:0.1482098365948101\n",
      "train loss:0.12410092542020378\n",
      "train loss:0.055187565107481705\n",
      "train loss:0.12872196076259332\n",
      "train loss:0.26811291378611685\n",
      "train loss:0.09024975645012435\n",
      "train loss:0.14743509812555122\n",
      "train loss:0.09591830641433187\n",
      "train loss:0.1398979797032291\n",
      "train loss:0.12720446970034177\n",
      "train loss:0.16361703679150974\n",
      "train loss:0.08137035713706736\n",
      "train loss:0.16004764272358618\n",
      "train loss:0.05314734759315938\n",
      "=== epoch:9, train acc:0.962, test acc:0.94 ===\n",
      "train loss:0.09725851458810121\n",
      "train loss:0.14389686011503727\n",
      "train loss:0.12891934704102223\n",
      "train loss:0.08537850437121594\n",
      "train loss:0.15468741860053187\n",
      "train loss:0.08681680318844136\n",
      "train loss:0.09078892006420944\n",
      "train loss:0.11535512663624113\n",
      "train loss:0.06402514325316468\n",
      "train loss:0.07549735283498107\n",
      "train loss:0.08771901764495689\n",
      "train loss:0.05260094846841918\n",
      "train loss:0.07655337928662653\n",
      "train loss:0.11951143557106575\n",
      "train loss:0.07724686153797877\n",
      "train loss:0.07821110251041777\n",
      "train loss:0.15533616223201688\n",
      "train loss:0.03351931774376914\n",
      "train loss:0.19583822344256493\n",
      "train loss:0.08187690798663778\n",
      "train loss:0.11686282556617605\n",
      "train loss:0.1095621361084749\n",
      "train loss:0.06470982658365561\n",
      "train loss:0.07071269476153288\n",
      "train loss:0.07419130815740543\n",
      "train loss:0.10241065247171449\n",
      "train loss:0.1642978262071122\n",
      "train loss:0.1262837964564053\n",
      "train loss:0.24930933646892847\n",
      "train loss:0.12060955139357914\n",
      "train loss:0.0417899388029504\n",
      "train loss:0.07493110193744525\n",
      "train loss:0.11811526702926235\n",
      "train loss:0.15116356686005378\n",
      "train loss:0.16329230388987212\n",
      "train loss:0.15211263374459782\n",
      "train loss:0.12134780298481941\n",
      "train loss:0.0564727445920603\n",
      "train loss:0.10733019745476018\n",
      "train loss:0.20226332432515054\n",
      "train loss:0.05834833851746402\n",
      "train loss:0.10231085031811599\n",
      "train loss:0.10473565640779395\n",
      "train loss:0.1129597963800524\n",
      "train loss:0.08612013162591596\n",
      "train loss:0.06550569121646976\n",
      "train loss:0.10332716991253706\n",
      "train loss:0.12137239146745307\n",
      "train loss:0.05431313358268509\n",
      "train loss:0.02903483613939512\n",
      "=== epoch:10, train acc:0.963, test acc:0.943 ===\n",
      "train loss:0.15258615911208556\n",
      "train loss:0.12269297345498321\n",
      "train loss:0.055478686810610495\n",
      "train loss:0.09195161559749143\n",
      "train loss:0.09021790525732393\n",
      "train loss:0.03225182506268566\n",
      "train loss:0.037454445598133025\n",
      "train loss:0.0812038361471872\n",
      "train loss:0.03375251089254063\n",
      "train loss:0.13079545198550246\n",
      "train loss:0.07531395566757779\n",
      "train loss:0.07932972506085509\n",
      "train loss:0.059953318636835534\n",
      "train loss:0.048833334345173986\n",
      "train loss:0.08107573836674885\n",
      "train loss:0.10298812054801507\n",
      "train loss:0.06052930904122091\n",
      "train loss:0.13083918051444038\n",
      "train loss:0.07036402010374515\n",
      "train loss:0.14489880968561453\n",
      "train loss:0.09150818601117468\n",
      "train loss:0.11581638170060121\n",
      "train loss:0.0527406338305958\n",
      "train loss:0.05129644417277586\n",
      "train loss:0.0699405978251117\n",
      "train loss:0.0616898537313826\n",
      "train loss:0.08443284489101063\n",
      "train loss:0.02778300699027368\n",
      "train loss:0.06673647865713074\n",
      "train loss:0.04465199299658077\n",
      "train loss:0.05292579284219778\n",
      "train loss:0.0893663046547771\n",
      "train loss:0.0783875972041487\n",
      "train loss:0.05301919835599425\n",
      "train loss:0.047607330559432955\n",
      "train loss:0.09957762375953128\n",
      "train loss:0.06161628600807724\n",
      "train loss:0.17018137811223233\n",
      "train loss:0.06540830385455698\n",
      "train loss:0.08227681095290995\n",
      "train loss:0.1752423701231448\n",
      "train loss:0.044350578130677575\n",
      "train loss:0.12518051137079275\n",
      "train loss:0.11060301008089893\n",
      "train loss:0.08048134732683535\n",
      "train loss:0.03014063980090889\n",
      "train loss:0.1032450504085889\n",
      "train loss:0.11110975835906096\n",
      "train loss:0.043048283831121435\n",
      "train loss:0.07219117298233209\n",
      "=== epoch:11, train acc:0.971, test acc:0.952 ===\n",
      "train loss:0.03269619309367038\n",
      "train loss:0.06808206169782446\n",
      "train loss:0.062363855546533904\n",
      "train loss:0.05749170960943932\n",
      "train loss:0.13535279900371502\n",
      "train loss:0.0775143658210696\n",
      "train loss:0.07850455892815499\n",
      "train loss:0.0776377270753166\n",
      "train loss:0.06356545213237479\n",
      "train loss:0.11606139978995143\n",
      "train loss:0.11112201130641731\n",
      "train loss:0.08795004640276312\n",
      "train loss:0.22153591544060805\n",
      "train loss:0.03288749070145205\n",
      "train loss:0.0933763319149207\n",
      "train loss:0.042060221612750824\n",
      "train loss:0.062180084420598955\n",
      "train loss:0.07852670104652056\n",
      "train loss:0.028109881491445644\n",
      "train loss:0.08381510073057935\n",
      "train loss:0.040371227745905\n",
      "train loss:0.015921065765119136\n",
      "train loss:0.049377519555712374\n",
      "train loss:0.14526915175468766\n",
      "train loss:0.1081951783642808\n",
      "train loss:0.04068222251237015\n",
      "train loss:0.08671394467832219\n",
      "train loss:0.01724484722023086\n",
      "train loss:0.03815159865948807\n",
      "train loss:0.07475528567660143\n",
      "train loss:0.04486251512695957\n",
      "train loss:0.09517637407303686\n",
      "train loss:0.04488054184592347\n",
      "train loss:0.05169677044003862\n",
      "train loss:0.06281544660798027\n",
      "train loss:0.1079091000290082\n",
      "train loss:0.05489699293715831\n",
      "train loss:0.03982233755030052\n",
      "train loss:0.0668654097567446\n",
      "train loss:0.05397843241354722\n",
      "train loss:0.08416627994426838\n",
      "train loss:0.09174244823889506\n",
      "train loss:0.10656555997984181\n",
      "train loss:0.030944924836124783\n",
      "train loss:0.08514098825378938\n",
      "train loss:0.03596080752917845\n",
      "train loss:0.1011610508423435\n",
      "train loss:0.062467868572039585\n",
      "train loss:0.07586483799119004\n",
      "train loss:0.053028167134578245\n",
      "=== epoch:12, train acc:0.975, test acc:0.954 ===\n",
      "train loss:0.04395700185695475\n",
      "train loss:0.0961830286371138\n",
      "train loss:0.026020642714121127\n",
      "train loss:0.04986813320056129\n",
      "train loss:0.1769513027485238\n",
      "train loss:0.06863681543696179\n",
      "train loss:0.04688756376580109\n",
      "train loss:0.039165745530514086\n",
      "train loss:0.050098950121245205\n",
      "train loss:0.06629429052714929\n",
      "train loss:0.08216263258921933\n",
      "train loss:0.039397114833023154\n",
      "train loss:0.07259732454642348\n",
      "train loss:0.07352766325779136\n",
      "train loss:0.06501928512767395\n",
      "train loss:0.07435005383704153\n",
      "train loss:0.15902744885979708\n",
      "train loss:0.07731003630520228\n",
      "train loss:0.02505311113147761\n",
      "train loss:0.09610682090572702\n",
      "train loss:0.03172040352469509\n",
      "train loss:0.021613291472732592\n",
      "train loss:0.05606301082550822\n",
      "train loss:0.049520711787239415\n",
      "train loss:0.03886447141921052\n",
      "train loss:0.06822326667126273\n",
      "train loss:0.031928702355521234\n",
      "train loss:0.03136051428524423\n",
      "train loss:0.04743013778646994\n",
      "train loss:0.09515358606137576\n",
      "train loss:0.022819585515285637\n",
      "train loss:0.04451409139148233\n",
      "train loss:0.06647911869746014\n",
      "train loss:0.17142245635481823\n",
      "train loss:0.04314491844080387\n",
      "train loss:0.07988860144306026\n",
      "train loss:0.07580276770745722\n",
      "train loss:0.07127822372661975\n",
      "train loss:0.11273907048381927\n",
      "train loss:0.09783365864156522\n",
      "train loss:0.09950313354513021\n",
      "train loss:0.06497208121280107\n",
      "train loss:0.07475669891837222\n",
      "train loss:0.05152837268402365\n",
      "train loss:0.04756796136637435\n",
      "train loss:0.02414487530830989\n",
      "train loss:0.06339082942600928\n",
      "train loss:0.06821860037199048\n",
      "train loss:0.042533489980504005\n",
      "train loss:0.05173871893208808\n",
      "=== epoch:13, train acc:0.978, test acc:0.951 ===\n",
      "train loss:0.05159343189002161\n",
      "train loss:0.030218575011730135\n",
      "train loss:0.04938234393529333\n",
      "train loss:0.11130928652194039\n",
      "train loss:0.030632037170477168\n",
      "train loss:0.018368818277578393\n",
      "train loss:0.09512490438841657\n",
      "train loss:0.026277716619561492\n",
      "train loss:0.08025082628251079\n",
      "train loss:0.030517125074778346\n",
      "train loss:0.040859090448290766\n",
      "train loss:0.09979055090483165\n",
      "train loss:0.03834427068257343\n",
      "train loss:0.045832113826729304\n",
      "train loss:0.09522412592051827\n",
      "train loss:0.04983705178783266\n",
      "train loss:0.03653135410262448\n",
      "train loss:0.13110547139833623\n",
      "train loss:0.06638606638110006\n",
      "train loss:0.04704170995299424\n",
      "train loss:0.13137376500866535\n",
      "train loss:0.03212888536012685\n",
      "train loss:0.06334792644622811\n",
      "train loss:0.07357457533274094\n",
      "train loss:0.0618483554855896\n",
      "train loss:0.06298097197429933\n",
      "train loss:0.07064370359305791\n",
      "train loss:0.053290550760457044\n",
      "train loss:0.12384215708447702\n",
      "train loss:0.042200019013683907\n",
      "train loss:0.06468086528286751\n",
      "train loss:0.04682038354086661\n",
      "train loss:0.03979910496955527\n",
      "train loss:0.045983724221381486\n",
      "train loss:0.031185181070789953\n",
      "train loss:0.12587963571602978\n",
      "train loss:0.03187441119225478\n",
      "train loss:0.044318459061100414\n",
      "train loss:0.056980834846722575\n",
      "train loss:0.02780232400144941\n",
      "train loss:0.035444103207237336\n",
      "train loss:0.03265753773562655\n",
      "train loss:0.06299844645204092\n",
      "train loss:0.042096214176058126\n",
      "train loss:0.14707297863609203\n",
      "train loss:0.02596910664374317\n",
      "train loss:0.027341578798078237\n",
      "train loss:0.03710465515550549\n",
      "train loss:0.02544353277102665\n",
      "train loss:0.04315117703289428\n",
      "=== epoch:14, train acc:0.985, test acc:0.954 ===\n",
      "train loss:0.09976350907952476\n",
      "train loss:0.045158586593072814\n",
      "train loss:0.1473254550594825\n",
      "train loss:0.04059364387898383\n",
      "train loss:0.03843861589241563\n",
      "train loss:0.05704500547314437\n",
      "train loss:0.025453898959685153\n",
      "train loss:0.06743036428869163\n",
      "train loss:0.03217814159398268\n",
      "train loss:0.0651630295696024\n",
      "train loss:0.037426371814909126\n",
      "train loss:0.07471415111551244\n",
      "train loss:0.0401910444933149\n",
      "train loss:0.030833617978337117\n",
      "train loss:0.02164123847206656\n",
      "train loss:0.049508357891440806\n",
      "train loss:0.06201284546011622\n",
      "train loss:0.09561455698367412\n",
      "train loss:0.056547627247341546\n",
      "train loss:0.035417279292837944\n",
      "train loss:0.02591763604962974\n",
      "train loss:0.08664522277813144\n",
      "train loss:0.03597282672047605\n",
      "train loss:0.01992846705540049\n",
      "train loss:0.026560316100780294\n",
      "train loss:0.023002203630423858\n",
      "train loss:0.04891972830645577\n",
      "train loss:0.10114848809510749\n",
      "train loss:0.028435196031673726\n",
      "train loss:0.027280871077654534\n",
      "train loss:0.02440302766101986\n",
      "train loss:0.030757275562987775\n",
      "train loss:0.0539088986187719\n",
      "train loss:0.03776859879097611\n",
      "train loss:0.029408811230496906\n",
      "train loss:0.016653982432712634\n",
      "train loss:0.03677721260501527\n",
      "train loss:0.030266776210627584\n",
      "train loss:0.030280253206336122\n",
      "train loss:0.0750568822444092\n",
      "train loss:0.01562131171115122\n",
      "train loss:0.0714486832348561\n",
      "train loss:0.033762224058330945\n",
      "train loss:0.0933130579016958\n",
      "train loss:0.03676315120246889\n",
      "train loss:0.01756209001679307\n",
      "train loss:0.03848169648876385\n",
      "train loss:0.04273548832700755\n",
      "train loss:0.018733422347379007\n",
      "train loss:0.04322222984183213\n",
      "=== epoch:15, train acc:0.979, test acc:0.95 ===\n",
      "train loss:0.01640186537220807\n",
      "train loss:0.04429326832866337\n",
      "train loss:0.041930831208086994\n",
      "train loss:0.00559051974766721\n",
      "train loss:0.031366642352709355\n",
      "train loss:0.08130088498871166\n",
      "train loss:0.031158185610438215\n",
      "train loss:0.06676035623732539\n",
      "train loss:0.04978250723027521\n",
      "train loss:0.044947283192138705\n",
      "train loss:0.01888923788275763\n",
      "train loss:0.08761747105003367\n",
      "train loss:0.04168845817230737\n",
      "train loss:0.06653826409502445\n",
      "train loss:0.058827959765175054\n",
      "train loss:0.023324284655219126\n",
      "train loss:0.013848812724090004\n",
      "train loss:0.044024134513220826\n",
      "train loss:0.04368126378820513\n",
      "train loss:0.027314371101657867\n",
      "train loss:0.03738447545309854\n",
      "train loss:0.0516221253799473\n",
      "train loss:0.025545472876150957\n",
      "train loss:0.030273077493595868\n",
      "train loss:0.042943744012844016\n",
      "train loss:0.05919589689307692\n",
      "train loss:0.020206984715479687\n",
      "train loss:0.0655517289550407\n",
      "train loss:0.028596683687002286\n",
      "train loss:0.06910541681402715\n",
      "train loss:0.027035474439275536\n",
      "train loss:0.02262371125931982\n",
      "train loss:0.04302443829128669\n",
      "train loss:0.0377743115579903\n",
      "train loss:0.03646602838703142\n",
      "train loss:0.027053144254769065\n",
      "train loss:0.018672510311245984\n",
      "train loss:0.011131695021536651\n",
      "train loss:0.044363006126568905\n",
      "train loss:0.030309528491145165\n",
      "train loss:0.02795296944286478\n",
      "train loss:0.038268094254578895\n",
      "train loss:0.028896843126398602\n",
      "train loss:0.02964753189134406\n",
      "train loss:0.04061534622378402\n",
      "train loss:0.045668579189547406\n",
      "train loss:0.02981301339095366\n",
      "train loss:0.026652228816881434\n",
      "train loss:0.03519012276722518\n",
      "train loss:0.10997646957462011\n",
      "=== epoch:16, train acc:0.984, test acc:0.953 ===\n",
      "train loss:0.06152509166868323\n",
      "train loss:0.031856539279609804\n",
      "train loss:0.06048429991952867\n",
      "train loss:0.014232138639635114\n",
      "train loss:0.02390814495004263\n",
      "train loss:0.05332517054097553\n",
      "train loss:0.01573433437400766\n",
      "train loss:0.024682516212580673\n",
      "train loss:0.03270538513012916\n",
      "train loss:0.012876914798287994\n",
      "train loss:0.04305451675349041\n",
      "train loss:0.04071730285165974\n",
      "train loss:0.02156944457088467\n",
      "train loss:0.04892006882902358\n",
      "train loss:0.03028187744180349\n",
      "train loss:0.02904123140097721\n",
      "train loss:0.02160323936090465\n",
      "train loss:0.06115494012093223\n",
      "train loss:0.029589013255434587\n",
      "train loss:0.033503570406803344\n",
      "train loss:0.04259354282754588\n",
      "train loss:0.011651932356800187\n",
      "train loss:0.054242161040381535\n",
      "train loss:0.04716592157543326\n",
      "train loss:0.035273463170567676\n",
      "train loss:0.019003589887273176\n",
      "train loss:0.06405319328698124\n",
      "train loss:0.05262606270673116\n",
      "train loss:0.010908583609026603\n",
      "train loss:0.035056394717388285\n",
      "train loss:0.0408735088823041\n",
      "train loss:0.056890856268339125\n",
      "train loss:0.023054545530497173\n",
      "train loss:0.030432624808635792\n",
      "train loss:0.0213706867873815\n",
      "train loss:0.06201089374535437\n",
      "train loss:0.022692034546320395\n",
      "train loss:0.024067663192693628\n",
      "train loss:0.06270755083972779\n",
      "train loss:0.0344952254385985\n",
      "train loss:0.03709004644485296\n",
      "train loss:0.045921338310050866\n",
      "train loss:0.07532782676610998\n",
      "train loss:0.028343580544114132\n",
      "train loss:0.04358605057815855\n",
      "train loss:0.041641690570748985\n",
      "train loss:0.01646623928473271\n",
      "train loss:0.06512356114892992\n",
      "train loss:0.038421834226490315\n",
      "train loss:0.03654152756790347\n",
      "=== epoch:17, train acc:0.985, test acc:0.959 ===\n",
      "train loss:0.09708085825195846\n",
      "train loss:0.027845265282090997\n",
      "train loss:0.03627015329552019\n",
      "train loss:0.02834917229395787\n",
      "train loss:0.05046920039559291\n",
      "train loss:0.03293614584768919\n",
      "train loss:0.009207183367678956\n",
      "train loss:0.01015515647484161\n",
      "train loss:0.029553416578014056\n",
      "train loss:0.03250048016380077\n",
      "train loss:0.08731753859843963\n",
      "train loss:0.05607053572080035\n",
      "train loss:0.016188912585240772\n",
      "train loss:0.013366235925683905\n",
      "train loss:0.007840162821008191\n",
      "train loss:0.02039518862772327\n",
      "train loss:0.03314144748891656\n",
      "train loss:0.060800158879903955\n",
      "train loss:0.022039166781348967\n",
      "train loss:0.01571303806329383\n",
      "train loss:0.05392599416079759\n",
      "train loss:0.01909261459389815\n",
      "train loss:0.05014286639481754\n",
      "train loss:0.03274257038511786\n",
      "train loss:0.0536328902616837\n",
      "train loss:0.019536280486985164\n",
      "train loss:0.01089520210187569\n",
      "train loss:0.06027781902451407\n",
      "train loss:0.03971730877431536\n",
      "train loss:0.05202405335234788\n",
      "train loss:0.019605924942428462\n",
      "train loss:0.05002975330518719\n",
      "train loss:0.028533502842299708\n",
      "train loss:0.027057923774250855\n",
      "train loss:0.03742292530943145\n",
      "train loss:0.017907827953407153\n",
      "train loss:0.01471404935052873\n",
      "train loss:0.029019009051723957\n",
      "train loss:0.031265052842645726\n",
      "train loss:0.013939242652181964\n",
      "train loss:0.047626937957833855\n",
      "train loss:0.017371238121835456\n",
      "train loss:0.01636310413103105\n",
      "train loss:0.01550024287837406\n",
      "train loss:0.07320050187382092\n",
      "train loss:0.008935483728098126\n",
      "train loss:0.05758695044394128\n",
      "train loss:0.04869582902606101\n",
      "train loss:0.023344031807587977\n",
      "train loss:0.04023766937052051\n",
      "=== epoch:18, train acc:0.993, test acc:0.959 ===\n",
      "train loss:0.026397723993704908\n",
      "train loss:0.021388330981097273\n",
      "train loss:0.005364627769684024\n",
      "train loss:0.03947311131424959\n",
      "train loss:0.013691444385834035\n",
      "train loss:0.04184507814966147\n",
      "train loss:0.007552194028386171\n",
      "train loss:0.0092125403744505\n",
      "train loss:0.007403670034417717\n",
      "train loss:0.0106607492482748\n",
      "train loss:0.01561156174005736\n",
      "train loss:0.024916358675557332\n",
      "train loss:0.028445339393915196\n",
      "train loss:0.014284041388328778\n",
      "train loss:0.011338924860387456\n",
      "train loss:0.018405111233418474\n",
      "train loss:0.021253408259517034\n",
      "train loss:0.07153936810681427\n",
      "train loss:0.009840345571774052\n",
      "train loss:0.028778784430164378\n",
      "train loss:0.024001219082931177\n",
      "train loss:0.03036961276878219\n",
      "train loss:0.018845966765933794\n",
      "train loss:0.016252700144852167\n",
      "train loss:0.03146730500198467\n",
      "train loss:0.016705649298236452\n",
      "train loss:0.012501509710125664\n",
      "train loss:0.015458956495508871\n",
      "train loss:0.01273305408792446\n",
      "train loss:0.03128253931850747\n",
      "train loss:0.012137657939915825\n",
      "train loss:0.007112743122540026\n",
      "train loss:0.0181060673076334\n",
      "train loss:0.011089193075486663\n",
      "train loss:0.014990808334945498\n",
      "train loss:0.01838787320376534\n",
      "train loss:0.008153978733764798\n",
      "train loss:0.0417063478732822\n",
      "train loss:0.07832817103353044\n",
      "train loss:0.030147021612062615\n",
      "train loss:0.035999532156716745\n",
      "train loss:0.026447692499357828\n",
      "train loss:0.02841523071832367\n",
      "train loss:0.02939143490423318\n",
      "train loss:0.043177274904166986\n",
      "train loss:0.021410475030492343\n",
      "train loss:0.020533345298008387\n",
      "train loss:0.0772665578142552\n",
      "train loss:0.019579855718051\n",
      "train loss:0.006056706783156991\n",
      "=== epoch:19, train acc:0.992, test acc:0.965 ===\n",
      "train loss:0.006088044566292397\n",
      "train loss:0.011213434399848333\n",
      "train loss:0.028544107598289053\n",
      "train loss:0.05886095549971377\n",
      "train loss:0.01867843495770334\n",
      "train loss:0.01780185952526996\n",
      "train loss:0.015180597598778671\n",
      "train loss:0.017846339533552368\n",
      "train loss:0.015572805257280099\n",
      "train loss:0.08020891857631283\n",
      "train loss:0.015463563197948308\n",
      "train loss:0.016055194315552367\n",
      "train loss:0.012123829084414896\n",
      "train loss:0.012641575445735884\n",
      "train loss:0.04337704868082103\n",
      "train loss:0.016377084501160065\n",
      "train loss:0.017263729742666746\n",
      "train loss:0.004545462880885391\n",
      "train loss:0.040127691999356\n",
      "train loss:0.016197137933884855\n",
      "train loss:0.019105807794286465\n",
      "train loss:0.027454993497088007\n",
      "train loss:0.009587853854494156\n",
      "train loss:0.022242167181480137\n",
      "train loss:0.011456997543149057\n",
      "train loss:0.007988463575128718\n",
      "train loss:0.006567970737325525\n",
      "train loss:0.009635903460408265\n",
      "train loss:0.01306830467972956\n",
      "train loss:0.009132458548290307\n",
      "train loss:0.02222660598996822\n",
      "train loss:0.03257832414738178\n",
      "train loss:0.020629501045137853\n",
      "train loss:0.034523456515492865\n",
      "train loss:0.020527932329893424\n",
      "train loss:0.010956484656420531\n",
      "train loss:0.012266034794901959\n",
      "train loss:0.022596234919583753\n",
      "train loss:0.020365554834663242\n",
      "train loss:0.02649395112676998\n",
      "train loss:0.008815258352408846\n",
      "train loss:0.030742696500247985\n",
      "train loss:0.01083994988076281\n",
      "train loss:0.019655240614789116\n",
      "train loss:0.017055984740613882\n",
      "train loss:0.021913768796538433\n",
      "train loss:0.016443591072896058\n",
      "train loss:0.01426272641853935\n",
      "train loss:0.03444739072982435\n",
      "train loss:0.014583156221870446\n",
      "=== epoch:20, train acc:0.991, test acc:0.964 ===\n",
      "train loss:0.05051256344579261\n",
      "train loss:0.01166982882527757\n",
      "train loss:0.009734577258907409\n",
      "train loss:0.010157381359664897\n",
      "train loss:0.04054694911399813\n",
      "train loss:0.020036039217852818\n",
      "train loss:0.00923685978995818\n",
      "train loss:0.004794709414956772\n",
      "train loss:0.030733816514391527\n",
      "train loss:0.013140401130310255\n",
      "train loss:0.008113525015314487\n",
      "train loss:0.05097596663173621\n",
      "train loss:0.014305413728324542\n",
      "train loss:0.00806056727334928\n",
      "train loss:0.014170231894761309\n",
      "train loss:0.015794861339139033\n",
      "train loss:0.01321944205269384\n",
      "train loss:0.010393322315250378\n",
      "train loss:0.008689073439127148\n",
      "train loss:0.02632658807093131\n",
      "train loss:0.010784908021943618\n",
      "train loss:0.00812712385618887\n",
      "train loss:0.008173308466559748\n",
      "train loss:0.008978993410004778\n",
      "train loss:0.05655291196134813\n",
      "train loss:0.030616391387423024\n",
      "train loss:0.009431532700514923\n",
      "train loss:0.006777509391234794\n",
      "train loss:0.04556298768801303\n",
      "train loss:0.008466518462463416\n",
      "train loss:0.011181774046563979\n",
      "train loss:0.016894144435598092\n",
      "train loss:0.012351795857108421\n",
      "train loss:0.035771606353700844\n",
      "train loss:0.011390044435877706\n",
      "train loss:0.025056552588312523\n",
      "train loss:0.018733402421113383\n",
      "train loss:0.011107583559548525\n",
      "train loss:0.012769549480596776\n",
      "train loss:0.022230305959803003\n",
      "train loss:0.025011362042915434\n",
      "train loss:0.010110173584062904\n",
      "train loss:0.010338379341247235\n",
      "train loss:0.009806620068912702\n",
      "train loss:0.009975310315829926\n",
      "train loss:0.024283191821252846\n",
      "train loss:0.0070152820680725635\n",
      "train loss:0.00543660334160108\n",
      "train loss:0.005919670645217099\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.958\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "til-machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
