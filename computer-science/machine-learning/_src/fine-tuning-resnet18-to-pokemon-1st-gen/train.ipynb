{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d mikoajkolman/pokemon-images-first-generation17000-files -p \"data/\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('data/pokemon-images-first-generation17000-files.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# Define the transformations to apply to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# Load the entire dataset\n",
    "dataset = ImageFolder('data/pokemon', transform=transform)\n",
    "\n",
    "# Split the dataset into train and validation datasets\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = len(dataset) - train_size  # 20% for validation\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Abra': 0, 'Aerodactyl': 1, 'Alakazam': 2, 'Arbok': 3, 'Arcanine': 4, 'Articuno': 5, 'Beedrill': 6, 'Bellsprout': 7, 'Blastoise': 8, 'Bulbasaur': 9, 'Butterfree': 10, 'Caterpie': 11, 'Chansey': 12, 'Charizard': 13, 'Charmander': 14, 'Charmeleon': 15, 'Clefable': 16, 'Clefairy': 17, 'Cloyster': 18, 'Cubone': 19, 'Dewgong': 20, 'Diglett': 21, 'Ditto': 22, 'Dodrio': 23, 'Doduo': 24, 'Dragonair': 25, 'Dragonite': 26, 'Dratini': 27, 'Drowzee': 28, 'Dugtrio': 29, 'Eevee': 30, 'Ekans': 31, 'Electabuzz': 32, 'Electrode': 33, 'Exeggcute': 34, 'Exeggutor': 35, 'Farfetchd': 36, 'Fearow': 37, 'Flareon': 38, 'Gastly': 39, 'Gengar': 40, 'Geodude': 41, 'Gloom': 42, 'Golbat': 43, 'Goldeen': 44, 'Golduck': 45, 'Graveler': 46, 'Grimer': 47, 'Growlithe': 48, 'Gyarados': 49, 'Haunter': 50, 'Hitmonchan': 51, 'Hitmonlee': 52, 'Horsea': 53, 'Hypno': 54, 'Ivysaur': 55, 'Jigglypuff': 56, 'Jolteon': 57, 'Jynx': 58, 'Kabutops': 59, 'Kadabra': 60, 'Kakuna': 61, 'Kangaskhan': 62, 'Kingler': 63, 'Koffing': 64, 'Lapras': 65, 'Lickitung': 66, 'Machamp': 67, 'Machoke': 68, 'Machop': 69, 'Magikarp': 70, 'Magmar': 71, 'Magnemite': 72, 'Magneton': 73, 'Mankey': 74, 'Marowak': 75, 'Meowth': 76, 'Metapod': 77, 'Mew': 78, 'Mewtwo': 79, 'Moltres': 80, 'Mr. Mime': 81, 'MrMime': 82, 'Nidoking': 83, 'Nidoqueen': 84, 'Nidorina': 85, 'Nidorino': 86, 'Ninetales': 87, 'Oddish': 88, 'Omanyte': 89, 'Omastar': 90, 'Parasect': 91, 'Pidgeot': 92, 'Pidgeotto': 93, 'Pidgey': 94, 'Pikachu': 95, 'Pinsir': 96, 'Poliwag': 97, 'Poliwhirl': 98, 'Poliwrath': 99, 'Ponyta': 100, 'Porygon': 101, 'Primeape': 102, 'Psyduck': 103, 'Raichu': 104, 'Rapidash': 105, 'Raticate': 106, 'Rattata': 107, 'Rhydon': 108, 'Rhyhorn': 109, 'Sandshrew': 110, 'Sandslash': 111, 'Scyther': 112, 'Seadra': 113, 'Seaking': 114, 'Seel': 115, 'Shellder': 116, 'Slowbro': 117, 'Slowpoke': 118, 'Snorlax': 119, 'Spearow': 120, 'Squirtle': 121, 'Starmie': 122, 'Staryu': 123, 'Tangela': 124, 'Tauros': 125, 'Tentacool': 126, 'Tentacruel': 127, 'Vaporeon': 128, 'Venomoth': 129, 'Venonat': 130, 'Venusaur': 131, 'Victreebel': 132, 'Vileplume': 133, 'Voltorb': 134, 'Vulpix': 135, 'Wartortle': 136, 'Weedle': 137, 'Weepinbell': 138, 'Weezing': 139, 'Wigglytuff': 140, 'Zapdos': 141, 'Zubat': 142}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "class_to_idx = dataset.class_to_idx\n",
    "print(class_to_idx)\n",
    "\n",
    "labels = list(class_to_idx.keys())\n",
    "\n",
    "with open('data/pokemon-1st-gen-labels.json', 'w') as f:\n",
    "    json.dump(labels, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create data loaders for the train and validation datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hiroga\\miniconda3\\envs\\til-machine-learning\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hiroga\\miniconda3\\envs\\til-machine-learning\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the pre-trained layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the last layer of the model\n",
    "num_classes = len(labels)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    # Train the model for the specified number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to train mode\n",
    "        model.train()\n",
    "\n",
    "        # Initialize the running loss and accuracy\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over the batches of the train loader\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move the inputs and labels to the device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the optimizer gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimizer step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the running loss and accuracy\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        # Calculate the train loss and accuracy\n",
    "        train_loss = running_loss / len(train_dataset)\n",
    "        train_acc = running_corrects.double() / len(train_dataset)\n",
    "\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Initialize the running loss and accuracy\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over the batches of the validation loader\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                # Move the inputs and labels to the device\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Update the running loss and accuracy\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        # Calculate the validation loss and accuracy\n",
    "        val_loss = running_loss / len(val_dataset)\n",
    "        val_acc = running_corrects.double() / len(val_dataset)\n",
    "\n",
    "        # Print the epoch results\n",
    "        print('Epoch [{}/{}], train loss: {:.4f}, train acc: {:.4f}, val loss: {:.4f}, val acc: {:.4f}'\n",
    "              .format(epoch+1, num_epochs, train_loss, train_acc, val_loss, val_acc))\n",
    "        wandb.log({\"epoch\": epoch+1, \"train_loss\": train_loss, \"train_acc\": train_acc, \"val_loss\": val_loss, \"val_acc\": val_acc})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bo20rncc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9385855974bb437caaf01ac391237475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁</td></tr><tr><td>train_acc</td><td>▁█</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>val_acc</td><td>▁█</td></tr><tr><td>val_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train_acc</td><td>0.85828</td></tr><tr><td>train_loss</td><td>0.73933</td></tr><tr><td>val_acc</td><td>0.85168</td></tr><tr><td>val_loss</td><td>0.6861</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">brilliant-lantern-6</strong> at: <a href='https://wandb.ai/hiroga/fine-tuning-resnet18-to-pokemon-1st-gen/runs/bo20rncc' target=\"_blank\">https://wandb.ai/hiroga/fine-tuning-resnet18-to-pokemon-1st-gen/runs/bo20rncc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240210_135116-bo20rncc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bo20rncc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\hiroga\\Documents\\GitHub\\til\\computer-science\\machine-learning\\_src\\fine-tuning-resnet18\\wandb\\run-20240210_140215-umm0rx9g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hiroga/fine-tuning-resnet18-to-pokemon-1st-gen/runs/umm0rx9g' target=\"_blank\">resplendent-dragon-7</a></strong> to <a href='https://wandb.ai/hiroga/fine-tuning-resnet18-to-pokemon-1st-gen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hiroga/fine-tuning-resnet18-to-pokemon-1st-gen' target=\"_blank\">https://wandb.ai/hiroga/fine-tuning-resnet18-to-pokemon-1st-gen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hiroga/fine-tuning-resnet18-to-pokemon-1st-gen/runs/umm0rx9g' target=\"_blank\">https://wandb.ai/hiroga/fine-tuning-resnet18-to-pokemon-1st-gen/runs/umm0rx9g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], train loss: 0.5637, train acc: 0.8849, val loss: 0.5393, val acc: 0.8714\n",
      "Epoch [2/5], train loss: 0.3923, train acc: 0.9194, val loss: 0.4777, val acc: 0.8847\n",
      "Epoch [3/5], train loss: 0.3157, train acc: 0.9307, val loss: 0.4399, val acc: 0.8914\n",
      "Epoch [4/5], train loss: 0.2585, train acc: 0.9471, val loss: 0.4175, val acc: 0.8951\n",
      "Epoch [5/5], train loss: 0.2241, train acc: 0.9545, val loss: 0.3945, val acc: 0.8984\n",
      "Epoch [1/10], train loss: 0.1615, train acc: 0.9727, val loss: 0.3547, val acc: 0.9123\n",
      "Epoch [2/10], train loss: 0.1365, train acc: 0.9787, val loss: 0.3364, val acc: 0.9160\n",
      "Epoch [3/10], train loss: 0.1190, train acc: 0.9838, val loss: 0.3252, val acc: 0.9184\n",
      "Epoch [4/10], train loss: 0.1059, train acc: 0.9860, val loss: 0.3162, val acc: 0.9202\n",
      "Epoch [5/10], train loss: 0.0980, train acc: 0.9878, val loss: 0.3070, val acc: 0.9239\n",
      "Epoch [6/10], train loss: 0.0870, train acc: 0.9900, val loss: 0.3000, val acc: 0.9281\n",
      "Epoch [7/10], train loss: 0.0829, train acc: 0.9901, val loss: 0.3013, val acc: 0.9245\n",
      "Epoch [8/10], train loss: 0.0778, train acc: 0.9907, val loss: 0.2960, val acc: 0.9290\n",
      "Epoch [9/10], train loss: 0.0713, train acc: 0.9911, val loss: 0.2863, val acc: 0.9290\n",
      "Epoch [10/10], train loss: 0.0671, train acc: 0.9924, val loss: 0.2865, val acc: 0.9281\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "last_layer_learning_rate = 0.01\n",
    "last_layer_momentum = 0.9\n",
    "last_layer_epoches = 5\n",
    "full_layer_learning_rate = 0.001\n",
    "full_layer_momentum = 0.001\n",
    "full_layer_epoches = 10\n",
    "\n",
    "wandb.init(\n",
    "    project=\"fine-tuning-resnet18-to-pokemon-1st-gen\",\n",
    "    config={\n",
    "        \"last_layer_learning_rate\": last_layer_learning_rate,\n",
    "        \"last_layer_momentum\": last_layer_momentum,\n",
    "        \"last_layer_epochs\": last_layer_epoches,\n",
    "        \"full_layer_learning_rate\": full_layer_learning_rate,\n",
    "        \"full_layer_momentum\": full_layer_momentum,\n",
    "        \"full_layer_epochs\": full_layer_epoches,\n",
    "        \"architecture\": \"CNN\",\n",
    "        \"dataset\": \"mikoajkolman/pokemon-images-first-generation17000-files\",\n",
    "    }\n",
    ")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Fine-tune the last layer for a few epochs\n",
    "optimizer = torch.optim.SGD(model.fc.parameters(), lr=last_layer_learning_rate, momentum=last_layer_momentum)\n",
    "train(model, train_loader, val_loader, criterion, optimizer, num_epochs=last_layer_epoches)\n",
    "\n",
    "# Unfreeze all the layers and fine-tune the entire network for a few more epochs\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=full_layer_learning_rate, momentum=full_layer_momentum)\n",
    "train(model, train_loader, val_loader, criterion, optimizer, num_epochs=full_layer_epoches)\n",
    "\n",
    "save_file(model.state_dict(), \"models/model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "til-machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
