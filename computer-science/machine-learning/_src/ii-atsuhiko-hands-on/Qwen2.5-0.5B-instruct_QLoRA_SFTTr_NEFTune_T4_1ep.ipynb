{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original code is written by [@Atsuhiko](https://github.com/Atsuhiko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 13 07:10:15 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0  On |                  Off |\n",
      "|  0%   30C    P8             16W /  450W |   20875MiB /  24564MiB |      3%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    242576      C   /python3.12                                 N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本パラメータ\n",
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "peft_name = \"NEFTune_Qwen2.5-0.5B-inst_T4_1ep\"\n",
    "output_dir = \"output_neftune\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリーのインストール\n",
    "import torch\n",
    "import wandb\n",
    "from torch import cuda, bfloat16\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# trl: Transformer Reinfocement Learning, DPOにも対応している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072bc057a7fe4c668cfb5cc04bdf9988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 量子化設定\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# モデルの設定\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    # token=token, # HuggingFaceにログインしておけば不要\n",
    "    quantization_config=bnb_config,  # 量子化\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # attn_implementation=\"flash_attention_2\" # T4では使えないし、RTX4090などの消費者用GPUでも対応していない\n",
    ")\n",
    "\n",
    "# tokenizerの設定\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, padding_side=\"right\", add_eos_token=True\n",
    ")\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ファインチューニング前のモデルでテキスト生成テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大阪の美味しいお酒は、大阪天守りに由来した「大阪天井酒」が挙げられます。この酒は1948年から販売さ\n",
      "れ、現在も大阪の観光地として親しまされています。また、大阪府の名物として知られているのが「味噌汁」と\n",
      "「ラーメン」といった食べ方や特徴があります。\n",
      "CPU times: user 3.92 s, sys: 337 ms, total: 4.25 s\n",
      "Wall time: 4.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"あなたは日本語で回答するアシスタントです。\"},\n",
    "    {\"role\": \"user\", \"content\": \"大阪でおいしいものはなんですか？\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "# terminators = [\n",
    "#     tokenizer.eos_token_id,\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "# ]\n",
    "# Qwen用に変更\n",
    "# Modify the terminators list to only include the eos_token_id\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id\n",
    "]  # Remove tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,  # クリエイティブ（再現性なし）\n",
    "    top_p=0.8,  # 多様な選択肢から単語を選ぶ\n",
    "    pad_token_id=tokenizer.eos_token_id,  # 追加\n",
    "    attention_mask=torch.ones(input_ids.shape, dtype=torch.long).cuda(),  # 追加\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1] :]\n",
    "\n",
    "# print(tokenizer.decode(response, skip_special_tokens=True))\n",
    "\n",
    "import textwrap\n",
    "\n",
    "s = tokenizer.decode(response, skip_special_tokens=True)\n",
    "s_wrap_list = textwrap.wrap(s, 50)  # 50字で改行したリストに変換\n",
    "print(\"\\n\".join(s_wrap_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"./Hands-on/01_Instruction_tuning_QLoRA/dataset\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'わい、思いますねん。 いいえ。\\nステイルメイトとは、引き分けた状態のことやねん。どちらがより多くの駒を捕獲したか、または優勢であるかは関係ない、知らんけど。',\n",
       " 'index': 5,\n",
       " 'instruction': 'ステイルメイトの時に、私の方が多くの駒を持っていたら、私の勝ちですか？'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## プロンプトテンプレートの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "あなたは日本語で回答するアシスタントです<|im_end|>\n",
      "<|im_start|>user\n",
      "ステイルメイトの時に、私の方が多くの駒を持っていたら、私の勝ちですか？<|im_end|>\n",
      "<|im_start|>assistant\n",
      "わい、思いますねん。 いいえ。\n",
      "ステイルメイトとは、引き分けた状態のことやねん。どちらがより多くの駒を捕獲したか、または優勢であるかは関係ない、知らんけど。<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def formatting_func(example):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"あなたは日本語で回答するアシスタントです\"},\n",
    "        {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"output\"]},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "\n",
    "def update_dataset(example):\n",
    "    example[\"text\"] = formatting_func(example)\n",
    "    for field in [\"index\", \"category\", \"instruction\", \"input\", \"output\"]:\n",
    "        example.pop(field, None)\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset = dataset.map(update_dataset)\n",
    "\n",
    "print(dataset[5][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRAパラメータの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['q_proj', 'o_proj', 'up_proj', 'k_proj', 'gate_proj', 'v_proj', 'down_proj']\n"
     ]
    }
   ],
   "source": [
    "# モデルから（4ビット量子化された）線形層の名前を取得する関数\n",
    "# https://zenn.dev/yumefuku/articles/llm-finetuning-qlora?fbclid=IwY2xjawEih_9leHRuA2FlbQIxMQABHXbPcwqf0DgjPSI9dMMqyuQhUV2z1m2QZLepRWytrm3LOLQkHz9lrETzEg_aem_UTJYtvb55qSBL8Qi3Lttwg\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    target_class = bnb.nn.Linear4bit\n",
    "    linear_layer_names = set()\n",
    "    for name_list, module in model.named_modules():\n",
    "        if isinstance(module, target_class):\n",
    "            names = name_list.split(\".\")\n",
    "            layer_name = names[-1] if len(names) > 1 else names[0]\n",
    "            linear_layer_names.add(layer_name)\n",
    "    if \"lm_head\" in linear_layer_names:\n",
    "        linear_layer_names.remove(\"lm_head\")\n",
    "    return list(linear_layer_names)\n",
    "\n",
    "\n",
    "# 線形層の名前を取得\n",
    "target_modules = find_all_linear_names(model)\n",
    "print(target_modules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    # lora_alpha=32,\n",
    "    lora_alpha=16,  # 変更\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=target_modules,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"embed_tokens\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習パラメータの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_steps = 20\n",
    "save_steps = 20\n",
    "logging_steps = 20\n",
    "\n",
    "training_arguments = SFTConfig(\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=3,  # 3エポックに変更\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    group_by_length=True,\n",
    "    # report_to=\"wandb\"\n",
    "    report_to=\"none\",  # 変更\n",
    "    logging_steps=logging_steps,  # 追加\n",
    "    eval_steps=eval_steps,  # 追加\n",
    "    save_steps=save_steps,  # 追加\n",
    "    output_dir=output_dir,  # 追加\n",
    "    save_total_limit=3,  # 追加\n",
    "    push_to_hub=False,\n",
    "    auto_find_batch_size=True,  # 追加：これを入れないとGPUメモリがオーバーフローする\n",
    "    packing=True,\n",
    "    max_seq_length=1024,\n",
    "    neftune_noise_alpha=5,  # ★★NEFTune設定\n",
    "    dataset_text_field=\"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFTrainerの設定、学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:mekf4q24) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f7afd54d5744369f64210563a3ba55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">noble-sun-1</strong> at: <a href='https://wandb.ai/hiroga/Qwen2.5_sftqlora/runs/mekf4q24' target=\"_blank\">https://wandb.ai/hiroga/Qwen2.5_sftqlora/runs/mekf4q24</a><br/> View project at: <a href='https://wandb.ai/hiroga/Qwen2.5_sftqlora' target=\"_blank\">https://wandb.ai/hiroga/Qwen2.5_sftqlora</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241113_062351-mekf4q24/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:mekf4q24). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/hiroga/Documents/GitHub/til/computer-science/machine-learning/_src/ii-atsuhiko-hands-on/wandb/run-20241113_071025-7o2mxiu2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hiroga/Qwen2.5_sftqlora/runs/7o2mxiu2' target=\"_blank\">decent-lion-2</a></strong> to <a href='https://wandb.ai/hiroga/Qwen2.5_sftqlora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hiroga/Qwen2.5_sftqlora' target=\"_blank\">https://wandb.ai/hiroga/Qwen2.5_sftqlora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hiroga/Qwen2.5_sftqlora/runs/7o2mxiu2' target=\"_blank\">https://wandb.ai/hiroga/Qwen2.5_sftqlora/runs/7o2mxiu2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hiroga/Qwen2.5_sftqlora/runs/7o2mxiu2?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f7645045af0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "wandb.init(project=\"Qwen2.5_sftqlora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [123/123 11:34, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.647200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.330700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.087100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.924600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.886100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 43s, sys: 2min 28s, total: 11min 12s\n",
      "Wall time: 11min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()\n",
    "model.config.use_cache = True\n",
    "\n",
    "# QLoRAモデルの保存\n",
    "trainer.model.save_pretrained(peft_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 140,533,760 || all params: 634,566,528 || trainable%: 22.1464\n"
     ]
    }
   ],
   "source": [
    "# 学習したパラメータの比率確認\n",
    "trainer.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
