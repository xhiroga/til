{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ベルマン方程式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 状態価値関数とベルマン方程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def r_revenue(state, action, next_state):\n",
    "    return 1 # TODO\n",
    "\n",
    "def next_state_function(state, action):\n",
    "    next_state = state # TODO\n",
    "    return 1, next_state\n",
    "\n",
    "def expected_sum(policy, state, **revenue_or_return):\n",
    "    # E[X+Y] = E[X] + E[Y] (期待値の線形性による）\n",
    "    result = 0\n",
    "    for func in revenue_or_return:\n",
    "        result += expected(policy, state, revenue_or_return=func)\n",
    "    return result\n",
    "\n",
    "def expected(policy, state, revenue_or_return):\n",
    "    # E[X] = ∑ x P(X=x) x\n",
    "    result = 0\n",
    "    for prob_action, action in policy(state):\n",
    "        for prob_next_state, next_state in next_state_function(state, action):\n",
    "            result += prob_action * prob_next_state * revenue_or_return(state, action, next_state)\n",
    "    return result\n",
    "\n",
    "# 状態価値関数 = 状態sにいることの価値\n",
    "# vπ(s) = E[Gt|St = s, π]\n",
    "def state_value_function(policy, state, gamma=1.0):\n",
    "    # Eπ[Gt+1|St = s] = Eπ[Rt|St = s] + γEπ[Gt+1|St = s]\n",
    "    # = ∑ a,s′ π(a|s) p(s′|s, a) {r(s, a, s′) + γvπ(s′)}\n",
    "    # このように再帰的に計算できる。別の言い方をすれば、漸化式で表せる。これが状態価値関数におけるBellman方程式である。\n",
    "    return expected_sum(policy, state, r_revenue, lambda _state, _action, next_state: gamma * state_value_function(policy, next_state))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 行動価値関数とベルマン方程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行動価値関数 = 状態sにいて行動aを取ったことの価値。なお、それ以降の行動は方策πに従う。\n",
    "# qπ(s, a) = Eπ[Gt|St = s, At = a]\n",
    "def action_value_function(policy, state, action, gamma=1.0):\n",
    "    pass # 状態価値関数とほとんど同じなので省略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ベルマン最適方程式・最適方策"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "復習となるが、方策とは状態を入力すると行動の確率分布を返す関数である。\n",
    "\n",
    "最適な方策とは、決定論的な方策である（行動の候補が複数ある中で、最も収益が高いものを常に100%選ぶ）。\n",
    "\n",
    "したがって、ベルマン方程式のシグマの部分、確率分布と収益の積和は不要であり、単に状態から考えうる最も良い収益を選べば良い。よってmax演算子が使える。\n",
    "\n",
    "（個人的には、ここまで方策には四則演算の関数のような印象だったが、maxのような操作ができるなら、自律的なロボットのように捉えたほうが誤解がなさそうだ）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、最適方策は次の通り。\n",
    "\n",
    "$\\mu_*(s) = \\underset{a}{\\arg\\max} q_*(s, a) = \\underset{a}{\\arg\\max} \\underset{s`}\\sum p(s′|s, a) \\{r(s, a, s′) + γv∗(s′)\\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適方策をPythonで読み下す\n",
    "\n",
    "type State = int\n",
    "type Action = dict[int, float] # {action: probability}\n",
    "\n",
    "# μ∗(s) = argmax a q∗(s, a)\n",
    "def mu_star_policy(state: State) -> Action:\n",
    "    action_candidates = get_action_candidates(state)\n",
    "    return max(action_candidates, key=lambda action: optimal_action_value_function(state, action))\n",
    "\n",
    "def get_action_candidates(state: State) -> Action:\n",
    "    pass # TODO\n",
    "\n",
    "def optimal_action_value_function(state: State, action: Action, gamma=1.0):\n",
    "    pass # TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "til-machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
