---
title: Machine Learning
lang: jp
editor:
  render-on-save: true
---

# Machine Learning

機械学習には次のようなモデルがあります。

| モデル | 分類 | 説明 | 代表的なモデル | 有名な応用例 |
|---|---|---|---|---|
| 線形回帰 | 教師あり学習 | 最も単純な回帰手法であり、入力に対して線形の関係を仮定する | - | 家屋価格の予測、人口増加にP伴う犯罪発生数の予測 |
| ロジスティック回帰 | 教師あり学習 | 二値分類問題や多クラス分類問題に使用され、確率的な分類を行う | - | 顧客の購入意向の予測、スパムメールの判別 |
| 決定木 | 教師あり学習| インスタンスの属性値による条件分岐によって、目的変数を予測する | ランダムフォレスト | データの可視化、健康診断の結果からの疾患予測 |
| k近傍法[^k-NN] | 教師あり学習 | もっとも近いk個のデータ点を参照して、入力データに最も近いカテゴリを予測する | k最近傍法 | 推薦システム |
| サポートベクターマシン | 教師あり学習 |分類と回帰に使用され、高次元空間での線形および非線形分類を行う | - | 手書き数字の認識、がんの検出 |
| ニューラルネットワーク | 多層パーセプトロン、畳み込みニューラルネットワーク、再帰ニューラルネットワークなどがある | LeNet、AlexNet、VGG、ResNetなど | 画像認識、音声認識、自然言語処理、自動運転 |
| XGBoost        | 勾配ブースティングに基づくアンサンブル学

[^k-NN]: k-nearest neighbor algorithm, k-NN

# Neural Network

![ニューラルネットワークの構造](https://miyabi-lab.space/assets/imgs/blog/upload/images/nn_fig17.001_ut1523588254.jpeg "初心者必読！MNIST実行環境の準備から手書き文字識別までを徹底解説！ - MIYABI Lab")

## Sigmoid function

シグモイド関数。ギリシア文字Σの語末系ςに似ていることから、*Sigma(シグマ)*+*-oid(~状のもの)*でシグモイドと呼ぶ。

### Starndard Sigmoid Function

```{python}
import matplotlib.pyplot as plt
import numpy as np

def sigmoid(x: int):
    return 1 / (1 + np.e ** -x)

x = np.linspace(-10, 10, 100)
y = sigmoid(x)
 
fig = plt.figure(figsize = (10, 5))
plt.plot(x, y)
plt.show()
```

## Gradient Descent

機械学習の訓練中に使用される最適化アルゴリズム[^optimizer]の一つ。

[^optimizer]: [【最適化手法】SGD・Momentum・AdaGrad・RMSProp・Adamを図と数式で理解しよう。](https://kunassy.com/oprimizer/)を参照。

訓練中の予測結果と実際の値の誤差を各パラメータに戻し、パラメータを更新することで、誤差が最小になるようにパラメータを更新していく。

## GPT

あえて訳すと生成的な事前訓練を行ったトランスフォーマー、となる。

生成的って何？と思ったが、文章の生成タスクのための、という意味合い。
