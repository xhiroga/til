---
title: Machine Learning (機械学習)
lang: jp
editor:
  render-on-save: true
---

# Machine Learning (機械学習)

機械学習には次のようなモデルがあります。

| モデル | 分類 | 説明 | 代表的なモデル | 応用例 |
|---|---|---|---|---|
| 線形回帰 | 教師あり学習 | 最も単純な回帰手法であり、入力に対して線形の関係を仮定する | - | 家屋価格の予測、人口増加にP伴う犯罪発生数の予測 |
| ロジスティック回帰 | 教師あり学習 | 二値分類問題や多クラス分類問題に使用され、確率的な分類を行う | - | 顧客の購入意向の予測、スパムメールの判別 |
| 決定木 | 教師あり学習| インスタンスの属性値による条件分岐によって、目的変数を予測する | ランダムフォレスト | データの可視化、健康診断の結果からの疾患予測 |
| k近傍法[^k-NN] | 教師あり学習 | もっとも近いk個のデータ点を参照して、入力データに最も近いカテゴリを予測する | k最近傍法 | 推薦システム |
| サポートベクターマシン | 教師あり学習 | 分類と回帰に使用され、高次元空間での線形および非線形分類を行う | - | 手書き数字の認識、がんの検出 |
| ニューラルネットワーク | 教師あり・なし両方 | 生物の神経細胞を真似したモデル。多層のノードが、ニューロンのように信号を処理・伝達する。 | CNN[^cnn]、RNN[^rnn]、オートエンコーダ、Transformer、GAN[^gan]など | 画像認識、音声認識、自然言語処理、自動運転 |

[^k-NN]: k-nearest neighbor algorithm, k-NN
[^cnn]: Convolutional neural network, 畳み込みニューラルネットワーク
[^rnn]: Recurrent neural network, 再帰型（回帰型）ニューラルネットワーク
[^gan]: Generative Adversarial Network, 敵対的生成ネットワーク

## k近傍法

教師あり学習の手法の一つ。ラベルが未知の入力データに対して、入力データと全データの間の距離を測定する。例えば、環境（気温・湿度・風速）から天気を予測する分類問題なら、気温・湿度・風速の3次元空間でのデータ間の距離を測定する。距離はユークリッド距離を使うことが多いが、原理的にはマンハッタン距離などでも構わない。

## Neural Network

![ニューラルネットワークの構造](https://miyabi-lab.space/assets/imgs/blog/upload/images/nn_fig17.001_ut1523588254.jpeg)
> [初心者必読！MNIST実行環境の準備から手書き文字識別までを徹底解説！ - MIYABI Lab](https://miyabi-lab.space/blog/10)

### RNN

### LSTM

### オートエンコーダ

次元削減や特徴抽出で便利なモデル。非線形の構造を持つデータにも有効な点が、PCAなどの従来の削減手法と異なる。1980年代にHintonらによって紹介されたとされる。[^Autoencoders_Unsupervised_Learning_and_Deep_Architectures]
（検索した限り、元論文には"autoencoder"という単語はないようだ。）[^Learning_internal_representations_by_error_propagation]

[^Autoencoders_Unsupervised_Learning_and_Deep_Architectures]: [Baldi, P. (2012, June). Autoencoders, unsupervised learning, and deep architectures. In Proceedings of ICML workshop on unsupervised and transfer learning (pp. 37-49). JMLR Workshop and Conference Proceedings.](http://proceedings.mlr.press/v27/baldi12a/baldi12a.pdf)
: [Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1985). Learning internal representations by error propagation. California Univ San Diego La Jolla Inst for Cognitive Science.](https://cs.uwaterloo.ca/~y328yu/classics/bp.pdf)

2000年代に研究が再燃し、画像のノイズ除去などに用いられるようになる。

### Transformer

#### Self-Attention

#### Encoder/Decoder


## Activation Function

活性化関数。非線形性（non-linearity）の1つ。

### Sigmoid function

シグモイド関数。ギリシア文字Σの語末系ςに似ていることから、*Sigma(シグマ)*+*-oid(~状のもの)*でシグモイドと呼ぶ。

#### Starndard Sigmoid Function

```{python}
import matplotlib.pyplot as plt
import numpy as np

def sigmoid(x: int):
    return 1 / (1 + np.e ** -x)

x = np.linspace(-10, 10, 100)
y = sigmoid(x)
 
fig = plt.figure(figsize = (10, 5))
plt.plot(x, y)
plt.show()
```

### Softmax Function

ソフトマックス関数とは、数値の配列を確率の配列に変換する関数。

```{python}
import numpy as np

def softmax(x: float):
  # 2乗でもよいが、数値が大きくなった時に急激に差をつけられる点・計算コストが少ない点から、自然対数を用いるらしい。
  return np.exp(x) / np.sum(np.exp(x))

x = np.array([3, 1, 2])
y = softmax(x)

print(y)  # [0.66524096 0.09003057 0.24472847]
```

### ReLU

レルー（ランプ関数、正規化線形ユニット（Rectified Linear Unit））は、主にディープニューラルネットワークの中間層で用いられる活性化関数。

```{python}
import matplotlib.pyplot as plt
import numpy as np

def relu(x: int) -> int:
  return np.maximum(x, 0)

x = np.linspace(-4, 4, 100)
y = relu(x)
fig = plt.figure(figsize = (10, 5))
plt.plot(x, y)
plt.show()
```

### GELU

ガウス誤差線形ユニット（Gaussian Error Linear Unit）は、Transformer系のモデルでも採用される活性化関数。

## Gradient Descent

機械学習の訓練中に使用される最適化アルゴリズム[^optimizer]の一つ。

[^optimizer]: [【最適化手法】SGD・Momentum・AdaGrad・RMSProp・Adamを図と数式で理解しよう。](https://kunassy.com/oprimizer/)を参照。

訓練中の予測結果と実際の値の誤差を各パラメータに戻し、パラメータを更新することで、誤差が最小になるようにパラメータを更新していく。

## 
