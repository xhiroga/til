---
title: FAQ
lang: jp
editor:
  render-on-save: true
---

## Sigmoid function

シグモイド関数。ギリシア文字Σの語末系ςに似ていることから、*Sigma(シグマ)*+*-oid(~状のもの)*でシグモイドと呼ぶ。

### Starndard Sigmoid Function

```{python}
import matplotlib.pyplot as plt
import numpy as np

def sigmoid(x: int):
    return 1 / (1 + np.e ** -x)

x = np.linspace(-10, 10, 100)
y = sigmoid(x)
 
fig = plt.figure(figsize = (10, 5))
plt.plot(x, y)
plt.show()
```

## Gradient Descent

機械学習の訓練中に使用される最適化アルゴリズム[^optimizer]の一つ。
[^optimizer]: [【最適化手法】SGD・Momentum・AdaGrad・RMSProp・Adamを図と数式で理解しよう。](https://kunassy.com/oprimizer/)を参照。
訓練中の予測結果と実際の値の誤差を各パラメータに戻し、パラメータを更新することで、誤差が最小になるようにパラメータを更新していく。

## GPT

あえて訳すと生成的な事前訓練を行ったトランスフォーマー、となる。

生成的って何？と思ったが、文章の生成タスクのための、という意味合い。

