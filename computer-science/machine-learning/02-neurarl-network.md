# ニューラルネットワーク (neural network), 深層学習 (deep learning)

ページの構成の際、次の情報源を参考にした。

- [深層学習 改訂第2版](https://www.kspub.co.jp/book/detail/5133323.html)
- [深層学習による画像認識の基礎](https://amzn.to/3wMy8sQ)
- [ゼロから作るDeep Learning ❷](https://amzn.to/3zSlbif)

## 概要

深い[^deep]層を持つニューラルネットワークをディープニューラルネットワークといい、ディープニューラルネットワークを用いた機械学習を深層学習と呼ぶ。

狭義の機械学習ではハンドクラフトで特徴量の抽出を行っていたが、深層学習ではモデルに任せている。

<!-- ここもっと良い書き方ある -->

従来は層を深くすると、学習が進まなくなる課題があった。誤差逆伝播法などのテクニックの導入によって、深い層のネットワークでも学習が進むようになった。

<!-- あれ、逆に誤差逆伝播法以前ってどうやってフィードバックしてたんだっけ？ -->

[^deep]: 何層からが深いかの厳密な定義はない。多くの研究者が、CAPが2より多い場合に深いと考えているという主張がある。[^sugiyama_2019]初期の深層学習の論文[^hinton_2006]では隠れ層が3層あるため、3層以上を深いともいえる。
[^sugiyama_2019]: [Human Behavior and Another Kind in Consciousness](https://amzn.to/3VpevAm) / [Google Books](https://books.google.com/books?id=9CqQDwAAQBAJ&pg=PA15)
[^hinton_2006]: [A fast learning algorithm for deep belief nets](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)

## 活性化関数

活性化関数。非線形性 (non-linearity)の1つ。

### Sigmoid function

シグモイド関数。ギリシア文字Σの語末系ςに似ていることから、*Sigma(シグマ)*+*-oid(~状のもの)*でシグモイドと呼ぶ。

### Standard Sigmoid Function

```python
import matplotlib.pyplot as plt
import numpy as np

def sigmoid(x: int):
    return 1 / (1 + np.e ** -x)

x = np.linspace(-10, 10, 100)
y = sigmoid(x)
 
fig = plt.figure(figsize = (10, 5))
plt.plot(x, y)
plt.show()
```

### Softmax Function

ソフトマックス関数とは、数値の配列を確率の配列に変換する関数。

```python
import numpy as np

def softmax(x: float):
  # 2乗でもよいが、数値が大きくなった時に急激に差をつけられる点・計算コストが少ない点から、自然対数を用いるらしい。
  return np.exp(x) / np.sum(np.exp(x))

x = np.array([3, 1, 2])
y = softmax(x)

print(y)  # [0.66524096 0.09003057 0.24472847]
```

### ReLU

ReLU (正規化線形ユニット, Rectified Linear Unit)またはランプ関数は、主にディープニューラルネットワークの中間層で用いられる活性化関数。

```python
import matplotlib.pyplot as plt
import numpy as np

def relu(x: int) -> int:
  return np.maximum(x, 0)

x = np.linspace(-4, 4, 100)
y = relu(x)
fig = plt.figure(figsize = (10, 5))
plt.plot(x, y)
plt.show()
```

### GELU

ガウス誤差線形ユニット (Gaussian Error Linear Unit)は、Transformer系のモデルでも採用される活性化関数。

## 確率的勾配降下法 (SGD)

機械学習の訓練中に使用される最適化アルゴリズム[^optimizer]の一つ。

[^optimizer]: [【最適化手法】SGD・Momentum・AdaGrad・RMSProp・Adamを図と数式で理解しよう。](https://kunassy.com/oprimizer/)を参照。

訓練中の予測結果と実際の値の誤差を各パラメータに戻し、パラメータを更新することで、誤差が最小になるようにパラメータを更新していく。

## 誤差逆伝播法 (BP, Backpropagation)

単純パーセプトロンは特徴空間内で超平面で分離（線形分離）できる分類問題しか解けない。そのため、多層パーセプトロンが考案された。しかし、当初は多層パーセプトロンを効率的に学習させる方法が見つかっていなかった。例えば、ランダムに重みを更新する、層別に学習させる、などの方法が取られていた可能性がある。その後、ニューラルネットワークを微分可能な関数として見なし、隠れ層の誤差を、次の層の誤差を重みで偏微分して計算する誤差逆伝播法が考案された。

### 勾配消失, 勾配爆発 (vanishing gradient problem, exploding gradient problem)

誤差逆伝播法において、誤差が伝播する内に小さくなって消失したり、または大きくなって発散することがある。この性質は、シグモイド関数を考えると想像しやすい。シグモイド関数では、数字を0~1の値に圧縮する反面、出力から引数を復元するのは難しい。誤差逆伝播法において誤差が消失または発散することを、勾配消失・勾配爆発という。

勾配消失・勾配爆発の対策として、活性化関数の差し替えがある。例えば、値の範囲を圧縮するシグモイド関数に代わって、0以上の値を保つReLUを用いることが考えられる。

また、勾配の値を正規化することも考えられる。同じ層のすべての勾配のノルムが閾値になるように調整すれば良い。これを勾配クリッピングという。

勾配消失・勾配爆発は、時系列データを扱うRNNで特に問題となる。詳しくは[LSTM](#lstm-long-short-term-memory)を参照。

### 残差接続

## 深層学習のアーキテクチャ

### 畳み込みニューラルネットワーク (CNN, Convolutional Neural Network)

画像の分類タスクを考える。例えば、トラとライオンを分類するとする。トラには縞模様があるから、ニューラルネットワークの下位レイヤーは模様を検出しそうだ。そのような働きを促進するため、訓練可能なフィルターを設けることを考える。具体的には、入力データと積和演算を行う適当なサイズの行列（例えば、3x3）を作り、そのフィルターを1ピクセルづつ（この間隔は調整できる）ずらしていく。このような層を畳み込み層という。

また、画像処理は1ピクセルのズレに対して鈍感であってほしい。トラの画像をトリムしたり回転させてもトラであるため。そこで、入力データを縮小することを考える。具体的には、適当なサイズ（例えば、2x2）ごとに入力データのの最大値や平均を取って、新たな行列の要素とする。この対象領域をウィンドウと呼ぶが、例えばウィンドウが2x2なら、入力に対する出力のサイズは1/4になる。このような層をプーリング層という。

### リカレントニューラルネットワーク (RNN, Recurrent Neural Network)

ニューラルネットワークによる文章の生成を考える。文章は単語や文字をベクトル（例：one-hot encodingや単語埋め込み）に変換することでコンピュータが扱える形式になる。そのため、ベクトルを入力にベクトルを出力するニューラルネットワークを考えれば良い。

しかし、単純な実装ではトークン数とベクトルの次元の積の入力が必要となる。この方式は入力可能なトークン数に限度があるし、限度を変更する度に再度訓練が必要になる。そこで、トークンは1つづつ処理することにする。代わりに、前のトークンを入力にした隠れ層を次のトークンの入力と合わせて使う。この隠れ層は過去の情報を要約して保持する役割を果たす。これを循環的に行うことで、入力サイズは固定長でありながら、自由な長さの文字列を扱うことができる。このモデルをRNNという。

個人的な印象としては、RNNの隠れ状態の伝播と全加算器における繰り上がりは似ている。実際、RNNで全加算器を実装した記事も存在する。[^aaaaaaaaaaaaaaaaaa_2021]
[^aaaaaaaaaaaaaaaaaa_2021]: [E検定で出てくるリカレントニューラルネットワークと強化学習について](https://qiita.com/aaaaaaaaaaaaaaaaaa/items/2f6b023066b05dd1b371)

RNNは長い系列を扱える一方で、系列が長くなると過去の情報を保持しにくくなる課題もある。

### LSTM (Long short-term memory)

RNNでは、隠れ層の状態を次のタイムステップにおける同じ隠れ層の入力に用いる（隠れ状態）ことによって状態を扱っている。しかし、隠れ状態は活性化関数を経由するため勾配消失が発生しやすい。また、次のタイムステップのために何を忘れ、何を覚えるかを明示的に学習させることができない。そこで、忘れたり、新たなタイムステップの入力を覚えるにあたって、活性化関数を用いない状態を別途設ける。これをセル状態と呼ぶ。隠れ層の出力にあたっては、セル状態と隠れ状態の2つを入力として計算する。しかし、隠れ状態とは別にセル状態も出力し、どちらも次のタイムステップの入力となる。

セル状態について、隠れ状態と入力を元に何を忘れるかを選択する部品をforgetゲートと呼ぶ。また、隠れ状態と入力を元に新たなタイムステップの情報を追加する部品をinputゲート、セル状態と隠れ状態を合わせて出力する部品をoutputゲートと呼ぶ。セル状態と3つのゲートを備えたRNNをLSTMと呼び、短期記憶を長期に渡って保持することを指す。

Elman RNN(LSTMではないシンプルなRNNを指す)やLSTMは、分類や予測・生成タスクに用いることができる。分類タスクの場合は最後のタイムステップの出力のみを用いる。

### seq2seq

LSTMでは、系列の分類タスクや、系列のタイムステップと出力が1:1で対応するようなタスクを処理できた。次に、系列から系列を予測・生成するが、タイムステップどうしが必ずしも1:1で対応しないタスクを考える。例えば、日本語から英語への翻訳である。

系列から系列の変換タスクでは、一度全ての入力情報が隠れ状態に含まれるのを待ち、それから出力を始める。したがって、入力が完了したことを示す工夫などが求められる。そこで、入力を隠れ状態へと変換するLSTM（エンコーダ）と、隠れ状態を元に系列を生成するLSTM（デコーダ）を繋げることを考える。2つのLSTMで役割を分担することで、エンコード・デコードに特化した学習や機能の導入が簡単になる。このようなアーキテクチャをエンコーダ・デコーダモデルといい、エンコーダ・デコーダモデルを含む系列から系列への変換を行うモデルをseq2seqという。

### seq2seq with attention

エンコーダ・デコーダモデルによる翻訳タスクを考える。固定長である。

### グラフニューラルネットワーク (GNN)

（要レビュー）ニューラルネットワークは一般的に、データを多次元変数として捉えた上で、変数の重み付きの和を新たな次元とすることで特徴量を自動で作る。CNNでは周辺のマスの重み付き和を、Transformerでは全範囲の重み付き和を用いる。これは、入力の範囲をグラフ構造で与えることで一般化できる。物体の各点が近い点からの相互作用を受けることに着目し、GNNを用いて自然なシミュレーションを行った応用などがある。[^joisino_2024]
[^joisino_2024]: [僕たちがグラフニューラルネットワークを学ぶ理由](https://speakerdeck.com/joisino/pu-tatigagurahuniyurarunetutowakuwoxue-buli-you)

## 推論の信頼性

## 説明と可視化

## 深層学習のいろいろな学習手法

### 継続, 追加学習

### 知識蒸留

### 枝刈り

### 量子化

### ネットワーク構造探索 (NAS)

## データが少ない場合の学習
