# 創造情報学 第1問

## (1)

$$
\begin{align}
\binom{N}{r}\theta^{r}(1-\theta)^{N-r}
\end{align}
$$

## (2)

対数尤度関数は次の通り。

なお、対数の扱いを分かっていることを示すため、対数の基本的な法則の範囲で式を展開しておく

$$
\begin{align}
\ln(\binom{N}{r}\theta^{r}(1-\theta)^{N-r}) =\ln\binom{N}{r} + r\ln\theta + (N-r)\ln(1-\theta)
\end{align}
$$

二項分布の対数尤度関数は下に凸であることが知られているため、導関数が0のとき値が最大となる。よって次の方程式を解くことで、対数尤度関数を最大にする$\theta$を求める。

$$
\begin{align}
\frac{\partial}{\partial \theta}\ln\binom{N}{r} + r\ln\theta + (N-r)\ln(1-\theta) &= 0 \\
\frac{r}{\theta} + \frac{(N-r)}{(1-\theta)} &= 0 \\
r(\theta-1) + (N-r)\theta &= 0 \\
r\theta - r + N\theta - r\theta &= 0 \\
\theta &= \frac{r}{N}
\end{align}
$$

## (3)

$x_1,x_2,...,x_n$の同時確率を考え、それを対数尤度にする。

$$
\begin{align}
\ln(\prod_{i=1}^N f(x_i;\mu,\sigma^2)) &= \ln(\prod_{i=1}^N\{\frac{1}{\sqrt{2\pi\sigma}}\exp[-\frac{(x_i-\mu)^2}{2\sigma^2}]\}) \\
&= \sum_{i=1}^N \{ -\frac{1}{2}\ln 2\pi\sigma -\frac{(x_i-\mu)^2}{2\sigma^2}\}
\end{align}
$$

## (4)

平均$\mu$、分散$\sigma^2$について対数最大尤度を最大化する値を求める問題。

(3)の対数尤度関数が下に凸であることが知られているため、導関数を0とおいて方程式を解く。

$\mu$について次の通り。

$$
\begin{align}
\frac{\partial}{\partial\mu}\sum_{i=1}^N \{ -\frac{1}{2}\ln 2\pi\sigma^2 -\frac{(x_i-\mu)^2}{2\sigma^2}\} &= 0 \\
\sum_{i=1}^N \frac{2(x_i-\mu)}{2\sigma^2} &= 0 \\
\sum_{i=1}^N (x_i-\mu) &= 0 \\
\sum_{i=1}^N x_i - N\mu &= 0 \\
\mu &= \frac{\sum_{i=1}^N x_i}{N}
\end{align}
$$

$\mu$の最尤推定値$\hat{\mu}$を用いて、$\sigma^2$の最尤推定値を求める。

$$
\begin{align}
\frac{\partial}{\partial\sigma^2}\sum_{i=1}^N \{ -\frac{1}{2}\ln 2\pi\sigma^2 -\frac{(x_i-\hat{\mu})^2}{2\sigma^2}\} &= 0 \\
\sum_{i=1}^N \{ -\frac{1}{2\sigma^2} +\frac{(x_i-\hat{\mu})^2}{2\sigma^4}\} &= 0 \\
\sum_{i=1}^N \{ \sigma^2 -(x_i-\hat{\mu})^2\} &= 0 \\
\sigma &= \frac{\sum_{i=1}^N(x_i-\hat{\mu})^2}{N}
\end{align}
$$

## (5)

## (6)

EMアルゴリズムが確かに尤度関数を極大化することの証明問題。

なお、読みやすさのため$\theta'$を$\theta^{old}$, $\Theta'$を$\Theta^{old}$とおく。

1. step1: $\theta = \argmax_{\theta^{old}} G(\Theta, \theta^{old})$より、step1による$\theta$の更新は$G(\Theta, \theta)$を最大化する
2. step2: $\Theta = \argmax_{\Theta^{old}} G(\Theta^{old}, \theta)$より、step1による$\Theta$の更新は$G(\Theta, \theta)$を最大化する
3. $D(\Theta) = \max_\theta G(\Theta, \theta)$ より、$G(\Theta, \theta)$の増加は$D(\Theta)$を増加させる

## (7)

混合ガウス分布の対数尤度の下界としてELBOが利用できることを示すために、Jensen(イェンゼン)の不等式を証明する問題。
