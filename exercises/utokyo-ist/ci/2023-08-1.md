# 2024年度 創造情報学 第1問

## (1)

$$
\begin{align}
\binom{N}{r}\theta^{r}(1-\theta)^{N-r}
\end{align}
$$

## (2)

対数尤度関数は次の通り。

なお、対数の扱いを分かっていることを示すため、対数の基本的な法則の範囲で式を展開しておく

$$
\begin{align}
\ln(\binom{N}{r}\theta^{r}(1-\theta)^{N-r}) =\ln\binom{N}{r} + r\ln\theta + (N-r)\ln(1-\theta)
\end{align}
$$

二項分布の対数尤度関数は下に凸であることが知られているため、導関数が0のとき値が最大となる。よって次の方程式を解くことで、対数尤度関数を最大にする$\theta$を求める。

$$
\begin{align}
\frac{\partial}{\partial \theta}\ln\binom{N}{r} + r\ln\theta + (N-r)\ln(1-\theta) &= 0 \\
\frac{r}{\theta} + \frac{(N-r)}{(1-\theta)} &= 0 \\
r(\theta-1) + (N-r)\theta &= 0 \\
r\theta - r + N\theta - r\theta &= 0 \\
\theta &= \frac{r}{N}
\end{align}
$$

## (3)

$x_1,x_2,...,x_n$の同時確率を考え、それを対数尤度にする。

$$
\begin{align}
\ln(\prod_{i=1}^N f(x_i;\mu,\sigma^2)) &= \ln(\prod_{i=1}^N\{\frac{1}{\sqrt{2\pi\sigma}}\exp[-\frac{(x_i-\mu)^2}{2\sigma^2}]\}) \\
&= \sum_{i=1}^N \{ -\frac{1}{2}\ln 2\pi\sigma -\frac{(x_i-\mu)^2}{2\sigma^2}\}
\end{align}
$$

> [!WARNING]
> 展開したほうが望ましい。時間がある時に書き直す。

## (4)

平均$\mu$、分散$\sigma^2$について対数最大尤度を最大化する値を求める問題。

(3)の対数尤度関数が下に凸であることが知られているため、導関数を0とおいて方程式を解く。

$\mu$について次の通り。

$$
\begin{align}
\frac{\partial}{\partial\mu}\sum_{i=1}^N \{ -\frac{1}{2}\ln 2\pi\sigma^2 -\frac{(x_i-\mu)^2}{2\sigma^2}\} &= 0 \\
\sum_{i=1}^N \frac{2(x_i-\mu)}{2\sigma^2} &= 0 \\
\sum_{i=1}^N (x_i-\mu) &= 0 \\
\sum_{i=1}^N x_i - N\mu &= 0 \\
\mu &= \frac{\sum_{i=1}^N x_i}{N}
\end{align}
$$

$\mu$の最尤推定値$\hat{\mu}$を用いて、$\sigma^2$の最尤推定値を求める。

$$
\begin{align}
\frac{\partial}{\partial\sigma^2}\sum_{i=1}^N \{ -\frac{1}{2}\ln 2\pi\sigma^2 -\frac{(x_i-\hat{\mu})^2}{2\sigma^2}\} &= 0 \\
\sum_{i=1}^N \{ -\frac{1}{2\sigma^2} +\frac{(x_i-\hat{\mu})^2}{2\sigma^4}\} &= 0 \\
\sum_{i=1}^N \{ \sigma^2 -(x_i-\hat{\mu})^2\} &= 0 \\
\sigma^2 &= \frac{\sum_{i=1}^N(x_i-\hat{\mu})^2}{N}
\end{align}
$$

## (5)

混合ガウスモデルの対数尤度関数を設計する問題。潜在変数$Z$が生成される確率は次の通り。

$$
\begin{align}
p(Z|\Theta)=\prod_{i=1}^N\pi_{z_i}
\end{align}
$$

また、潜在変数$Z$が与えられた元での$X$の生成確率は次の通り。

$$
\begin{align}
p(X|Z,\Theta)=\prod_{n=1}^N f(x_n;\mu_{z_n},\sigma_{z_n}^2)
\end{align}
$$

求める確率は、観測変数$X$の生成確率なので、$p(X|Z,\Theta)$を$p(Z|\Theta)$で周辺化すれば良い。

$$
\begin{align}
p(X|\Theta) = \prod_{n=1}^N\sum_{k=1}^K(\pi_k f(x_n;\mu_k,\sigma_k^2))
\end{align}
$$

よって、対数尤度関数は次の通り。

$$
\begin{align}
\ln L(X|\Theta) &= \ln(\prod_{n=1}^N\sum_{k=1}^K(\pi_k f(x_n;\mu_k,\sigma_k^2))) \\
&= \sum_{n=1}^N\ln(\sum_{k=1}^K\pi_k f(x_n;\mu_k,\sigma_k^2))
\end{align}
$$

## (6)

EMアルゴリズムが確かに尤度関数を極大化することの証明問題。

なお、読みやすさのため$\theta'$を$\theta^{old}$, $\Theta'$を$\Theta^{old}$とおく。

1. step1: $\theta = \argmax_{\theta^{old}} G(\Theta, \theta^{old})$より、step1による$\theta$の更新は$G(\Theta, \theta)$を最大化する
2. step2: $\Theta = \argmax_{\Theta^{old}} G(\Theta^{old}, \theta)$より、step1による$\Theta$の更新は$G(\Theta, \theta)$を最大化する
3. $D(\Theta) = \max_\theta G(\Theta, \theta)$ より、$G(\Theta, \theta)$の増加は$D(\Theta)$を増加させる

## (7)

混合ガウス分布の対数尤度の下界としてELBOが利用できることを示すために、Jensen(イェンゼン)の不等式を証明する問題。問題で示される不等式は次の通り。ただし、$\sum_i\lambda_i=1, (0<\lambda_i<1)$である。

$$
\begin{align}
\ln(\sum_i \lambda_i y_i) \ge \sum_i \lambda_i \ln(y_i)
\end{align}
$$

対数関数は下に凸な関数であるため、任意の$x_1$,$x_2$,$\lambda (0<\lambda<1)$について、$\ln(\lambda x_1 + (1-\lambda)x_2) \ge \ln(\lambda x_1) + \ln((1-\lambda)x_2)$が成り立つことが知られている。

ここで命題を眺めると、$\sum_{i=1}^N\lambda_i=1$であるなら、$\lambda_1+\sum_{i=2}^N\lambda_i=1$も成り立つことが分かる。これを用いて、帰納法で証明すれば良い。

> [!WARNING]
> 帰納法もきちんと書いた方が良い気がする。

## (8)

EMアルゴリズムのEステップを設計する問題。

$\lambda_{nk}$を、観測変数$x_n$ごとに異なる分布（負担率）で潜在変数$z_n$の値を最適化する変数とする。問題文の案内の通り、(5)の対数尤度関数を変形して$\lambda_{nk}$を導入する。

$$
\begin{align}
\ln L(X|\Theta) &= \sum_{n=1}^N\ln(\sum_{k=1}^K\lambda_{nk}\frac{\pi_k f(x_n;\mu_k,\sigma_k^2)}{\lambda_{nk}})
\end{align}
$$

Jensenの不等式により、次の不等式が成り立つ。

$$
\begin{align}
\sum_{n=1}^N\ln(\sum_{k=1}^K\lambda_{nk}\frac{\pi_k f(x_n;\mu_k,\sigma_k^2)}{\lambda_{nk}}) \ge \sum_{n=1}^N\sum_{k=1}^K\lambda_{nk}\ln(\frac{\pi_k f(x_n;\mu_k,\sigma_k^2)}{\lambda_{nk}})
\end{align}
$$

よって、(5)の対数尤度関数を次の通り不等式に変形できる。

$$
\begin{align}
\ln L(X|\Theta) \ge \sum_{n=1}^N\sum_{k=1}^K\lambda_{nk}\ln(\frac{\pi_k f(x_n;\mu_k,\sigma_k^2)}{\lambda_{nk}})
\end{align}
$$

変形した式は求めたい対数尤度関数の下界を成すため、補助関数として機能する。よって次の通り定義する。

$$
\begin{align}
G(X|\Theta,\lambda) &= \sum_{n=1}^N\sum_{k=1}^K\lambda_{nk}\ln(\frac{\pi_k f(x_n;\mu_k,\sigma_k^2)}{\lambda_{nk}}) \\
&= \sum_{n=1}^N\sum_{k=1}^K\lambda_{nk}(\ln\pi_k + \ln f(x_n;\mu_k,\sigma_k^2) - \ln\lambda_{nk})
\end{align}
$$

## (9)

[step 1]として、補助関数を最大化する$\lambda_{nk}$を求める。

$\lambda_{nk}$はすなわち$x_n$が与えられた上での$z_k$の条件付き確率（負担率）である。よって次の形に持ち込むことを念頭に置いて式の展開を行う。

$$
\begin{align}
\lambda_{nk} = P(z_n=k|x_n,\Theta) = \frac{P(x_n,z_n=k|\Theta)}{P(x_n|\Theta)}
\end{align}
$$

さて、$\lambda_{nk}$には、$\sum_{k=1}^K\lambda_{nk}=1$の制約が存在する。制約を含めた形で最適化を行うため、ラグランジュ関数を次の通り定義する。

$$
\begin{align}
L(X|\Theta,\nu_n) = \sum_{n=1}^N\sum_{k=1}^K\lambda_{nk}(\ln\pi_k + \ln f(x_n;\mu_k,\sigma_k^2) - \ln\lambda_{nk}) + \nu_n(1-\sum_{k=1}^K\lambda_{nk})
\end{align}
$$

ラグランジュ関数を最大化する$\lambda_{nk}$を解く。

$$
\begin{align}
\frac{\partial{L}}{\partial{\lambda_{nk}}} \sum_{n=1}^N\sum_{k=1}^K\lambda_{nk}(\ln\pi_k + \ln f(x_n;\mu_k,\sigma_k^2) - \ln\lambda_{nk}) + \nu_n(1-\sum_{n=1}^N\lambda_{nk}) &= 0 \\
\ln\pi_k + \ln f(x_n;\mu_k,\sigma_k^2) - \ln\lambda_{nk} - 1 - \nu_n &= 0 \\
\ln\pi_k + \ln f(x_n;\mu_k,\sigma_k^2) + \ln\frac{1}{\mathrm{e}} + \ln\frac{1}{\exp(\nu_n)} &= \ln\lambda_{nk} \\
\ln(\pi_k \cdot f(x_n;\mu_k,\sigma_k^2) \cdot \frac{1}{\mathrm{e}} \cdot \frac{1}{\exp(\nu_n)}) &= \ln\lambda_{nk} \\
\frac{\pi_k \cdot f(x_n;\mu_k,\sigma_k^2)}{\exp(1+\nu_n)} &= \lambda_{nk} \\
\end{align}
$$

$\exp(1+\nu_n)$を解く。ラグランジュ関数によって最大化した$\lambda_{nk}$の値を、制約に代入する。

$$
\begin{align}
\sum_{k=1}^K \lambda_{nk} &= 1 \\
\sum_{k=1}^K \frac{\pi_k \cdot f(x_n;\mu_k,\sigma_k^2)}{\exp(1+\nu_n)} &= 1 \\
\sum_{k=1}^K \pi_k \cdot f(x_n;\mu_k,\sigma_k^2) &= \exp(1+\nu_n)
\end{align}
$$

よって、$\lambda_{nk}$が次の通り求まる。これは念頭に置いた負担率の式とも一致する。

$$
\begin{align}
\lambda_{nk} &= \frac{\pi_k \cdot f(x_n;\mu_k,\sigma_k^2)}{\sum_{k=1}^K \pi_k \cdot f(x_n;\mu_k,\sigma_k^2)}
\end{align}
$$

[step 2]として、補助変数$\lambda_{nk}$を固定し、補助関数を最大化するパラメータ$\Theta$を求める。(8)で定義した補助関数を再び示す。続く変形のために、ガウス分布も展開しておく。（紛らわしいが、$\pi_k$はカテゴリカル分布の確率、$\frac{1}{\sqrt{2\pi\sigma}}$の$\pi$は3.14...の定数と、異なる値であることに注意。）

$$
\begin{align}
G(X|\Theta,\lambda) &= \sum_{n=1}^N\sum_{k=1}^K\lambda_{nk}(\ln\pi_k + \ln f(x_n;\mu_k,\sigma_k^2) - \ln\lambda_{nk}) \\
&= \sum_{n=1}^N\sum_{k=1}^K\lambda_{nk}(\ln\pi_k + \ln(\frac{1}{\sigma_k\sqrt{2\pi}}\exp[-\frac{(x_n-\mu_k)^2}{2\sigma_k^2}]) - \ln\lambda_{nk}) \\
&= \sum_{n=1}^N\sum_{k=1}^K\lambda_{nk}(\ln\pi_k - \frac{1}{2}\ln2\pi-\ln\sigma_k-\frac{(x_n-\mu_k)^2}{2\sigma_k^2} - \ln\lambda_{nk}) \\
\end{align}
$$

$\pi_k$について、$\sum_{k=1}^K\pi_k = 1$の制約があるため、ラグランジュ関数を次の通り定義する。

$$
\begin{align}
L(\pi,\eta)=\frac{\partial}{\partial\pi_{k}}\sum_{n=1}^N\sum_{k=1}^K\lambda_{nk}(\ln\pi_k - \frac{1}{2}\ln2\pi-\ln\sigma_k-\frac{(x_n-\mu_k)^2}{2\sigma_k^2} - \ln\lambda_{nk}) + \eta(1-\sum_{k=1}^K\pi_k)
\end{align}
$$

ラグランジュ関数を0とおいて$\pi_k$を解く。

$$
\begin{align}
\frac{\partial}{\partial\pi_{k}}\sum_{n=1}^N\sum_{k=1}^K\lambda_{nk}(\ln\pi_k - \frac{1}{2}\ln2\pi-\ln\sigma_k-\frac{(x_n-\mu_k)^2}{2\sigma_k^2} - \ln\lambda_{nk}) + \eta(1-\sum_{k=1}^K\pi_k) &= 0 \\
\sum_{n=1}^N\frac{\lambda_{nk}}{\pi_k} - \eta &= 0 \\
\sum_{n=1}^N\frac{\lambda_{nk}}{\pi_k} &= \eta \\
\frac{1}{\eta}\sum_{n=1}^N\lambda_{nk} &= \pi_k \\
\end{align}
$$

$\eta$を求める。ラグランジュ関数によって最大化した$\pi_k$の値を、$\sum_{k=1}^K\pi_k = 1$に代入する。

$$
\begin{align}
\sum_{k=1}^K\frac{1}{\eta}\sum_{n=1}^N\lambda_{nk} &= 1 \\
\sum_{k=1}^K\sum_{n=1}^N\lambda_{nk} &= \eta \\
N &= \eta
\end{align}
$$

よって、次の通り$\pi_k$が求まる。負担率の平均と考えれば妥当である。

$$
\begin{align}
\pi_k &= \frac{1}{N}\sum_{n=1}^N\lambda_{nk}
\end{align}
$$

$\mu_k$を求める。

$$
\begin{align}
\frac{\partial}{\partial\mu_k}\sum_{n=1}^N\sum_{k=1}^K\lambda_{nk}(\ln\pi_k - \frac{1}{2}\ln2\pi-\ln\sigma_k-\frac{(x_n-\mu_k)^2}{2\sigma_k^2} - \ln\lambda_{nk}) &= 0 \\
\frac{\partial}{\partial\mu_k}\sum_{n=1}^N\lambda_{nk}(-\frac{1}{2\sigma_k^2}(x_n-\mu_k)^2) &= 0 \\
\sum_{n=1}^N\lambda_{nk}(\frac{1}{\sigma_k^2}(x_n-\mu_k)) &= 0 \\
\sum_{n=1}^N\lambda_{nk}(x_n-\mu_k) &= 0 \\
\sum_{n=1}^N\lambda_{nk}x_n-\sum_{n=1}^N\lambda_{nk}\mu_k &= 0 \\
\sum_{n=1}^N\lambda_{nk}x_n &= \sum_{n=1}^N\lambda_{nk}\mu_k \\
\frac{\sum_{n=1}^N\lambda_{nk}x_n}{\sum_{n=1}^N\lambda_{nk}} &= \mu_k
\end{align}
$$

平均を計算するとき、Nに代わって$k={1,2...,K}$ごとに所属するデータ数の期待値を用いていると考えられ、妥当である。

最後に$\sigma^2$を求める。

$$
\begin{align}
\frac{\partial}{\partial\sigma_k^2}\sum_{n=1}^N\sum_{k=1}^K\lambda_{nk}(\ln\pi_k - \frac{1}{2}\ln2\pi-\ln\sigma_k-\frac{(x_n-\mu_k)^2}{2\sigma_k^2} - \ln\lambda_{nk}) &= 0 \\
\frac{\partial}{\partial\sigma_k^2}\sum_{k=1}^K\lambda_{nk}(-\ln\sigma_k-\frac{(x_n-\mu_k)^2}{2\sigma_k^2}) &= 0 \\
\frac{\partial}{\partial\sigma_k^2}\sum_{k=1}^K\lambda_{nk}(-\frac{1}{2}\ln\sigma_k^2-\frac{(x_n-\mu_k)^2}{2\sigma_k^2}) &= 0 \\
\sum_{k=1}^K\lambda_{nk}(-\frac{1}{2\sigma_k^2}+\frac{(x_n-\mu_k)^2}{2\sigma_k^4}) &= 0 \\
\sum_{k=1}^K\lambda_{nk}(-\sigma_k^2+(x_n-\mu_k)^2) &= 0 \\
\sum_{k=1}^K\lambda_{nk}(x_n-\mu_k)^2 &= \sum_{k=1}^K\lambda_{nk}\sigma_k^2 \\
\frac{\sum_{k=1}^K\lambda_{nk}(x_n-\mu_k)^2}{\sum_{k=1}^K\lambda_{nk}} &= \sigma_k^2
\end{align}
$$
