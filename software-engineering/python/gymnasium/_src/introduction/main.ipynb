{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as th\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "class CustomCallBack(BaseCallback):\n",
    "    def __init__(self, log_dir, verbose=0):\n",
    "        super(CustomCallBack, self).__init__(verbose)\n",
    "        self.log_path = f\"logs/{log_dir}\"\n",
    "        os.makedirs(self.log_path, exist_ok=True)\n",
    "        self.before_update_param = None\n",
    "        self.update_counter = 0\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        if self.before_update_param is not None:\n",
    "            self.compare_weights()\n",
    "\n",
    "        self.before_update_param = {\n",
    "            name: param.clone() for name, param in self.model.policy.named_parameters()\n",
    "        }\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        self._write_log(\"on_step\", f\"{self.n_calls=}, {self.locals=}\")\n",
    "        return True\n",
    "\n",
    "    # _on_rollout_endの終了後にポリシーが更新されるため、このタイミングで利用可能になる新しい情報はない...はず...\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        self.update_counter += 1\n",
    "\n",
    "    def compare_weights(self):\n",
    "        for name, param in self.model.policy.named_parameters():\n",
    "            before = self.before_update_param[name]\n",
    "            after = param.clone()\n",
    "            mean_diff = th.mean(after - before).item()\n",
    "            max_diff = th.max(after - before).item()\n",
    "            self._write_log(\n",
    "                \"compare_weights\",\n",
    "                f\"Update #{self.update_counter} Layer {name} | {mean_diff=} | {max_diff=}\",\n",
    "            )\n",
    "\n",
    "    def _write_log(self, file_name, message):\n",
    "        with open(f\"{self.log_path}/{file_name}.log\", \"a\") as f:\n",
    "            f.write(message + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 115      |\n",
      "|    ep_rew_mean     | -432     |\n",
      "| time/              |          |\n",
      "|    fps             | 46       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 128      |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 104          |\n",
      "|    ep_rew_mean          | -295         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 256          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002175998 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | -0.006083846 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.76e+03     |\n",
      "|    n_updates            | 5            |\n",
      "|    policy_gradient_loss | -0.000859    |\n",
      "|    value_loss           | 6.38e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 102           |\n",
      "|    ep_rew_mean          | -368          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 8             |\n",
      "|    total_timesteps      | 384           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035119988 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | -0.008388877  |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 692           |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.00189      |\n",
      "|    value_loss           | 1.52e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 100           |\n",
      "|    ep_rew_mean          | -361          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 11            |\n",
      "|    total_timesteps      | 512           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015371805 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | -0.050677776  |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.67e+03      |\n",
      "|    n_updates            | 15            |\n",
      "|    policy_gradient_loss | -0.000709     |\n",
      "|    value_loss           | 4.15e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 98.2          |\n",
      "|    ep_rew_mean          | -332          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 13            |\n",
      "|    total_timesteps      | 640           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024322374 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | -0.010666251  |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.27e+03      |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00265      |\n",
      "|    value_loss           | 4.5e+03       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 99            |\n",
      "|    ep_rew_mean          | -321          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 16            |\n",
      "|    total_timesteps      | 768           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00056962576 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | -0.009985805  |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 459           |\n",
      "|    n_updates            | 25            |\n",
      "|    policy_gradient_loss | -0.00376      |\n",
      "|    value_loss           | 900           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -290          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 19            |\n",
      "|    total_timesteps      | 896           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00034222845 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | -0.042557     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 687           |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00115      |\n",
      "|    value_loss           | 1.16e+03      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 101          |\n",
      "|    ep_rew_mean          | -267         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 1024         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018412392 |\n",
      "|    clip_fraction        | 0.00156      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.012295246 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 768          |\n",
      "|    n_updates            | 35           |\n",
      "|    policy_gradient_loss | -0.00795     |\n",
      "|    value_loss           | 1.08e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x166f5fb90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "model = PPO(\"MlpPolicy\", env, n_steps=128, n_epochs=5, batch_size=16, verbose=1)\n",
    "model.learn(total_timesteps=1024, callback=CustomCallBack(\"luna_lander\", verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
