{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CaboCha\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "df: DataFrame = pd.read_table('./data/ai.ja/ai.ja.txt', header=None, names=['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CaboCha import Tree\n",
    "from typing import List\n",
    "\n",
    "def printTree(tree: Tree):\n",
    "    print(tree.toString(CaboCha.FORMAT_LATTICE))\n",
    "\n",
    "def headTrees(trees: List[Tree], from_int: int, to_int: int) -> None:\n",
    "    for tree in trees[from_int:to_int]:\n",
    "        printTree(tree)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse()の返り値が最新のTreeオブジェクトになっている問題に対する試み(1)\n",
    "# toString() を呼び出せばポイント参照から実態の参照へと切り替わるのでは？と期待したのだが、失敗。\n",
    "# def noLazyParse(sentence: str) -> Tree:\n",
    "#     parsed = c.parse(sentence)\n",
    "#     parsed.toString(CaboCha.FORMAT_LATTICE)\n",
    "#     return parsed\n",
    "\n",
    "# trees = df['sentence'].map(noLazyParse)\n",
    "# trees.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse()の返り値が最新のTreeオブジェクトになっている問題に対する試み(2)\n",
    "# いっそ要素ごとにParserを作ってしまおう、とやってみたが、iPhythonのKernelが落ちる。\n",
    "# trees = df['sentence'].map(CaboCha.Parser().parse)\n",
    "# trees.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    * 0 -1D 1/1 0.000000\\n人工\\t名詞,一般,*,*,*,*,人工,ジンコ...\n",
       "1    * 0 17D 1/1 0.388993\\n人工\\t名詞,一般,*,*,*,*,人工,ジンコ...\n",
       "2    * 0 1D 6/7 3.194287\\n『\\t記号,括弧開,*,*,*,*,『,『,『\\n...\n",
       "3    * 0 1D 0/1 1.813378\\n人間\\t名詞,一般,*,*,*,*,人間,ニンゲン...\n",
       "4    * 0 1D 1/2 0.906609\\nプログラミング\\t名詞,サ変接続,*,*,*,*,...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse()の返り値が最新のTreeオブジェクトになっている問題に対する試み(3)\n",
    "# parse()実行直後にプロパティをコピーする。\n",
    "# 上手く行ったと思っていたが、後々問題があることが分かった。ChunkかTokenのオブジェクトがSwingObjectなのだろうか？\n",
    "\n",
    "from CaboCha import Tree\n",
    "class TreeObject:\n",
    "    def __init__(self, tree: Tree):\n",
    "        self._str = tree.toString(CaboCha.FORMAT_LATTICE)\n",
    "        self.sentence = tree.sentence()\n",
    "        self.chunks = [tree.chunk(i) for i in range(tree.chunk_size())]\n",
    "        self.tokens = [tree.token(i) for i in range(tree.token_size())]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self._str\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self._str\n",
    "\n",
    "c = CaboCha.Parser()\n",
    "\n",
    "trees = df['sentence'].map(lambda s: TreeObject(c.parse(s)))\n",
    "trees.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40. 係り受け解析結果の読み込み（形態素）\n",
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(self.surface, self.base, self.pos, self.pos1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "class ChunkObject():\n",
    "    def __init__(self, morphs, dst):\n",
    "        self.morphs = morphs\n",
    "        self.dst = dst\n",
    "        self.srcs = []\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'morphs[{}]\\tdst[{}]\\tsrcs[{}]'.format(self.morphs, self.dst, self.srcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk.link: 20\n",
      "chunk.token_pos: 0\n",
      "chunk.token_size: 2\n",
      "tokens: ['対談', 'で']\n",
      "chunk.link: 16\n",
      "chunk.token_pos: 2\n",
      "chunk.token_size: 2\n",
      "tokens: ['須藤', 'は']\n",
      "chunk.link: 6\n",
      "chunk.token_pos: 4\n",
      "chunk.token_size: 3\n",
      "tokens: ['「', 'これ', 'まで']\n",
      "chunk.link: 6\n",
      "chunk.token_pos: 7\n",
      "chunk.token_size: 1\n",
      "tokens: ['けっこう']\n",
      "chunk.link: 6\n",
      "chunk.token_pos: 8\n",
      "chunk.token_size: 1\n",
      "tokens: ['長時間']\n",
      "chunk.link: 6\n",
      "chunk.token_pos: 9\n",
      "chunk.token_size: 2\n",
      "tokens: ['議論', 'を']\n",
      "chunk.link: 15\n",
      "chunk.token_pos: 11\n",
      "chunk.token_size: 6\n",
      "tokens: ['行っ', 'て', 'き', 'まし', 'た', '。']\n",
      "chunk.link: 10\n",
      "chunk.token_pos: 17\n",
      "chunk.token_size: 3\n",
      "tokens: ['おかげ', 'で', '、']\n",
      "chunk.link: 9\n",
      "chunk.token_pos: 20\n",
      "chunk.token_size: 2\n",
      "tokens: ['意見', 'の']\n",
      "chunk.link: 10\n",
      "chunk.token_pos: 22\n",
      "chunk.token_size: 2\n",
      "tokens: ['違い', 'は']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def debugCabochaChunk(chunk: CaboCha.Chunk, tokens: List[CaboCha.Token]) -> None:\n",
    "    print(f\"chunk.link: {chunk.link}\")\n",
    "    print(f\"chunk.token_pos: {chunk.token_pos}\")\n",
    "    print(f\"chunk.token_size: {chunk.token_size}\")\n",
    "    print(f\"tokens: {list(map(lambda t: t.surface, tokens[chunk.token_pos:chunk.token_pos + chunk.token_size]))}\")\n",
    "\n",
    "[debugCabochaChunk(chunk, trees[1].tokens) for chunk in trees[1].chunks[0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface: 対談\n",
      "pos: 対談\n"
     ]
    }
   ],
   "source": [
    "def debugCabochaToken(token: CaboCha.Token) -> None:\n",
    "    print(f\"surface: {token.surface}\")\n",
    "    print(f\"pos: {token.normalized_surface}\")\n",
    "\n",
    "debugCabochaToken(trees[1].tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cabochaTokenFeatureToMorph(token: CaboCha.Token) -> Morph:\n",
    "    features = token.feature.split(',')\n",
    "    try:\n",
    "        return Morph(token.surface, features[6], features[0], features[1])\n",
    "    except (IndexError, UnicodeEncodeError) as e:        \n",
    "        print(f\"token.feature: {token.feature}\")\n",
    "        return Morph(token.surface, token.surface, '*', '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ChunkObject at 0x1323206d0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cabochaChunkToChunkObject(chunk: CaboCha.Chunk, tokens: List[CaboCha.Token]) -> ChunkObject:\n",
    "    tokens: List[CaboCha.Token] = tokens[chunk.token_pos:chunk.token_pos + chunk.token_size]\n",
    "    morphs = list(map(cabochaTokenFeatureToMorph, tokens))\n",
    "    return ChunkObject(morphs, chunk.link)\n",
    "\n",
    "cabochaChunkToChunkObject(trees[0].chunks[0], trees[0].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 97, in json_packer\n",
      "    return json.dumps(\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode character '\\udc80' in position 138: surrogates not allowed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/ipykernel/iostream.py\", line 126, in _handle_event\n",
      "    event_f()\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/ipykernel/iostream.py\", line 497, in _flush\n",
      "    self.session.send(self.pub_thread, 'stream', content=content,\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 842, in send\n",
      "    to_send = self.serialize(msg, ident)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 716, in serialize\n",
      "    content = self.pack(content)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 105, in json_packer\n",
      "    packed = json.dumps(\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode character '\\udc80' in position 138: surrogates not allowed\n",
      "ERROR:tornado.general:Uncaught exception in zmqstream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 97, in json_packer\n",
      "    return json.dumps(\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode character '\\udc80' in position 138: surrogates not allowed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/ipykernel/iostream.py\", line 126, in _handle_event\n",
      "    event_f()\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/ipykernel/iostream.py\", line 497, in _flush\n",
      "    self.session.send(self.pub_thread, 'stream', content=content,\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 842, in send\n",
      "    to_send = self.serialize(msg, ident)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 716, in serialize\n",
      "    content = self.pack(content)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 105, in json_packer\n",
      "    packed = json.dumps(\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode character '\\udc80' in position 138: surrogates not allowed\n",
      "ERROR:tornado.application:Exception in callback functools.partial(<function ZMQStream._update_handler.<locals>.<lambda> at 0x13260cee0>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 97, in json_packer\n",
      "    return json.dumps(\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode character '\\udc80' in position 138: surrogates not allowed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n",
      "    self.io_loop.add_callback(lambda: self._handle_events(self.socket, 0))\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/ipykernel/iostream.py\", line 126, in _handle_event\n",
      "    event_f()\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/ipykernel/iostream.py\", line 497, in _flush\n",
      "    self.session.send(self.pub_thread, 'stream', content=content,\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 842, in send\n",
      "    to_send = self.serialize(msg, ident)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 716, in serialize\n",
      "    content = self.pack(content)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 105, in json_packer\n",
      "    packed = json.dumps(\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode character '\\udc80' in position 138: surrogates not allowed\n",
      "ERROR:tornado.application:Exception in callback functools.partial(<bound method OutStream._flush of <ipykernel.iostream.OutStream object at 0x10e9ccc40>>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 97, in json_packer\n",
      "    return json.dumps(\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode character '\\udca9' in position 157: surrogates not allowed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/ipykernel/iostream.py\", line 497, in _flush\n",
      "    self.session.send(self.pub_thread, 'stream', content=content,\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 842, in send\n",
      "    to_send = self.serialize(msg, ident)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 716, in serialize\n",
      "    content = self.pack(content)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 105, in json_packer\n",
      "    packed = json.dumps(\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode character '\\udca9' in position 157: surrogates not allowed\n",
      "ERROR:tornado.application:Exception in callback functools.partial(<bound method OutStream._flush of <ipykernel.iostream.OutStream object at 0x10e9ccc40>>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 97, in json_packer\n",
      "    return json.dumps(\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode characters in position 82-83: surrogates not allowed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/ipykernel/iostream.py\", line 497, in _flush\n",
      "    self.session.send(self.pub_thread, 'stream', content=content,\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 842, in send\n",
      "    to_send = self.serialize(msg, ident)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 716, in serialize\n",
      "    content = self.pack(content)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 105, in json_packer\n",
      "    packed = json.dumps(\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode characters in position 82-83: surrogates not allowed\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 97, in json_packer\n",
      "    return json.dumps(\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode characters in position 76-77: surrogates not allowed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/ipykernel/iostream.py\", line 126, in _handle_event\n",
      "    event_f()\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/ipykernel/iostream.py\", line 497, in _flush\n",
      "    self.session.send(self.pub_thread, 'stream', content=content,\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 842, in send\n",
      "    to_send = self.serialize(msg, ident)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 716, in serialize\n",
      "    content = self.pack(content)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 105, in json_packer\n",
      "    packed = json.dumps(\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode characters in position 76-77: surrogates not allowed\n",
      "ERROR:tornado.general:Uncaught exception in zmqstream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 97, in json_packer\n",
      "    return json.dumps(\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode characters in position 76-77: surrogates not allowed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/ipykernel/iostream.py\", line 126, in _handle_event\n",
      "    event_f()\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/ipykernel/iostream.py\", line 497, in _flush\n",
      "    self.session.send(self.pub_thread, 'stream', content=content,\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 842, in send\n",
      "    to_send = self.serialize(msg, ident)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 716, in serialize\n",
      "    content = self.pack(content)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 105, in json_packer\n",
      "    packed = json.dumps(\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode characters in position 76-77: surrogates not allowed\n",
      "ERROR:tornado.application:Exception in callback functools.partial(<function ZMQStream._update_handler.<locals>.<lambda> at 0x13250cc10>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 97, in json_packer\n",
      "    return json.dumps(\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode characters in position 76-77: surrogates not allowed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n",
      "    self.io_loop.add_callback(lambda: self._handle_events(self.socket, 0))\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/ipykernel/iostream.py\", line 126, in _handle_event\n",
      "    event_f()\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/ipykernel/iostream.py\", line 497, in _flush\n",
      "    self.session.send(self.pub_thread, 'stream', content=content,\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 842, in send\n",
      "    to_send = self.serialize(msg, ident)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 716, in serialize\n",
      "    content = self.pack(content)\n",
      "  File \"/Users/hiroga/.ghq/github.com/xhiroga/til/computer-science/nlp/nlp100/.venv/lib/python3.9/site-packages/jupyter_client/session.py\", line 105, in json_packer\n",
      "    packed = json.dumps(\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode characters in position 76-77: surrogates not allowed\n"
     ]
    }
   ],
   "source": [
    "chunksArray = [ \n",
    "    [cabochaChunkToChunkObject(chunk, tree.tokens) for chunk in tree.chunks] for tree in trees\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedSrcsToChunk(index: int, chunk: ChunkObject, chunks: List[ChunkObject]) -> None:\n",
    "    srcs = [i for i, chunk in enumerate(chunks) if chunk.dst == index]\n",
    "    chunk.srcs = srcs\n",
    "\n",
    "\n",
    "for chunks in chunksArray:\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        embedSrcsToChunk(i, chunk, chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'対談で'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 42. 係り元と係り先の文節の表示\n",
    "\n",
    "def getMorphsConcatinated(chunk: ChunkObject) -> None:\n",
    "    return ''.join([morph.surface for morph in chunk.morphs])\n",
    "\n",
    "getMorphsConcatinated(chunksArray[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "対談で\t\n"
     ]
    }
   ],
   "source": [
    "tab = \"\\t\"\n",
    "\n",
    "def printChunkWithSrcs(chunk: ChunkObject, chunks: List[ChunkObject]) -> None:\n",
    "    print(f\"{getMorphsConcatinated(chunk)}{tab}{tab.join([getMorphsConcatinated(chunks[src]) for src in chunk.srcs])}\")\n",
    "\n",
    "printChunkWithSrcs(chunksArray[0][0], chunksArray[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksArrayStr = [(printChunkWithSrcs(chunk, chunks)\n",
    "                   for chunk in chunks)for chunks in chunksArray]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9829b077c52378d5e1bc55420cf8cc5a4f9f459501a552e8de61cb75f7311e5d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
