{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Process](https://huggingface.co/docs/datasets/process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Format transform](https://huggingface.co/docs/datasets/process#format-transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hiroga\\miniconda3\\envs\\huggingface-datasets-vision-v2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 2 examples [00:00, 102.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"data/animal\") # ちなみに、`drop_label` の値にかかわらず、`metadata.csv`がある場合は未記載のファイルがあるとエラーになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=652x515>,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=652x515>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=660x660>],\n",
       " 'label': [0, 0]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# 辞書の値がリストであることに注意。\n",
    "# dataset[\"train\"][0] で呼び出した場合は、len(batch[\"image\"]) == 1 となる。\n",
    "# dataset[\"train\"][0:] で呼び出した場合は、len(batch[\"image\"]) は元配列の長さとなりそう。 \n",
    "def transform(batch: dict[str, list[any]]) -> dict[str, list[any]]:\n",
    "    cropped = [transforms.functional.center_crop(image, 100) for image in batch[\"image\"]]\n",
    "    return {\"image\": cropped, \"label\": batch[\"label\"]}\n",
    "\n",
    "dataset.set_transform(transform, [\"image\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': [<PIL.Image.Image image mode=RGB size=100x100>,\n",
       "  <PIL.Image.Image image mode=RGB size=100x100>],\n",
       " 'label': [0, 0]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Format](https://huggingface.co/docs/datasets/process#format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"data/animal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data={'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=652x515 at 0x2D2DFE892B0>, 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "for data in dataset[\"train\"]:\n",
    "    print(f\"{data=}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実験: PIL.Imageを含むデータセットをDataloaderで処理する場合、PyTorch,HuggingFaceともにTensorへの変換が必要なことを確認。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m torch_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(torch_dataset)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata\u001b[49m\u001b[38;5;132;43;01m=}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\hiroga\\miniconda3\\envs\\huggingface-datasets-vision-v2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\hiroga\\miniconda3\\envs\\huggingface-datasets-vision-v2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\hiroga\\miniconda3\\envs\\huggingface-datasets-vision-v2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hiroga\\miniconda3\\envs\\huggingface-datasets-vision-v2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hiroga\\miniconda3\\envs\\huggingface-datasets-vision-v2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hiroga\\miniconda3\\envs\\huggingface-datasets-vision-v2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:152\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    150\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# PyTorchの方のDatasetは、PIL.Imageを内部でTensorに変換してくれる...と思っていたが全然勘違いで、普通に transform で変換する必要がある\n",
    "torch_dataset = ImageFolder(\"data/animal\")\n",
    "torch_dataloader = DataLoader(torch_dataset)\n",
    "\n",
    "# TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n",
    "for data in torch_dataloader:\n",
    "    print(f\"{data=}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.JpegImagePlugin.JpegImageFile'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hiroga\\miniconda3\\envs\\huggingface-datasets-vision-v2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:129\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hiroga\\miniconda3\\envs\\huggingface-datasets-vision-v2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:152\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.JpegImagePlugin.JpegImageFile'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m      3\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata\u001b[49m\u001b[38;5;132;43;01m=}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\hiroga\\miniconda3\\envs\\huggingface-datasets-vision-v2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\hiroga\\miniconda3\\envs\\huggingface-datasets-vision-v2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\hiroga\\miniconda3\\envs\\huggingface-datasets-vision-v2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hiroga\\miniconda3\\envs\\huggingface-datasets-vision-v2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hiroga\\miniconda3\\envs\\huggingface-datasets-vision-v2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:132\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(elem, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_fields\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# namedtuple\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\u001b[38;5;241m*\u001b[39m(collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)))\n",
      "File \u001b[1;32mc:\\Users\\hiroga\\miniconda3\\envs\\huggingface-datasets-vision-v2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:152\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    150\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.JpegImagePlugin.JpegImageFile'>"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset[\"train\"])\n",
    "\n",
    "# TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
    "for data in dataloader:\n",
    "    print(f\"{data=}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実験: PyTorch,HuggingFaceのデータセットと、HuggingFaceのデータセットにtorchのフォーマットを設定した場合の3種類で、DataLoaderから読みだした値の型を確認。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset= Dataset ImageFolder\n",
      "    Number of datapoints: 2\n",
      "    Root location: data/animal\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(16, 16), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "           ),\n",
      "dataset[0]= (tensor([[[0.1020, 0.0863, 0.0980, 0.1686, 0.2039, 0.3059, 0.4353, 0.4275,\n",
      "          0.4196, 0.4706, 0.5412, 0.3725, 0.3961, 0.5098, 0.5529, 0.5373],\n",
      "         [0.1412, 0.1373, 0.1333, 0.1922, 0.2784, 0.4314, 0.4471, 0.4314,\n",
      "          0.4706, 0.5333, 0.5686, 0.4980, 0.7608, 0.7686, 0.6078, 0.4745],\n",
      "         [0.2157, 0.2118, 0.2314, 0.2784, 0.3647, 0.4000, 0.4275, 0.4824,\n",
      "          0.5176, 0.5490, 0.4980, 0.6235, 0.7961, 0.7961, 0.7255, 0.4353],\n",
      "         [0.2471, 0.2902, 0.4000, 0.4784, 0.5098, 0.4784, 0.4980, 0.5333,\n",
      "          0.5216, 0.4824, 0.5020, 0.7882, 0.8235, 0.6549, 0.5216, 0.3804],\n",
      "         [0.3686, 0.5412, 0.5569, 0.5020, 0.4745, 0.4706, 0.4510, 0.5216,\n",
      "          0.5059, 0.6039, 0.8118, 0.7804, 0.7176, 0.5294, 0.3333, 0.3137],\n",
      "         [0.3804, 0.7333, 0.8000, 0.6471, 0.6235, 0.7647, 0.8549, 0.9059,\n",
      "          0.9137, 0.9451, 0.9686, 0.8941, 0.5451, 0.4196, 0.3098, 0.3176],\n",
      "         [0.2353, 0.5569, 0.9137, 0.9373, 0.9451, 0.9804, 0.9882, 0.9882,\n",
      "          0.9882, 0.9882, 0.9843, 0.9373, 0.7294, 0.4510, 0.2863, 0.2745],\n",
      "         [0.2627, 0.3765, 0.7294, 0.8824, 0.9608, 0.9843, 0.9843, 0.9882,\n",
      "          0.9882, 0.9882, 0.9882, 0.9451, 0.8039, 0.4706, 0.2824, 0.2706],\n",
      "         [0.2863, 0.2863, 0.4745, 0.8431, 0.9490, 0.9412, 0.9647, 0.9843,\n",
      "          0.9843, 0.9882, 0.9843, 0.9176, 0.7490, 0.4431, 0.3176, 0.3216],\n",
      "         [0.3255, 0.2980, 0.4078, 0.7804, 0.8667, 0.7216, 0.8039, 0.9137,\n",
      "          0.9686, 0.9765, 0.9725, 0.8118, 0.5647, 0.3490, 0.3255, 0.3216],\n",
      "         [0.2745, 0.2627, 0.4118, 0.7373, 0.6510, 0.4471, 0.5020, 0.5843,\n",
      "          0.7686, 0.9137, 0.9333, 0.6196, 0.4196, 0.2941, 0.3176, 0.3216],\n",
      "         [0.2392, 0.2510, 0.4980, 0.5412, 0.3569, 0.3333, 0.3059, 0.2941,\n",
      "          0.3569, 0.6353, 0.8627, 0.4314, 0.3137, 0.3059, 0.3137, 0.2980],\n",
      "         [0.2078, 0.2706, 0.4784, 0.2784, 0.3529, 0.2471, 0.2392, 0.2549,\n",
      "          0.2745, 0.4588, 0.8039, 0.3765, 0.2863, 0.3098, 0.2667, 0.2549],\n",
      "         [0.2000, 0.3216, 0.3725, 0.2392, 0.4353, 0.2863, 0.2588, 0.2510,\n",
      "          0.2667, 0.3882, 0.7176, 0.3882, 0.3647, 0.3725, 0.3529, 0.3922],\n",
      "         [0.2745, 0.3961, 0.3922, 0.3137, 0.4157, 0.3804, 0.3451, 0.3882,\n",
      "          0.4000, 0.4431, 0.6353, 0.3882, 0.2863, 0.3451, 0.3922, 0.4431],\n",
      "         [0.2941, 0.3569, 0.3843, 0.3765, 0.4000, 0.4275, 0.4706, 0.4941,\n",
      "          0.4627, 0.4824, 0.4588, 0.4275, 0.4078, 0.4196, 0.4157, 0.4078]],\n",
      "\n",
      "        [[0.0824, 0.0706, 0.0784, 0.1176, 0.1294, 0.1961, 0.2863, 0.2863,\n",
      "          0.2941, 0.3333, 0.3843, 0.2588, 0.2902, 0.3765, 0.3569, 0.3569],\n",
      "         [0.1333, 0.1137, 0.1098, 0.1333, 0.1804, 0.2824, 0.2824, 0.2824,\n",
      "          0.3255, 0.3843, 0.4078, 0.3725, 0.5961, 0.6392, 0.4667, 0.3373],\n",
      "         [0.2157, 0.1961, 0.2000, 0.2118, 0.2588, 0.2667, 0.2980, 0.3608,\n",
      "          0.3765, 0.4118, 0.3765, 0.4863, 0.6078, 0.6588, 0.6000, 0.3647],\n",
      "         [0.2353, 0.2588, 0.3373, 0.3922, 0.4196, 0.3725, 0.4000, 0.4510,\n",
      "          0.4314, 0.3882, 0.4118, 0.6275, 0.6392, 0.5255, 0.3961, 0.3373],\n",
      "         [0.3255, 0.4902, 0.4784, 0.4118, 0.4000, 0.4000, 0.3961, 0.4706,\n",
      "          0.4549, 0.5294, 0.6745, 0.6314, 0.5765, 0.4627, 0.3098, 0.2902],\n",
      "         [0.3961, 0.6941, 0.7294, 0.5725, 0.5333, 0.6745, 0.7765, 0.8353,\n",
      "          0.8118, 0.7961, 0.8235, 0.7686, 0.4510, 0.3882, 0.3255, 0.3294],\n",
      "         [0.3176, 0.5569, 0.8314, 0.7922, 0.8000, 0.9059, 0.9529, 0.9490,\n",
      "          0.9333, 0.8902, 0.8980, 0.8196, 0.5922, 0.4196, 0.3412, 0.3333],\n",
      "         [0.3569, 0.4353, 0.6863, 0.7725, 0.8392, 0.8941, 0.9373, 0.9451,\n",
      "          0.9294, 0.9333, 0.9373, 0.8431, 0.6588, 0.4471, 0.3490, 0.3451],\n",
      "         [0.3765, 0.3686, 0.5020, 0.7686, 0.8392, 0.8235, 0.8784, 0.9294,\n",
      "          0.9216, 0.9294, 0.9294, 0.8118, 0.6275, 0.4510, 0.3922, 0.3922],\n",
      "         [0.4000, 0.3804, 0.4588, 0.7137, 0.7647, 0.6314, 0.7098, 0.8118,\n",
      "          0.8667, 0.8824, 0.8902, 0.7098, 0.5176, 0.4078, 0.3961, 0.4000],\n",
      "         [0.3725, 0.3647, 0.4627, 0.6706, 0.5804, 0.3882, 0.4824, 0.5765,\n",
      "          0.7216, 0.8235, 0.8353, 0.5529, 0.4431, 0.3686, 0.3882, 0.3882],\n",
      "         [0.3412, 0.3529, 0.5137, 0.5255, 0.3529, 0.3333, 0.3725, 0.3725,\n",
      "          0.4078, 0.6039, 0.7647, 0.4157, 0.3725, 0.3686, 0.3882, 0.3725],\n",
      "         [0.3059, 0.3490, 0.4941, 0.3529, 0.3608, 0.2980, 0.3176, 0.3412,\n",
      "          0.3569, 0.4784, 0.7098, 0.3882, 0.3608, 0.3765, 0.3490, 0.3490],\n",
      "         [0.2980, 0.3765, 0.4078, 0.3137, 0.4314, 0.3451, 0.3333, 0.3294,\n",
      "          0.3412, 0.4196, 0.6431, 0.4000, 0.4078, 0.4196, 0.4078, 0.4392],\n",
      "         [0.3412, 0.4196, 0.4118, 0.3686, 0.4314, 0.4078, 0.3765, 0.4157,\n",
      "          0.4196, 0.4510, 0.5765, 0.3765, 0.3294, 0.3804, 0.4275, 0.4745],\n",
      "         [0.3529, 0.3961, 0.4157, 0.4039, 0.4078, 0.4353, 0.4784, 0.4941,\n",
      "          0.4627, 0.4706, 0.4549, 0.4314, 0.4275, 0.4353, 0.4353, 0.4235]],\n",
      "\n",
      "        [[0.0471, 0.0353, 0.0392, 0.0549, 0.0588, 0.0863, 0.1333, 0.1216,\n",
      "          0.1294, 0.1529, 0.2039, 0.1216, 0.1647, 0.2275, 0.1843, 0.1882],\n",
      "         [0.0863, 0.0627, 0.0510, 0.0588, 0.0863, 0.1373, 0.1294, 0.1255,\n",
      "          0.1490, 0.1922, 0.2392, 0.2392, 0.4353, 0.4941, 0.3216, 0.2000],\n",
      "         [0.1686, 0.1451, 0.1294, 0.1176, 0.1529, 0.1490, 0.1686, 0.2235,\n",
      "          0.2235, 0.2510, 0.2392, 0.3451, 0.4353, 0.5098, 0.4667, 0.2824],\n",
      "         [0.1922, 0.2000, 0.2471, 0.2784, 0.2980, 0.2706, 0.2902, 0.3451,\n",
      "          0.3137, 0.2706, 0.3020, 0.4627, 0.4510, 0.3961, 0.3176, 0.2863],\n",
      "         [0.2510, 0.4078, 0.3765, 0.3020, 0.2980, 0.3216, 0.3373, 0.3922,\n",
      "          0.3804, 0.4353, 0.5176, 0.4706, 0.4235, 0.3686, 0.2627, 0.2392],\n",
      "         [0.3529, 0.6235, 0.6235, 0.4706, 0.4314, 0.5333, 0.6118, 0.6588,\n",
      "          0.6235, 0.5882, 0.6000, 0.5843, 0.3333, 0.3216, 0.3059, 0.3098],\n",
      "         [0.3294, 0.5059, 0.6941, 0.6000, 0.6039, 0.6902, 0.7490, 0.7373,\n",
      "          0.7216, 0.6627, 0.6902, 0.6314, 0.4353, 0.3490, 0.3294, 0.3216],\n",
      "         [0.3725, 0.4235, 0.5922, 0.6235, 0.6510, 0.6824, 0.7255, 0.7412,\n",
      "          0.7176, 0.7294, 0.7647, 0.6706, 0.4824, 0.3882, 0.3647, 0.3608],\n",
      "         [0.3882, 0.3647, 0.4667, 0.6549, 0.6667, 0.6275, 0.6824, 0.7333,\n",
      "          0.7216, 0.7373, 0.7647, 0.6510, 0.4784, 0.4078, 0.4000, 0.4000],\n",
      "         [0.3922, 0.3725, 0.4471, 0.6196, 0.6196, 0.4902, 0.5490, 0.6314,\n",
      "          0.6706, 0.6902, 0.7098, 0.5529, 0.4235, 0.4000, 0.3922, 0.3961],\n",
      "         [0.3843, 0.3765, 0.4471, 0.5804, 0.4706, 0.2824, 0.4000, 0.5020,\n",
      "          0.5961, 0.6549, 0.6667, 0.4431, 0.4000, 0.3608, 0.3843, 0.3725],\n",
      "         [0.3647, 0.3647, 0.4784, 0.4627, 0.2902, 0.2667, 0.3686, 0.3765,\n",
      "          0.3922, 0.5098, 0.6039, 0.3490, 0.3647, 0.3608, 0.3843, 0.3686],\n",
      "         [0.3255, 0.3569, 0.4588, 0.3529, 0.3137, 0.2824, 0.3294, 0.3569,\n",
      "          0.3686, 0.4392, 0.5686, 0.3529, 0.3647, 0.3647, 0.3529, 0.3686],\n",
      "         [0.3137, 0.3725, 0.3882, 0.3059, 0.3686, 0.3333, 0.3294, 0.3294,\n",
      "          0.3333, 0.3804, 0.5255, 0.3608, 0.3725, 0.3804, 0.3843, 0.4078],\n",
      "         [0.3373, 0.3843, 0.3647, 0.3333, 0.3647, 0.3529, 0.3176, 0.3569,\n",
      "          0.3608, 0.3804, 0.4784, 0.3098, 0.2980, 0.3412, 0.3725, 0.4157],\n",
      "         [0.3216, 0.3412, 0.3569, 0.3333, 0.3216, 0.3569, 0.3843, 0.3922,\n",
      "          0.3765, 0.3882, 0.3843, 0.3647, 0.3647, 0.3608, 0.3608, 0.3490]]]), 0)\n",
      "data=[tensor([[[[0.1020, 0.0863, 0.0980,  ..., 0.5098, 0.5529, 0.5373],\n",
      "          [0.1412, 0.1373, 0.1333,  ..., 0.7686, 0.6078, 0.4745],\n",
      "          [0.2157, 0.2118, 0.2314,  ..., 0.7961, 0.7255, 0.4353],\n",
      "          ...,\n",
      "          [0.2000, 0.3216, 0.3725,  ..., 0.3725, 0.3529, 0.3922],\n",
      "          [0.2745, 0.3961, 0.3922,  ..., 0.3451, 0.3922, 0.4431],\n",
      "          [0.2941, 0.3569, 0.3843,  ..., 0.4196, 0.4157, 0.4078]],\n",
      "\n",
      "         [[0.0824, 0.0706, 0.0784,  ..., 0.3765, 0.3569, 0.3569],\n",
      "          [0.1333, 0.1137, 0.1098,  ..., 0.6392, 0.4667, 0.3373],\n",
      "          [0.2157, 0.1961, 0.2000,  ..., 0.6588, 0.6000, 0.3647],\n",
      "          ...,\n",
      "          [0.2980, 0.3765, 0.4078,  ..., 0.4196, 0.4078, 0.4392],\n",
      "          [0.3412, 0.4196, 0.4118,  ..., 0.3804, 0.4275, 0.4745],\n",
      "          [0.3529, 0.3961, 0.4157,  ..., 0.4353, 0.4353, 0.4235]],\n",
      "\n",
      "         [[0.0471, 0.0353, 0.0392,  ..., 0.2275, 0.1843, 0.1882],\n",
      "          [0.0863, 0.0627, 0.0510,  ..., 0.4941, 0.3216, 0.2000],\n",
      "          [0.1686, 0.1451, 0.1294,  ..., 0.5098, 0.4667, 0.2824],\n",
      "          ...,\n",
      "          [0.3137, 0.3725, 0.3882,  ..., 0.3804, 0.3843, 0.4078],\n",
      "          [0.3373, 0.3843, 0.3647,  ..., 0.3412, 0.3725, 0.4157],\n",
      "          [0.3216, 0.3412, 0.3569,  ..., 0.3608, 0.3608, 0.3490]]],\n",
      "\n",
      "\n",
      "        [[[0.2627, 0.2275, 0.2196,  ..., 0.3922, 0.3882, 0.3451],\n",
      "          [0.2314, 0.2235, 0.2275,  ..., 0.2667, 0.2706, 0.2627],\n",
      "          [0.3608, 0.3647, 0.3608,  ..., 0.5216, 0.5176, 0.4902],\n",
      "          ...,\n",
      "          [0.8000, 0.8314, 0.8588,  ..., 0.5333, 0.5490, 0.5176],\n",
      "          [0.8235, 0.8627, 0.9137,  ..., 0.5176, 0.5137, 0.5725],\n",
      "          [0.8157, 0.8941, 0.9216,  ..., 0.5843, 0.5569, 0.6353]],\n",
      "\n",
      "         [[0.2745, 0.2392, 0.2275,  ..., 0.4000, 0.3922, 0.3490],\n",
      "          [0.2392, 0.2314, 0.2353,  ..., 0.2745, 0.2784, 0.2667],\n",
      "          [0.3725, 0.3804, 0.3725,  ..., 0.5216, 0.5176, 0.4863],\n",
      "          ...,\n",
      "          [0.7725, 0.8078, 0.8157,  ..., 0.4941, 0.5059, 0.4745],\n",
      "          [0.8000, 0.8431, 0.8980,  ..., 0.4745, 0.4745, 0.5294],\n",
      "          [0.7843, 0.8784, 0.9020,  ..., 0.5373, 0.5059, 0.5843]],\n",
      "\n",
      "         [[0.2118, 0.1961, 0.1922,  ..., 0.3843, 0.3804, 0.3333],\n",
      "          [0.1961, 0.1922, 0.1882,  ..., 0.2471, 0.2510, 0.2471],\n",
      "          [0.2980, 0.3020, 0.2980,  ..., 0.4902, 0.4902, 0.4667],\n",
      "          ...,\n",
      "          [0.7333, 0.7725, 0.7608,  ..., 0.4353, 0.4431, 0.4196],\n",
      "          [0.7725, 0.8235, 0.8902,  ..., 0.4196, 0.4196, 0.4667],\n",
      "          [0.7569, 0.8667, 0.9020,  ..., 0.4824, 0.4549, 0.5255]]]]), tensor([0, 0])]\n",
      "images=tensor([[[[0.1020, 0.0863, 0.0980,  ..., 0.5098, 0.5529, 0.5373],\n",
      "          [0.1412, 0.1373, 0.1333,  ..., 0.7686, 0.6078, 0.4745],\n",
      "          [0.2157, 0.2118, 0.2314,  ..., 0.7961, 0.7255, 0.4353],\n",
      "          ...,\n",
      "          [0.2000, 0.3216, 0.3725,  ..., 0.3725, 0.3529, 0.3922],\n",
      "          [0.2745, 0.3961, 0.3922,  ..., 0.3451, 0.3922, 0.4431],\n",
      "          [0.2941, 0.3569, 0.3843,  ..., 0.4196, 0.4157, 0.4078]],\n",
      "\n",
      "         [[0.0824, 0.0706, 0.0784,  ..., 0.3765, 0.3569, 0.3569],\n",
      "          [0.1333, 0.1137, 0.1098,  ..., 0.6392, 0.4667, 0.3373],\n",
      "          [0.2157, 0.1961, 0.2000,  ..., 0.6588, 0.6000, 0.3647],\n",
      "          ...,\n",
      "          [0.2980, 0.3765, 0.4078,  ..., 0.4196, 0.4078, 0.4392],\n",
      "          [0.3412, 0.4196, 0.4118,  ..., 0.3804, 0.4275, 0.4745],\n",
      "          [0.3529, 0.3961, 0.4157,  ..., 0.4353, 0.4353, 0.4235]],\n",
      "\n",
      "         [[0.0471, 0.0353, 0.0392,  ..., 0.2275, 0.1843, 0.1882],\n",
      "          [0.0863, 0.0627, 0.0510,  ..., 0.4941, 0.3216, 0.2000],\n",
      "          [0.1686, 0.1451, 0.1294,  ..., 0.5098, 0.4667, 0.2824],\n",
      "          ...,\n",
      "          [0.3137, 0.3725, 0.3882,  ..., 0.3804, 0.3843, 0.4078],\n",
      "          [0.3373, 0.3843, 0.3647,  ..., 0.3412, 0.3725, 0.4157],\n",
      "          [0.3216, 0.3412, 0.3569,  ..., 0.3608, 0.3608, 0.3490]]],\n",
      "\n",
      "\n",
      "        [[[0.2627, 0.2275, 0.2196,  ..., 0.3922, 0.3882, 0.3451],\n",
      "          [0.2314, 0.2235, 0.2275,  ..., 0.2667, 0.2706, 0.2627],\n",
      "          [0.3608, 0.3647, 0.3608,  ..., 0.5216, 0.5176, 0.4902],\n",
      "          ...,\n",
      "          [0.8000, 0.8314, 0.8588,  ..., 0.5333, 0.5490, 0.5176],\n",
      "          [0.8235, 0.8627, 0.9137,  ..., 0.5176, 0.5137, 0.5725],\n",
      "          [0.8157, 0.8941, 0.9216,  ..., 0.5843, 0.5569, 0.6353]],\n",
      "\n",
      "         [[0.2745, 0.2392, 0.2275,  ..., 0.4000, 0.3922, 0.3490],\n",
      "          [0.2392, 0.2314, 0.2353,  ..., 0.2745, 0.2784, 0.2667],\n",
      "          [0.3725, 0.3804, 0.3725,  ..., 0.5216, 0.5176, 0.4863],\n",
      "          ...,\n",
      "          [0.7725, 0.8078, 0.8157,  ..., 0.4941, 0.5059, 0.4745],\n",
      "          [0.8000, 0.8431, 0.8980,  ..., 0.4745, 0.4745, 0.5294],\n",
      "          [0.7843, 0.8784, 0.9020,  ..., 0.5373, 0.5059, 0.5843]],\n",
      "\n",
      "         [[0.2118, 0.1961, 0.1922,  ..., 0.3843, 0.3804, 0.3333],\n",
      "          [0.1961, 0.1922, 0.1882,  ..., 0.2471, 0.2510, 0.2471],\n",
      "          [0.2980, 0.3020, 0.2980,  ..., 0.4902, 0.4902, 0.4667],\n",
      "          ...,\n",
      "          [0.7333, 0.7725, 0.7608,  ..., 0.4353, 0.4431, 0.4196],\n",
      "          [0.7725, 0.8235, 0.8902,  ..., 0.4196, 0.4196, 0.4667],\n",
      "          [0.7569, 0.8667, 0.9020,  ..., 0.4824, 0.4549, 0.5255]]]]), labels=tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "# パターン1: PyTorchのDatasetを使う場合\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((16, 16)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "torch_dataset_with_transform = ImageFolder(\"data/animal\", transform=transform)\n",
    "\n",
    "# ポイント: PyTorchのImageFolderで読み込んだ場合、DatasetDictではなくDatasetである（=\"train\"でsplitを指定しない）\n",
    "# また、個別のアイテムは、(image, label) のようなタプルである\n",
    "print(f\"dataset= {torch_dataset_with_transform},\\ndataset[0]= {torch_dataset_with_transform[0]}\")\n",
    "\n",
    "torch_dataloader_with_transform = DataLoader(torch_dataset_with_transform, batch_size=2)\n",
    "\n",
    "for data in torch_dataloader_with_transform:\n",
    "    print(f\"{data=}\") # list[torch.Tensor[batch_size, value]] となる\n",
    "    break\n",
    "\n",
    "for images, labels in torch_dataloader_with_transform:\n",
    "    print(f\"{images=}, {labels=}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2 examples [00:00, 782.81 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before set_transform\n",
      "dataset= DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "}),\n",
      "dataset['train']= Dataset({\n",
      "    features: ['image', 'label'],\n",
      "    num_rows: 2\n",
      "}),\n",
      "dataset['train'][0]= {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=652x515 at 0x2D2E1BE2B10>, 'label': 0}\n",
      "After set_transform\n",
      "dataset= DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "}),\n",
      "dataset['train']= Dataset({\n",
      "    features: ['image', 'label'],\n",
      "    num_rows: 2\n",
      "}),\n",
      "dataset['train'][0]= {'image': tensor([[[0.1020, 0.0863, 0.0980, 0.1686, 0.2039, 0.3059, 0.4353, 0.4275,\n",
      "          0.4196, 0.4706, 0.5412, 0.3725, 0.3961, 0.5098, 0.5529, 0.5373],\n",
      "         [0.1412, 0.1373, 0.1333, 0.1922, 0.2784, 0.4314, 0.4471, 0.4314,\n",
      "          0.4706, 0.5333, 0.5686, 0.4980, 0.7608, 0.7686, 0.6078, 0.4745],\n",
      "         [0.2157, 0.2118, 0.2314, 0.2784, 0.3647, 0.4000, 0.4275, 0.4824,\n",
      "          0.5176, 0.5490, 0.4980, 0.6235, 0.7961, 0.7961, 0.7255, 0.4353],\n",
      "         [0.2471, 0.2902, 0.4000, 0.4784, 0.5098, 0.4784, 0.4980, 0.5333,\n",
      "          0.5216, 0.4824, 0.5020, 0.7882, 0.8235, 0.6549, 0.5216, 0.3804],\n",
      "         [0.3686, 0.5412, 0.5569, 0.5020, 0.4745, 0.4706, 0.4510, 0.5216,\n",
      "          0.5059, 0.6039, 0.8118, 0.7804, 0.7176, 0.5294, 0.3333, 0.3137],\n",
      "         [0.3804, 0.7333, 0.8000, 0.6471, 0.6235, 0.7647, 0.8549, 0.9059,\n",
      "          0.9137, 0.9451, 0.9686, 0.8941, 0.5451, 0.4196, 0.3098, 0.3176],\n",
      "         [0.2353, 0.5569, 0.9137, 0.9373, 0.9451, 0.9804, 0.9882, 0.9882,\n",
      "          0.9882, 0.9882, 0.9843, 0.9373, 0.7294, 0.4510, 0.2863, 0.2745],\n",
      "         [0.2627, 0.3765, 0.7294, 0.8824, 0.9608, 0.9843, 0.9843, 0.9882,\n",
      "          0.9882, 0.9882, 0.9882, 0.9451, 0.8039, 0.4706, 0.2824, 0.2706],\n",
      "         [0.2863, 0.2863, 0.4745, 0.8431, 0.9490, 0.9412, 0.9647, 0.9843,\n",
      "          0.9843, 0.9882, 0.9843, 0.9176, 0.7490, 0.4431, 0.3176, 0.3216],\n",
      "         [0.3255, 0.2980, 0.4078, 0.7804, 0.8667, 0.7216, 0.8039, 0.9137,\n",
      "          0.9686, 0.9765, 0.9725, 0.8118, 0.5647, 0.3490, 0.3255, 0.3216],\n",
      "         [0.2745, 0.2627, 0.4118, 0.7373, 0.6510, 0.4471, 0.5020, 0.5843,\n",
      "          0.7686, 0.9137, 0.9333, 0.6196, 0.4196, 0.2941, 0.3176, 0.3216],\n",
      "         [0.2392, 0.2510, 0.4980, 0.5412, 0.3569, 0.3333, 0.3059, 0.2941,\n",
      "          0.3569, 0.6353, 0.8627, 0.4314, 0.3137, 0.3059, 0.3137, 0.2980],\n",
      "         [0.2078, 0.2706, 0.4784, 0.2784, 0.3529, 0.2471, 0.2392, 0.2549,\n",
      "          0.2745, 0.4588, 0.8039, 0.3765, 0.2863, 0.3098, 0.2667, 0.2549],\n",
      "         [0.2000, 0.3216, 0.3725, 0.2392, 0.4353, 0.2863, 0.2588, 0.2510,\n",
      "          0.2667, 0.3882, 0.7176, 0.3882, 0.3647, 0.3725, 0.3529, 0.3922],\n",
      "         [0.2745, 0.3961, 0.3922, 0.3137, 0.4157, 0.3804, 0.3451, 0.3882,\n",
      "          0.4000, 0.4431, 0.6353, 0.3882, 0.2863, 0.3451, 0.3922, 0.4431],\n",
      "         [0.2941, 0.3569, 0.3843, 0.3765, 0.4000, 0.4275, 0.4706, 0.4941,\n",
      "          0.4627, 0.4824, 0.4588, 0.4275, 0.4078, 0.4196, 0.4157, 0.4078]],\n",
      "\n",
      "        [[0.0824, 0.0706, 0.0784, 0.1176, 0.1294, 0.1961, 0.2863, 0.2863,\n",
      "          0.2941, 0.3333, 0.3843, 0.2588, 0.2902, 0.3765, 0.3569, 0.3569],\n",
      "         [0.1333, 0.1137, 0.1098, 0.1333, 0.1804, 0.2824, 0.2824, 0.2824,\n",
      "          0.3255, 0.3843, 0.4078, 0.3725, 0.5961, 0.6392, 0.4667, 0.3373],\n",
      "         [0.2157, 0.1961, 0.2000, 0.2118, 0.2588, 0.2667, 0.2980, 0.3608,\n",
      "          0.3765, 0.4118, 0.3765, 0.4863, 0.6078, 0.6588, 0.6000, 0.3647],\n",
      "         [0.2353, 0.2588, 0.3373, 0.3922, 0.4196, 0.3725, 0.4000, 0.4510,\n",
      "          0.4314, 0.3882, 0.4118, 0.6275, 0.6392, 0.5255, 0.3961, 0.3373],\n",
      "         [0.3255, 0.4902, 0.4784, 0.4118, 0.4000, 0.4000, 0.3961, 0.4706,\n",
      "          0.4549, 0.5294, 0.6745, 0.6314, 0.5765, 0.4627, 0.3098, 0.2902],\n",
      "         [0.3961, 0.6941, 0.7294, 0.5725, 0.5333, 0.6745, 0.7765, 0.8353,\n",
      "          0.8118, 0.7961, 0.8235, 0.7686, 0.4510, 0.3882, 0.3255, 0.3294],\n",
      "         [0.3176, 0.5569, 0.8314, 0.7922, 0.8000, 0.9059, 0.9529, 0.9490,\n",
      "          0.9333, 0.8902, 0.8980, 0.8196, 0.5922, 0.4196, 0.3412, 0.3333],\n",
      "         [0.3569, 0.4353, 0.6863, 0.7725, 0.8392, 0.8941, 0.9373, 0.9451,\n",
      "          0.9294, 0.9333, 0.9373, 0.8431, 0.6588, 0.4471, 0.3490, 0.3451],\n",
      "         [0.3765, 0.3686, 0.5020, 0.7686, 0.8392, 0.8235, 0.8784, 0.9294,\n",
      "          0.9216, 0.9294, 0.9294, 0.8118, 0.6275, 0.4510, 0.3922, 0.3922],\n",
      "         [0.4000, 0.3804, 0.4588, 0.7137, 0.7647, 0.6314, 0.7098, 0.8118,\n",
      "          0.8667, 0.8824, 0.8902, 0.7098, 0.5176, 0.4078, 0.3961, 0.4000],\n",
      "         [0.3725, 0.3647, 0.4627, 0.6706, 0.5804, 0.3882, 0.4824, 0.5765,\n",
      "          0.7216, 0.8235, 0.8353, 0.5529, 0.4431, 0.3686, 0.3882, 0.3882],\n",
      "         [0.3412, 0.3529, 0.5137, 0.5255, 0.3529, 0.3333, 0.3725, 0.3725,\n",
      "          0.4078, 0.6039, 0.7647, 0.4157, 0.3725, 0.3686, 0.3882, 0.3725],\n",
      "         [0.3059, 0.3490, 0.4941, 0.3529, 0.3608, 0.2980, 0.3176, 0.3412,\n",
      "          0.3569, 0.4784, 0.7098, 0.3882, 0.3608, 0.3765, 0.3490, 0.3490],\n",
      "         [0.2980, 0.3765, 0.4078, 0.3137, 0.4314, 0.3451, 0.3333, 0.3294,\n",
      "          0.3412, 0.4196, 0.6431, 0.4000, 0.4078, 0.4196, 0.4078, 0.4392],\n",
      "         [0.3412, 0.4196, 0.4118, 0.3686, 0.4314, 0.4078, 0.3765, 0.4157,\n",
      "          0.4196, 0.4510, 0.5765, 0.3765, 0.3294, 0.3804, 0.4275, 0.4745],\n",
      "         [0.3529, 0.3961, 0.4157, 0.4039, 0.4078, 0.4353, 0.4784, 0.4941,\n",
      "          0.4627, 0.4706, 0.4549, 0.4314, 0.4275, 0.4353, 0.4353, 0.4235]],\n",
      "\n",
      "        [[0.0471, 0.0353, 0.0392, 0.0549, 0.0588, 0.0863, 0.1333, 0.1216,\n",
      "          0.1294, 0.1529, 0.2039, 0.1216, 0.1647, 0.2275, 0.1843, 0.1882],\n",
      "         [0.0863, 0.0627, 0.0510, 0.0588, 0.0863, 0.1373, 0.1294, 0.1255,\n",
      "          0.1490, 0.1922, 0.2392, 0.2392, 0.4353, 0.4941, 0.3216, 0.2000],\n",
      "         [0.1686, 0.1451, 0.1294, 0.1176, 0.1529, 0.1490, 0.1686, 0.2235,\n",
      "          0.2235, 0.2510, 0.2392, 0.3451, 0.4353, 0.5098, 0.4667, 0.2824],\n",
      "         [0.1922, 0.2000, 0.2471, 0.2784, 0.2980, 0.2706, 0.2902, 0.3451,\n",
      "          0.3137, 0.2706, 0.3020, 0.4627, 0.4510, 0.3961, 0.3176, 0.2863],\n",
      "         [0.2510, 0.4078, 0.3765, 0.3020, 0.2980, 0.3216, 0.3373, 0.3922,\n",
      "          0.3804, 0.4353, 0.5176, 0.4706, 0.4235, 0.3686, 0.2627, 0.2392],\n",
      "         [0.3529, 0.6235, 0.6235, 0.4706, 0.4314, 0.5333, 0.6118, 0.6588,\n",
      "          0.6235, 0.5882, 0.6000, 0.5843, 0.3333, 0.3216, 0.3059, 0.3098],\n",
      "         [0.3294, 0.5059, 0.6941, 0.6000, 0.6039, 0.6902, 0.7490, 0.7373,\n",
      "          0.7216, 0.6627, 0.6902, 0.6314, 0.4353, 0.3490, 0.3294, 0.3216],\n",
      "         [0.3725, 0.4235, 0.5922, 0.6235, 0.6510, 0.6824, 0.7255, 0.7412,\n",
      "          0.7176, 0.7294, 0.7647, 0.6706, 0.4824, 0.3882, 0.3647, 0.3608],\n",
      "         [0.3882, 0.3647, 0.4667, 0.6549, 0.6667, 0.6275, 0.6824, 0.7333,\n",
      "          0.7216, 0.7373, 0.7647, 0.6510, 0.4784, 0.4078, 0.4000, 0.4000],\n",
      "         [0.3922, 0.3725, 0.4471, 0.6196, 0.6196, 0.4902, 0.5490, 0.6314,\n",
      "          0.6706, 0.6902, 0.7098, 0.5529, 0.4235, 0.4000, 0.3922, 0.3961],\n",
      "         [0.3843, 0.3765, 0.4471, 0.5804, 0.4706, 0.2824, 0.4000, 0.5020,\n",
      "          0.5961, 0.6549, 0.6667, 0.4431, 0.4000, 0.3608, 0.3843, 0.3725],\n",
      "         [0.3647, 0.3647, 0.4784, 0.4627, 0.2902, 0.2667, 0.3686, 0.3765,\n",
      "          0.3922, 0.5098, 0.6039, 0.3490, 0.3647, 0.3608, 0.3843, 0.3686],\n",
      "         [0.3255, 0.3569, 0.4588, 0.3529, 0.3137, 0.2824, 0.3294, 0.3569,\n",
      "          0.3686, 0.4392, 0.5686, 0.3529, 0.3647, 0.3647, 0.3529, 0.3686],\n",
      "         [0.3137, 0.3725, 0.3882, 0.3059, 0.3686, 0.3333, 0.3294, 0.3294,\n",
      "          0.3333, 0.3804, 0.5255, 0.3608, 0.3725, 0.3804, 0.3843, 0.4078],\n",
      "         [0.3373, 0.3843, 0.3647, 0.3333, 0.3647, 0.3529, 0.3176, 0.3569,\n",
      "          0.3608, 0.3804, 0.4784, 0.3098, 0.2980, 0.3412, 0.3725, 0.4157],\n",
      "         [0.3216, 0.3412, 0.3569, 0.3333, 0.3216, 0.3569, 0.3843, 0.3922,\n",
      "          0.3765, 0.3882, 0.3843, 0.3647, 0.3647, 0.3608, 0.3608, 0.3490]]]), 'label': 0}\n",
      "data={'image': tensor([[[[0.1020, 0.0863, 0.0980,  ..., 0.5098, 0.5529, 0.5373],\n",
      "          [0.1412, 0.1373, 0.1333,  ..., 0.7686, 0.6078, 0.4745],\n",
      "          [0.2157, 0.2118, 0.2314,  ..., 0.7961, 0.7255, 0.4353],\n",
      "          ...,\n",
      "          [0.2000, 0.3216, 0.3725,  ..., 0.3725, 0.3529, 0.3922],\n",
      "          [0.2745, 0.3961, 0.3922,  ..., 0.3451, 0.3922, 0.4431],\n",
      "          [0.2941, 0.3569, 0.3843,  ..., 0.4196, 0.4157, 0.4078]],\n",
      "\n",
      "         [[0.0824, 0.0706, 0.0784,  ..., 0.3765, 0.3569, 0.3569],\n",
      "          [0.1333, 0.1137, 0.1098,  ..., 0.6392, 0.4667, 0.3373],\n",
      "          [0.2157, 0.1961, 0.2000,  ..., 0.6588, 0.6000, 0.3647],\n",
      "          ...,\n",
      "          [0.2980, 0.3765, 0.4078,  ..., 0.4196, 0.4078, 0.4392],\n",
      "          [0.3412, 0.4196, 0.4118,  ..., 0.3804, 0.4275, 0.4745],\n",
      "          [0.3529, 0.3961, 0.4157,  ..., 0.4353, 0.4353, 0.4235]],\n",
      "\n",
      "         [[0.0471, 0.0353, 0.0392,  ..., 0.2275, 0.1843, 0.1882],\n",
      "          [0.0863, 0.0627, 0.0510,  ..., 0.4941, 0.3216, 0.2000],\n",
      "          [0.1686, 0.1451, 0.1294,  ..., 0.5098, 0.4667, 0.2824],\n",
      "          ...,\n",
      "          [0.3137, 0.3725, 0.3882,  ..., 0.3804, 0.3843, 0.4078],\n",
      "          [0.3373, 0.3843, 0.3647,  ..., 0.3412, 0.3725, 0.4157],\n",
      "          [0.3216, 0.3412, 0.3569,  ..., 0.3608, 0.3608, 0.3490]]],\n",
      "\n",
      "\n",
      "        [[[0.2627, 0.2275, 0.2196,  ..., 0.3922, 0.3882, 0.3451],\n",
      "          [0.2314, 0.2235, 0.2275,  ..., 0.2667, 0.2706, 0.2627],\n",
      "          [0.3608, 0.3647, 0.3608,  ..., 0.5216, 0.5176, 0.4902],\n",
      "          ...,\n",
      "          [0.8000, 0.8314, 0.8588,  ..., 0.5333, 0.5490, 0.5176],\n",
      "          [0.8235, 0.8627, 0.9137,  ..., 0.5176, 0.5137, 0.5725],\n",
      "          [0.8157, 0.8941, 0.9216,  ..., 0.5843, 0.5569, 0.6353]],\n",
      "\n",
      "         [[0.2745, 0.2392, 0.2275,  ..., 0.4000, 0.3922, 0.3490],\n",
      "          [0.2392, 0.2314, 0.2353,  ..., 0.2745, 0.2784, 0.2667],\n",
      "          [0.3725, 0.3804, 0.3725,  ..., 0.5216, 0.5176, 0.4863],\n",
      "          ...,\n",
      "          [0.7725, 0.8078, 0.8157,  ..., 0.4941, 0.5059, 0.4745],\n",
      "          [0.8000, 0.8431, 0.8980,  ..., 0.4745, 0.4745, 0.5294],\n",
      "          [0.7843, 0.8784, 0.9020,  ..., 0.5373, 0.5059, 0.5843]],\n",
      "\n",
      "         [[0.2118, 0.1961, 0.1922,  ..., 0.3843, 0.3804, 0.3333],\n",
      "          [0.1961, 0.1922, 0.1882,  ..., 0.2471, 0.2510, 0.2471],\n",
      "          [0.2980, 0.3020, 0.2980,  ..., 0.4902, 0.4902, 0.4667],\n",
      "          ...,\n",
      "          [0.7333, 0.7725, 0.7608,  ..., 0.4353, 0.4431, 0.4196],\n",
      "          [0.7725, 0.8235, 0.8902,  ..., 0.4196, 0.4196, 0.4667],\n",
      "          [0.7569, 0.8667, 0.9020,  ..., 0.4824, 0.4549, 0.5255]]]]), 'label': tensor([0, 0])}\n",
      "images='image', labels='label'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# パターン2: HuggingFaceのDatasetをそのまま使う場合\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "huggingface_datasets_with_transform = load_dataset(\"imagefolder\", data_dir=\"data/animal\")\n",
    "\n",
    "# ポイント: HuggingFaceのDatasetsのImageFolderで読み込んだ場合、DatasetDictDatasetである。\n",
    "# また、個別のアイテムは、{image: any, label: any} のような辞書である\n",
    "print(\"Before set_transform\")\n",
    "print(f\"dataset= {huggingface_datasets_with_transform},\\ndataset['train']= {huggingface_datasets_with_transform['train']},\\ndataset['train'][0]= {huggingface_datasets_with_transform['train'][0]}\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((16, 16)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "def transform_to_set(batch: dict[str, list[any]]) -> dict[str, list[any]]:\n",
    "    transformed = [transform(image) for image in batch[\"image\"]]\n",
    "    return {\"image\": transformed, \"label\": batch[\"label\"]}\n",
    "\n",
    "huggingface_datasets_with_transform.set_transform(transform_to_set)\n",
    "print(\"After set_transform\")\n",
    "print(f\"dataset= {huggingface_datasets_with_transform},\\ndataset['train']= {huggingface_datasets_with_transform['train']},\\ndataset['train'][0]= {huggingface_datasets_with_transform['train'][0]}\")\n",
    "\n",
    "huggingface_dataloader_with_transform = DataLoader(huggingface_datasets_with_transform[\"train\"], batch_size=2)\n",
    "\n",
    "for data in huggingface_dataloader_with_transform:\n",
    "    print(f\"{data=}\")  # dict[str, any] となる\n",
    "    break\n",
    "\n",
    "for images, labels in huggingface_dataloader_with_transform:\n",
    "    print(f\"{images=}, {labels=}\")  # \"image\", \"label\" という文字列が入っている。\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset= Dataset({\n",
      "    features: ['image', 'label'],\n",
      "    num_rows: 2\n",
      "}),\n",
      "dataset[0]= {'image': tensor([[[  5,   5,   5],\n",
      "         [  4,   4,   4],\n",
      "         [  3,   3,   1],\n",
      "         ...,\n",
      "         [120,  80,  31],\n",
      "         [107,  65,  15],\n",
      "         [138,  96,  46]],\n",
      "\n",
      "        [[  5,   5,   3],\n",
      "         [  5,   5,   3],\n",
      "         [  6,   6,   4],\n",
      "         ...,\n",
      "         [113,  70,  19],\n",
      "         [116,  73,  22],\n",
      "         [147, 104,  53]],\n",
      "\n",
      "        [[  3,   3,   1],\n",
      "         [  4,   4,   2],\n",
      "         [  5,   6,   1],\n",
      "         ...,\n",
      "         [122,  75,  23],\n",
      "         [121,  75,  25],\n",
      "         [153, 107,  57]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 97, 110, 100],\n",
      "         [ 40,  54,  41],\n",
      "         [ 39,  53,  38],\n",
      "         ...,\n",
      "         [ 96, 107, 101],\n",
      "         [ 35,  45,  37],\n",
      "         [ 91, 101,  93]],\n",
      "\n",
      "        [[ 58,  71,  61],\n",
      "         [ 43,  57,  44],\n",
      "         [ 52,  66,  51],\n",
      "         ...,\n",
      "         [107, 118, 112],\n",
      "         [146, 156, 148],\n",
      "         [124, 134, 126]],\n",
      "\n",
      "        [[ 42,  55,  45],\n",
      "         [ 38,  52,  39],\n",
      "         [ 60,  74,  59],\n",
      "         ...,\n",
      "         [102, 113, 107],\n",
      "         [129, 139, 131],\n",
      "         [ 68,  78,  70]]], dtype=torch.uint8), 'label': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "# パターン3: HuggingFaceのDatasetをPyTorchのDatasetに変換する場合\n",
    "from datasets import load_dataset\n",
    "\n",
    "huggingface_datasets = load_dataset(\"imagefolder\", data_dir=\"data/animal\")\n",
    "huggingface_datasets_to_torch = huggingface_datasets[\"train\"].with_format(\"torch\")\n",
    "\n",
    "# ポイント: フォーマットをtorchにしようが、個別のアイテムはタプルではなく辞書である。\n",
    "print(f\"dataset= {huggingface_datasets_to_torch},\\ndataset[0]= {huggingface_datasets_to_torch[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実験結果も踏まえた、正しいDataloaderでの読み込み方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images=tensor([[[[0.1020, 0.0863, 0.0980,  ..., 0.5098, 0.5529, 0.5373],\n",
      "          [0.1412, 0.1373, 0.1333,  ..., 0.7686, 0.6078, 0.4745],\n",
      "          [0.2157, 0.2118, 0.2314,  ..., 0.7961, 0.7255, 0.4353],\n",
      "          ...,\n",
      "          [0.2000, 0.3216, 0.3725,  ..., 0.3725, 0.3529, 0.3922],\n",
      "          [0.2745, 0.3961, 0.3922,  ..., 0.3451, 0.3922, 0.4431],\n",
      "          [0.2941, 0.3569, 0.3843,  ..., 0.4196, 0.4157, 0.4078]],\n",
      "\n",
      "         [[0.0824, 0.0706, 0.0784,  ..., 0.3765, 0.3569, 0.3569],\n",
      "          [0.1333, 0.1137, 0.1098,  ..., 0.6392, 0.4667, 0.3373],\n",
      "          [0.2157, 0.1961, 0.2000,  ..., 0.6588, 0.6000, 0.3647],\n",
      "          ...,\n",
      "          [0.2980, 0.3765, 0.4078,  ..., 0.4196, 0.4078, 0.4392],\n",
      "          [0.3412, 0.4196, 0.4118,  ..., 0.3804, 0.4275, 0.4745],\n",
      "          [0.3529, 0.3961, 0.4157,  ..., 0.4353, 0.4353, 0.4235]],\n",
      "\n",
      "         [[0.0471, 0.0353, 0.0392,  ..., 0.2275, 0.1843, 0.1882],\n",
      "          [0.0863, 0.0627, 0.0510,  ..., 0.4941, 0.3216, 0.2000],\n",
      "          [0.1686, 0.1451, 0.1294,  ..., 0.5098, 0.4667, 0.2824],\n",
      "          ...,\n",
      "          [0.3137, 0.3725, 0.3882,  ..., 0.3804, 0.3843, 0.4078],\n",
      "          [0.3373, 0.3843, 0.3647,  ..., 0.3412, 0.3725, 0.4157],\n",
      "          [0.3216, 0.3412, 0.3569,  ..., 0.3608, 0.3608, 0.3490]]],\n",
      "\n",
      "\n",
      "        [[[0.2627, 0.2275, 0.2196,  ..., 0.3922, 0.3882, 0.3451],\n",
      "          [0.2314, 0.2235, 0.2275,  ..., 0.2667, 0.2706, 0.2627],\n",
      "          [0.3608, 0.3647, 0.3608,  ..., 0.5216, 0.5176, 0.4902],\n",
      "          ...,\n",
      "          [0.8000, 0.8314, 0.8588,  ..., 0.5333, 0.5490, 0.5176],\n",
      "          [0.8235, 0.8627, 0.9137,  ..., 0.5176, 0.5137, 0.5725],\n",
      "          [0.8157, 0.8941, 0.9216,  ..., 0.5843, 0.5569, 0.6353]],\n",
      "\n",
      "         [[0.2745, 0.2392, 0.2275,  ..., 0.4000, 0.3922, 0.3490],\n",
      "          [0.2392, 0.2314, 0.2353,  ..., 0.2745, 0.2784, 0.2667],\n",
      "          [0.3725, 0.3804, 0.3725,  ..., 0.5216, 0.5176, 0.4863],\n",
      "          ...,\n",
      "          [0.7725, 0.8078, 0.8157,  ..., 0.4941, 0.5059, 0.4745],\n",
      "          [0.8000, 0.8431, 0.8980,  ..., 0.4745, 0.4745, 0.5294],\n",
      "          [0.7843, 0.8784, 0.9020,  ..., 0.5373, 0.5059, 0.5843]],\n",
      "\n",
      "         [[0.2118, 0.1961, 0.1922,  ..., 0.3843, 0.3804, 0.3333],\n",
      "          [0.1961, 0.1922, 0.1882,  ..., 0.2471, 0.2510, 0.2471],\n",
      "          [0.2980, 0.3020, 0.2980,  ..., 0.4902, 0.4902, 0.4667],\n",
      "          ...,\n",
      "          [0.7333, 0.7725, 0.7608,  ..., 0.4353, 0.4431, 0.4196],\n",
      "          [0.7725, 0.8235, 0.8902,  ..., 0.4196, 0.4196, 0.4667],\n",
      "          [0.7569, 0.8667, 0.9020,  ..., 0.4824, 0.4549, 0.5255]]]]), labels=tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "dataset_v2 = load_dataset(\"imagefolder\", data_dir=\"data/animal\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((16, 16)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "def transform_to_set(batch: dict[str, list[any]]) -> dict[str, list[any]]:\n",
    "    transformed = [transform(image) for image in batch[\"image\"]]\n",
    "    return {\"image\": transformed, \"label\": batch[\"label\"]}\n",
    "dataset_v2.set_transform(transform_to_set)\n",
    "\n",
    "dataloader_v2 = DataLoader(dataset_v2[\"train\"], batch_size=2)\n",
    "for batch in dataloader_v2:\n",
    "    images, labels = batch[\"image\"], batch[\"label\"]\n",
    "    print(f\"{images=}, {labels=}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[map()](https://huggingface.co/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)はCallableなら何でも渡せる。\n",
    "\n",
    "しかし、注意しないとDataLoaderでバッチ処理を行う際、バッチ次元を加えたテンソルではなく、テンソルのリストが返ってきてしまう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"xhiroga/MiniAlbum\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(batch[\"image\"])=<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# map() かつ batched=False で、transform するとリストになる。これは、HuggingFaceのDatasetが多様なデータ（テンソル以外を含む）を扱うための仕様と思われる。\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "compose = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "def transform(example: dict[str, any]) -> dict[str, any]:\n",
    "    return {\"image\": compose(example[\"image\"])}\n",
    "transformed = dataset.map(transform, batched=False)\n",
    "\n",
    "dataloader = DataLoader(transformed[\"train\"], batch_size=2)\n",
    "batch = next(iter(dataloader))\n",
    "print(f\"{type(batch[\"image\"])=}\") # <class 'list'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "type(batch[\"image\"])=<class 'list'>\n",
      "type(transformed['train']['image'])=<class 'torch.Tensor'>\n",
      "type(batch_torch[\"image\"])=<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "compose = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "def transform(batch: dict[str, list[any]]) -> dict[str, any]:\n",
    "    # type(batch[\"image\"]) == list だが、transforms.Compose() がバッチ処理を行う際は、対象はテンソルである必要がある。\n",
    "    # [Can transforms.Compose handle a batch of Images? - vision - PyTorch Forums](https://discuss.pytorch.org/t/can-transforms-compose-handle-a-batch-of-images/4850/4)\n",
    "    images = [compose(image) for image in batch[\"image\"]]\n",
    "    stack = torch.stack(images)\n",
    "    return {\"image\": stack}\n",
    "transformed = dataset.map(transform, batched=True) # <class 'list'> ...\n",
    "print(f\"{type(transformed['train']['image'])}\")\n",
    "\n",
    "dataloader = DataLoader(transformed[\"train\"], batch_size=2)\n",
    "batch = next(iter(dataloader))\n",
    "print(f\"{type(batch[\"image\"])=}\") # <class 'list'>\n",
    "\n",
    "# map() で変換した後に、set_format() で変換すると、リストからテンソルに変換される。\n",
    "transformed.set_format(\"torch\")\n",
    "print(f\"{type(transformed['train']['image'])=}\") # <class 'torch.Tensor'>\n",
    "\n",
    "dataloader_torch = DataLoader(transformed[\"train\"], batch_size=2)\n",
    "batch_torch = next(iter(dataloader_torch))\n",
    "print(f\"{type(batch_torch[\"image\"])=}\") # <class 'list'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ拡張も map() で行うのがよい。\n",
    "\n",
    "[How to augment data ? · Issue #365 · huggingface/datasets](https://github.com/huggingface/datasets/issues/365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "英文の一部の単語を置き換えることで、データのバリエーションを増やす例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 35.3k/35.3k [00:00<00:00, 11.8MB/s]\n",
      "Downloading data: 100%|██████████| 649k/649k [00:00<00:00, 921kB/s]\n",
      "Downloading data: 100%|██████████| 75.7k/75.7k [00:00<00:00, 357kB/s]\n",
      "Downloading data: 100%|██████████| 308k/308k [00:00<00:00, 1.44MB/s]\n",
      "Generating train split: 100%|██████████| 3668/3668 [00:00<00:00, 203283.61 examples/s]\n",
      "Generating validation split: 100%|██████████| 408/408 [00:00<00:00, 135988.24 examples/s]\n",
      "Generating test split: 100%|██████████| 1725/1725 [00:00<00:00, 431384.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")   # [General Language Understanding Evaluation](https://note.com/npaka/n/n5086fc19c5fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 3668/3668 [00:00<00:00, 76417.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from transformers import pipeline\n",
    "\n",
    "fillmask = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
    "mask_token = fillmask.tokenizer.mask_token\n",
    "smaller_dataset = dataset.filter(lambda e, i: i<10, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smaller_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(examples):\n",
    "    outputs = []\n",
    "    for sentence in examples[\"sentence1\"]:\n",
    "        words = sentence.split(' ')\n",
    "        K = randint(1, len(words)-1)\n",
    "        masked_sentence = \" \".join(words[:K]  + [mask_token] + words[K+1:])\n",
    "        predictions = fillmask(masked_sentence)\n",
    "        augmented_sequences = [predictions[i][\"sequence\"] for i in range(3)]\n",
    "        outputs += [sentence] + augmented_sequences\n",
    "    return {\"data\": outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 19.76 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'Amrozi accused his brother, whom he called \" the witness \", of deliberately distorting his evidence.',\n",
       " 'Amrozi accused his brother, whom he called \" the witness \" \" of deliberately distorting his evidence.',\n",
       " 'Amrozi accused his brother, whom he called \" the witness \",\" of deliberately distorting his evidence.',\n",
       " \"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\",\n",
       " \"Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $1 billion.\",\n",
       " \"Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $2 billion.\",\n",
       " \"Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $3 billion.\",\n",
       " 'They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_dataset = smaller_dataset.map(augment_data, batched=True, remove_columns=dataset.column_names, batch_size=8)\n",
    "augmented_dataset[:9][\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Batch processing](https://huggingface.co/docs/datasets/process#batch-processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Data augmentation](https://huggingface.co/docs/datasets/process#data-augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 39\n",
       "})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map() のデータ拡張のサンプル。ただし画像が数万枚ある場合は、CursorのJupyter Notebookで実行するとOOMで落ちた気がする。\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "dataset = load_dataset(\"xhiroga/MiniAlbum\", split=\"train\")\n",
    "\n",
    "compose = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def augmentation(batch: any):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for image, label in zip(batch[\"image\"], batch[\"label\"]):\n",
    "        images.append(compose(image))\n",
    "        labels.append(label)\n",
    "        images.append(compose(image))\n",
    "        labels.append(label)\n",
    "        images.append(compose(image))\n",
    "        labels.append(label)\n",
    "    return {\"image\": images, \"label\": labels}\n",
    "\n",
    "mapped = dataset.map(batched=True, function=augmentation)\n",
    "mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.ConcatDataset at 0x179021fb500>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OOMを避けるには、 set_transform() したデータセットを追加する方法がある\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "dataset = load_dataset(\"xhiroga/MiniAlbum\", split=\"train\")\n",
    "\n",
    "compose = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def transform(batch: dict[str, list[any]]) -> dict[str, list[any]]:\n",
    "    composed = [compose(image) for image in batch[\"image\"]]\n",
    "    return {\"image\": composed}\n",
    "\n",
    "dataset.set_transform(transform)\n",
    "\n",
    "# HuggingFaceのDatasetをPyTorchのConcatDatasetにそのまま突っ込んでいいのか...?\n",
    "multiplied_dataset = ConcatDataset([dataset]*3)\n",
    "multiplied_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface-datasets-vision-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
